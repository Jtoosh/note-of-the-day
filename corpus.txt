I found a neat website to give me a quick tutorial in Markdown syntax.To _italicize_ a word, you just surround the word with an underscore '_'.To make a word **bold** you surround it with two asterisks '**'.This can also span across _multiple words_, and you can make something _**both bold and italic**_.It doesn't matter what order the underscore and asterisks are in this case.To make a header in Markdown, you place a # in front of that word, at the beginning of the line.There are six header sizes, and to change the size you simply add the corresponding number of #s. (header 1 uses #, header 6 uses ######)
There are two types of links that you can make in Markdown.Inline links involve 2 parts: the link text, and the URL to link to.The link text is typed inside of square brackets '[]' and the URL is typed immediately after the brackets in parenthesis '()'.Here is an example:
I try to regularly check [MLB.com](https://www.mlb.com) to keep up on my favorite teams.Bolding and italicizing can alos be applied to the link text, and you can add inline links to headers as well.First, you need to declare the reference, and the thing it links to.Think of it as declaring a variable, whose value will be a page to visit.The advantage of this type of link is that if you link to a page multiple times, you can create the reference and then only ever have to edit the link once.Here is my example:
I am super excited for March Madness, so I can make a good [bracket][tournament-challenge]
[tournament-challenge]: https://fantasy.espn.com/games/tournament-challenge-bracket-2024/
The syntax for a reference is "[reference Name]: www.linktowebsite.com"
Images have the exact same syntax as links in Markdown, with only one minor addition: you precede them with a '!'.The two types of images are the exact same as links: inline image links, and reference image links.Also, and exclamation point is only needed with reference image links in the "call" to the reference, not the instantiation of the reference itself.To call special attetntion to some text in a Markdown file, the _blockquote_ syntax can be used.To do a multi-paragraph blockquote, simply put a > at the beginning of each line, including blank ones.If you don't do this, they will be 2 separate block quotes instead of 1 total.You can make ordered and unordered lists, and they can be nested within one another.To make an unordered list, you simply place an asterisk '*' at the beginning of the line.To make an ordered list, you simply place the number 1. at the start of the line, or whatever number is next in the list.Tears of the Kingdom  
    This one might be ranked higher, but I haven't finished it yet.Unfortunately, the list does not auto continue in all compilers.To nest a list you simply put the cursor in front of the bullet/number and indent, then resume making the list as normal.As I displayed above, you can also just have a simple paragraph nested in a list item, not another sublist.Similar to HTML, Markdown renders code as all one line,
even when words are on adjacent lines.THere are two ways to create a space between lines upon rendering.The other way is to simply press the spacebar twice after a line, then the next line will render on the line directly below, as it appears in the code.I demonstrated it here a few times, but it is hard to see visually.Anytime you see adjacent lines that are separated, like in this paragraph, you can know it is a soft break.This can be really useful in creating subparagraphs in lists.Thanks for visiting [The Markdown Guide](https://www.markdownguide.org)!This Markdown cheat sheet provides a quick overview of all the Markdown syntax elements.It can’t cover every edge case, so if you need more information about any of these elements, refer to the reference guides for [basic syntax](https://www.markdownguide.org/basic-syntax/) and [extended syntax](https://www.markdownguide.org/extended-syntax/).These are the elements outlined in John Gruber’s original design document.Third item
First item
Second item
Third item
`code`
---
[Markdown Guide](https://www.markdownguide.org)
!It requires a server to run, and is used one its own to build robust applications, or with HTML to create dynamic websites.Some of the capabilities of PHP include:
Access input into web forms and utilize it.Find out broswer and operating system of users
Display alternate web page layouts for other users, such as mobile users
Basic and advanced math
Create global HTML templates
PHP scripts can be placed anywhere within a document, HTML, for example.Using these tags, PHP scripts are placed inside of HTML documents just like any other HTML element.The only thing that is case-sensitive in PHP are the variables and constants.The syntax for variable instantiation in PHP is a $, followed by the variable name, =, and then the variable value.The value of a variable can be printed by simply putting the variable name in argument of the print function.Side note: the $ needs to be included in each instance of variable use, it is literally part of the variable name.Constants can be defined in PHP scripts using the `define` function.This is the syntax: `define("CONST_NAME", "Constant value");` Constants, like variables, are case sensitive, and by convention are always given all-caps names.From the "Getting your Hands Dirty" lesson, here are a few notes:
When using the `date()` method, PHP has to know the timezone where the server resides, in order to output the correct hour and date for that geographical location.The date_default_timezone_set() method takes a string that locates the server.The list of supported timezones can be found at [here] (<http://php.net/manual/en/timezones.php>)
Another note on the `date()` method, the arguments passed dictate how the date is displayed.Arrays work similarly in PHP as they do in other languages: they must contain elements of the same data type, each element has an index, starting at 0 and moving up.Arrays are created similarly to how variables are in PHP: `$my_array = array("value1", "value2", "value3", "value4");` This will create an array of strings with those 4 values.In these arrays, you can set the value of the key, and its corresponding element.Here is an example:
So, the way that a key is assigned to a value is by putting the key on the left side, followed by an equal sign and greater than sign (`=>`), with the value, whatever its data type, on the right side.Another common type of arrays in PHP are **Multi-dimensional Arrays**.Multi-dimensionals are simply arrays that contain other arrays as their elements.Each array is separated by a comma, just like each element in any normal array.In multi-dimensional arrays, the inner arrays do not need semi-colons following them, just commas.Here is the syntax for a conditional statement in PHP:
>Side note, `random_int(int min, int max)` can be used in PHP to give a random integer in the given range.This checks if the values are equal and of the same data type.Here is the syntax:
It is also worth mentioning that integers can be incremented or decrementing in php using the `++` and `--` operators.Typing `$a++` will increment the value of `$a` by 1, and `$a--` will decrement the value of `$a` by 1.Here is the syntax:
These are useful when you know how many times you want to loop through a block of code, or you have an iterable data type, such as an array.Here is the syntax:
This will print each value in the array, one at a time.So, Do while loops will always execute the first iteration, and while loops won't even start unless the condition is TRUE to begin with.Here is the syntax:
There are two types of functions in PHP: built-in functions and user-defined functions.There are thosuands of built-in functions in PHP, and they are used to perform specific tasks.These functions are used to perform specific tasks, and can be used in conjunction with user-defined functions to create more complex applications.Here is the syntax:
The function is then called like normal.To create parameters for the function is the same as other languages as well, you simply put the parameter name in the parentheses of the function definition.I can follow this pattern in other websites I may make, to better organize and style them.This is mainly driven by the PHP `include` function, which allows me to include the contents of one file in another file.This is a great way to keep the website consistent and organized.For example, in the website from the course, instead of coding the header and footer in the `index.php` file, I made separate `header.php` and `footer.php` files.I made an associative array in an `arrays.php` file, and then used a `foreach` loop to iterate over that array in my `nav.php` file.Then I simply included the `nav.php` file in the `header.php` file.So, with this setup, to and a new page, I simply code that page in its own php file, and then enter that file name and the page title into the array in `arrays.php` and then the PHP script in `nav.php` will automatically add that page to the nav bar.If I read the documentation I should be able to get it to work though.The underlying skill to learn here is using the documentation and analysis of the code in plugins that I include to improve my ability to include and use plugins in my projects.Thus, when getting the array for a certain item, the key and the value was needed, so the `foreach` syntax of `foreach($array as $key => $value)` was used.Creating these templates is useful, because now, all I need to do is create one template page for a dish, and then whenever I want to add a new dish, I simply input the info into the array.This can have great applications for other use cases, such as templating new page to a website, products in a store, or others.The query string is usually added to the end of a URL, and begins with a question mark, for example: `dish.php?item=mexican-barbacoa`.The word after the ?, `item` in this example, is the name of a variable, and the value following the equals sign is the value of that variable.The `$_GET` superglobal is used to collect that data from the query string.In the above stated example, the key would be `item`, so `$_GET['item']`would return the variable value, `mexican-barbacoa`.The method `isset()` is used to check if a variable is set, and works with `$_GET` to check if a variable is set in the query string.Another method used to protect against header injections (this was a more advanced move), is the `preg_replace` method, which takes in three arguments, first a regex pattern, second what to replace with, and third, the input string to examine.The form element is used to create the form, the input element is used to create the input fields, and the label element is used to create labels for the input fields.The form element has an `action` attribute, which is the URL of the page that will process the form data.The `method` attribute is used to specify how the form data should be sent.The `GET` method sends the form data in the URL, and the `POST` method sends the form data in the HTTP request body.The `POST` method is more secure than the `GET` method, because the form data is not visible in the URL.The `input` element is one of the most powerful HTML elements, driven mostly by its attributes.The `type` attribute is used to specify the type of input field, such as text, date, email, etc.The `name` attribute is used to specify the name of the input field, which also sets the key for the `$_POST` superglobal, with the inputted text being the value.Another common attribute is the `required` attribute, which is used to specify that the input field is required.To view a full list of the attributes, consult [MDN Web Docs] (<https://developer.mozilla.org/en-US/docs/Web/HTML/Element/input>).The `label` element is used to create labels for the input fields.The `for` attribute is used to specify which input element the label is for.The `for` attribute should have the same value as the `id` attribute of the input element.This also makes the form more accessible, as it allows screen readers to read the label when the input field is focused, and lets users click on the label to focus on the input field, which is often larger and easier to click on than the input field itself.The `$_POST` superglobal is used to collect form data that is sent with the `POST` method to either the current PHP page or another page.Like `$_GET`, the `$_POST` superglobal is an associative array, so the value of the variable is accessed by using the key.Another thing that emails need to be sent are the email headers.Email headers are like the envelope that describe the message.They are written in plain text, in specifications that adhere to internet protocols.Here is an example from this site:
Lastly, the `mail()` function is used to actually send the email.It takes in at least 4 parameters, in this order: `mail($recipient_email, $subject, $message_of_email, $email_headers)`
**Web Hosting**
Hosting my site on the web requires that I pay for hosting services to buy a domain.Author: **Brad Hussey**
---
Latest Update: **October 6, 2014**
Updated all associative arrays to include a string ( "" ) wrapping text within the square brackets ( [] ).Update: **August 14, 2014**
Updated all instances of `"/assets"` to `"../assets"`, except in **26_Final/Instructor** where `"/assets"` is  now `"../../assets"`.My ambient flashcard app: note of the day
I find myself often taking diligent notes in my college classes,  but not as often going back to review them.Over time, I've tried to get better and only putting the most precise and needed pieces of info into my notes, to keep fluff out.I also feel that many of the most important things I've learned in my classes are things that I fundamentally understand, but when I'm not using them consistently, I forget them.If I review my notes real fast things click again, but I don't find myself doing that as much as I'd like.This app is my attempt to remedy that, or at least alleviate it.Note of the day is meant to act like a "quote of the day" service, but for your notes.My vision is to allow a user to pick a file or directory, and have a simple desktop GUI display selected snippets of notes.BSD 3-Clause License
Copyright (c) 2013-2024, Kim Davies and contributors.Redistribution and use in source and binary forms, with or without
modification, are permitted provided that the following conditions are
met:
1.Redistributions of source code must retain the above copyright
   notice, this list of conditions and the following disclaimer.Redistributions in binary form must reproduce the above copyright
   notice, this list of conditions and the following disclaimer in the
   documentation and/or other materials provided with the distribution.Neither the name of the copyright holder nor the names of its
   contributors may be used to endorse or promote products derived from
   this software without specific prior written permission.THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS
"AS IS" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT
LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR
A PARTICULAR PURPOSE ARE DISCLAIMED.For documentation, please visit [nltk.org](https://www.nltk.org/).Please read [CONTRIBUTING.md](CONTRIBUTING.md) for more details.See also [how to contribute to NLTK](https://www.nltk.org/contribute.html).Please support NLTK development by donating
to the project via PayPal, using the link on the NLTK homepage.If you publish work that uses NLTK, please cite the NLTK book, as follows:
    Bird, Steven, Edward Loper and Ewan Klein (2009).Copyright (C) 2001-2024 NLTK Project
For license information, see [LICENSE.txt](LICENSE.txt).NLTK source code is distributed under the Apache 2.0 License.NLTK documentation is distributed under the Creative Commons
  Attribution-Noncommercial-No Derivative Works 3.0 United States license.NLTK corpora are provided under the terms given in the README file for each
  corpus; all are redistributable and available for non-commercial use.NLTK may be freely redistributed, subject to the provisions of these licenses.The `webbrowser.open('url')` method will open the passed in URL in the default browser.The `webbrowser.open_new_tab('url')` method will open the passed in URL in a new tab in the default browser.The main method is `requests.get('url')` which downloads the webpage for the passed in URL.The result of that method is an object of class "response" Response objects have some useful methods such as `response.status_code` which returns the status code of the request (200 is OK, 404 is not found, etc.Another method of response objects is `response.text` which returns the HTML of the webpage in a string.The `iter_content()` method downloads files in chunks, by iterating over the response object.You can specify the size of the chunks with the `chunk_size` method parameter.You can create a BeautifulSoup object by passing in the HTML string and the parser you want to use into the `bs4.BeautifulSoup()` method (case sensitive).HTML string is most easily passed in by passing a response object using the `.text` method.The parser is specified using '', like "html.parser" for example.The `prettify()` method of a BeautifulSoup object will return a nicely formatted string of the HTML.The prettify method will also add newlines and indentation to the HTML string, improving readability.When you make a BeautifulSoup object, each HTML tag in the document is turned into an object of the **tag** class.You can use the `tag` attribute to return the first tag object of the BeautifulSoup object.You can also use the `tags` attribute to get a list of all the tags in the soup object.The `tags` attribute will return a list of all the tags in the object.You can use the `get()` method of a tag object to get the value of an attribute of the tag.The `get()` method takes the name of the attribute as a string, and returns the value of the attribute.You can also use the `attrs` attribute of a tag object to get a dictionary of all the attributes of the tag.The `attrs` attribute will return a dictionary of all the attributes of the tag.Two main methods for parsing using bs4 are `find()` and `select()` methods.The `find()` method will return the first tag that matches the passed in string.The `select()` method will return a **list** of all tags that match the passed in string.A sort of middle ground to these tow is the `find.all()` method.The `find_all()` method will return a list of all tags that match the passed in string, and take the same type of parameters as the `find()` method (excluding the `limit` parameter, which is exclusive to `find.all`.It dictates how many of the corresponding elements will be returned when found.).The `find_all()` method is more powerful than the `find()` method, but the `select()` method is still more powerful than both, because the use of CSS selectors enables extremely specific searches.The Selenium Module is one that allows programs to control browser windows.The `webdriver.Firefox()` method will open a new Firefox window.The `webdriver.Chrome()` method will open a new Chrome window.The `webdriver.Ie()` method will open a new Internet Explorer window.The `webdriver.Safari()` method will open a new Safari window, and so on with ther browsers.The `get()` method of a webdriver object will open the passed in URL in the browser window.The `back()` method will go back to the previous page, and the `forward()` method will go forward to the next page.The following paragraph describes some methods that were deprecated when Selenium 4 was released:
    >The `find_element_by_css_selector()` method of a webdriver object will return the first element that matches the passed in CSS selector.The `find_elements_by_css_selector()` method will return a list of all elements that match the passed in CSS selector.This relationship also applies for the `find_element_by_name()` method and the `find_elements_by_name()` method.The same applies for `find_element_by_tag_name()` and `find_elements_by_tag_name()`; `find_element_by_class_name()` and `find_elements_by_class_name()`, and similar methods.The way that elements are found using Selenium 4 includes using a class called `By`.The `By` class has a number of methods that can be used to find elements.The `By.CSS_SELECTOR` method can be used to find elements by CSS selector.The `By.ID` for an element's ID, the `By.TAG_NAME` method for tag name, and etc.An example would be: `element = driver.find_element(By.CSS_SELECTOR, 'p')`.The `find_element()` method of a webdriver object will return the first element that matches the passed in method and selector.The `find_elements()` method will return a list of all elements that match the passed in method and selector.The `openpyxl.load_workbook('filemname.xlsx')` method will load an Excel file as a Workbook object.To get a specific sheet from the workbook, use `workbook['Sheet1']` or whatever the sheet name (the `get_sheet_by_name()` method has been deprecated, as well as the `get_sheet_names()` method).Use `workbook.sheetnames` to get a list of all the sheet names in the workbook.When a sheet is accessed, it is returned as a Worksheet object.To access a specific Cell (the Excel term for the intersection of a row and column), use `workbook['Sheet1']['A1']`.The `value` attribute of a Cell object will return the value of the cell.The `row` attribute will return the row number of the cell, and the `column` attribute will return the column letter of the cell.Another way to access a cell is to use the `cell()` method of a Worksheet object.The `cell()` method takes two parameters, the row number and the column number(1-26, with 1 being A and 26 being Z), and returns a Cell object.An example would be `workbook['Sheet1'].cell(row=1, column=1)`.The Worksheet object's `max_row` and `max_column` attributes will return the number of rows and columns in the worksheet, respectively.The `iter_rows()` method of a Worksheet object will return a generator object that will iterate over all the rows in the worksheet.The `iter_cols()` method will return a generator object that will iterate over all the columns in the worksheet.The `rows` attribute of a Worksheet object will return a tuple of all the rows in the worksheet.The `columns` attribute will return a tuple of all the columns in the worksheet.To convert the columns numbers to letters, use the `openpyxl.utils.get_column_letter()` method.This method takes an integer as an argument and returns the corresponding column letter.To convert column letters to numbers, use the `openpyxl.utils.column_index_from_string()` method.This method takes a string as an argument and returns the corresponding column number.Steps of the program:
Take in the URL as command line argument
Make sure to pass in the URL as a string with quotes, otherwise a terminal error keeps getting upset at the '&' characters in the URL.Download the page using the requests module
Because it will first go to the login page, find a way to keep the session open, or use Selenium to log in.Create a list of the URls of the job listings that contain the keywords
Either Open the URLs in the browser, or write them to an output file with hyperlinks.When we tell them that math is easy, when we encounter difficulty with them they get the message that the difficulty is not with the math but with them as people.When he really does expect that of us, he will request that we do so.This is hard to do with various online resources, so getting the understanding with the book first is good.Think about setting a limit of how many lines I code before testing.Both mean "power set"
*The* symbol for and is "*∧*" and for or is "*∨*"
*set* difference is written as "-" or "\"
"*∪*" and "*∩*" are union and intersection respectively.Lowercase letters are usually used to denote elements of sets.This done by listing the elements of the set within curly braces.For example, the set of all vowels in the English alphabet, V can be written as V = {a, e, i, o, u}.A datatype is the name of a set where all the elements of the set have satisfy certain properties, and can have certain operations performed on them.Subsets
: *A* is a subset of *B* if every element of *A* is also an element of *B*.To say that *B* is a superset of *A*, use the symbol *B* ⊇ *A*.Cardinality of a Set
: The cardinality of a set *A*, denoted |*A*|, is the number of *distinct* elements in *A*.Power Set
: The power set of a set *A*, denoted P(*A*), is the set of all subsets of *A*.If a set has *n* elements, then its power set has 2^n elements.Ordered Pairs (Ordered n-tuples)
: A pair of objects in a specific order.It is equal to {b1, b2, ... bn} if and only if a1 = b1, a2 = b2, ... an = bn.The ordered pair (a, b) is equal to the ordered pair (c, d) if and only if a = c and b = d.
Cartesian Product
: The *set of ordered pairs* where each element {*(a,b)* | a ∈ *A* ∧ b ∈ *B*}.The order of the sets in the Cartesian Product determines which values are the first coordinates and which are the second.The Cartesian Product of the sets *A1*, *A2*, ... *An* is an ordered tuple (*a1*, *a2*, ... *an*), such that *ai* ∈ *Ai* for all *i*.A relation from set *A* to set *B* is a subset of the Cartesian Product *A* x *B*.A relation from set *A* to itself is called a relation on *A*.A relation does not need to contain every element of the Cartesian Product.Intersection
: The intersection of two sets *A* and *B* is the set of all elements that are in both *A* and *B*.Difference
: The difference of two sets *A* and *B* is the set of all elements that are in *A* but not in *B*.This is denoted *A* - *B*, and can also be called the compliment of *B* with respect to *A*.The set of all elements that are in *B* but not in *A* is denoted *B* - *A*, and is called the compliment of *A* with respect to *B*.Complement
: The complement of a set *A*, denoted *A* (with a line above it lol), is also the compliment of *A* with respect to the universal set, *U*.Here is some important information about Python type annotations from the Project 0 starter code:
Here is a link to the commit message guidelines to follow for this class: [Git Commit Message Guidelines](https://gist.github.com/robertpainsi/b632364184e70900af4ab688decf6f53)
Functions
: Given two nonempty sets, *A* and *B*, a function *f* from *A* to *B* assigns exactly one element of *B* to each element of *A*.The element of *B* assigned to an element *a* of *A* is denoted *f(a)*.The set *A* is called the domain of *f*, and the set *B* is called the codomain of *f*.The range of *f* is the set of all elements of *B* that are assigned to elements of *A*.The image of *a* under *f* is the element of *B* assigned to *a*.If *f* is a function from *A* to *B*, we write *f*: *A* → *B*.A function is called a one-to-one function, or an injective function, if each element of the domain is assigned to a distinct element of the codomain.A function is called an onto function, or a surjective function, if each element of the codomain is assigned to an element of the domain.In other words, there are no leftover elements in the codomain.This means the range of the function and the codomain are the same.A function is called a one-to-one correspondence, or a bijective function, if it is both one-to-one and onto.A function is called an invertible function if it is a bijection.The inverse of a function *f*, denoted *f*^-1, is a function from the codomain of *f* to the domain of *f*.The composition of two functions *g*: *A* → *B* and *f*: *B* → *C* is *f*(*g*(a)).For this to be defined, the range of *g* must be a subset of the domain of *f*.A partial function is a function that maps *A* to *B*, but the domain of the function is a subset of *A*, not the whole set.Partial functions are undefined for some elements of the domain.A function where the domain is the whole set is called a total function.Finite State Machines
: Something that finds patterns and outputs something.It is something that be a mathematical model, or a computer program.It involes a few parts:
The set of states *S*
the set of inputs *I*
the set of outputs *O*
the transition function *f*
and the output function *g*
States have a small amount of memory, *s1* remembers that a 'c' has been seen, in the lecture example.However, *s2* will only remember that a 'o' has been seen, but it will know that a requirement of getting to *s2* is to come from *s1* where a 'c' must have been seen.There can also be an error state, labeled *sERR* in the lecture example.Having the state machine be defined as an object with methods for each state, rather than a slew of if-else suites, maintains the flow control, but is more readable and maintainable.It maintains the flow control because that code obviously doesn't do anything until the function is called.Symbols include ":"
*Facts* are things we know about the world.Symbols include ":-"
*Queries* are the things we want to know.It also has user-defined commenets, strings, and identifiers.Token
: A token consists of 3 parts: the type, the value, and the position (i.e.This means that any keyword will match the token of the keyword, and an ID token.I'll create a FSM for each specific pattern I want to recognize.If two FSMs have an read a pattern, the one that "wins" or the token type that gets added to the list is the one that has the longest pattern.Line 62 on `fsm.py` is an example of the Ternary operator in Python.I have discovered that this is happening because of the fsm.py implementation
The `match` keyword needs to be used for pattern matching for the tokens that take in a literal as declared in their class definitions.A static method is one called on the class itself, not an instance of the class.These methods can be called without passing their parameters, and are used to create utility functions that don't need to be called on an instance of the class.There is a very handy method for strings and chars called `isalnum()`.This method returns True if all the characters in the string are alphanumeric, meaning they are either letters or numbers.Another thing that helped a ton was drawing out the FSMs on paper.This made writing out the code and the state functions much easier.Formal Definition:
A finite set of states *S*
A finite set of input symbols *I*
A transition function *f*: *S* x *I* -> *S*
A start state *s0* ∈ *S*
A set of accept states *F*
Some symbols:
A double circle is an accept state
The lambda symbol is the empty string
Regular Expressions and FSA are cousins, both pattern manager tools.If V is the vocabulary of a language, then V* is the set of all strings that can be made from the symbols in V. This includes the empty string.String
: A sequence of elements of *V*, the accepted characters.Generators
: Tools used to create all strings in a language, or the set V* in this example.Recognizers
: Tools used to determine if a certain string is in a language.Regular Expression Base Cases are: *a*, *λ*, and *∅*, where *a* is a set {*a*}, *λ* is the empty string, and *∅* is the empty set.Regular expression operations are concatenation, union, and Kleene star.Concatenation is the operation of putting two regular expressions together.Union is the operation of combining two regular expressions.Kleene star is the operation of concatenating a regular expression with itself zero or more times.It is possible that no regex can be written to generate certain patterns, such as matching parenthesis or curly braces.This is because regexes are not powerful enough to match these patterns.Grammar
: *G* = (*V*, *T*, *P*, *S*), where *V* is the vocabulary, *T* is the set of terminals in the vocab, *V*-*T* is the set of nonterminals, *S* is the starting nonterminal and *P* is the set of productions.Terminal
: A symbol that is used to build up a string
Nonterminal
: A symbol that is used to build up a string, but is not in the final string.Production
: A rule that describes how to build up a string.Derivation
: A sequence of strings that starts with the start nonterminal and ends with a string of terminals.Derivation cannot stop when nonterminals are present in the string.Leftmost derivations start at the leftmost nonterminal and work their way to the right.Rightmost derivations start at the rightmost nonterminal and work their way to the left.The root is the start nonterminal, and the leaves are the terminals.Parse Trees are read from left to right, reading only the leaf nodes.Backus-Naur Form (BNF)
: A notation for describing the syntax of a language.Project 2 uses lowercase for the nonterminals and all caps for the terminals.The right side can be any string of terminals and nonterminals.Context-Sensitive Grammar
: A grammar where the left side of the production can be a string of terminals and nonterminals.Parse Tree
: A tree that shows the production rules used to derive a valid string.A grammar is ambiguous is there exists a terminal string that has more than one valid parse string.Ambiguity in computer science is bad because it leads to inconsistent outputs.Operators with higher precedence need to be lower in the parse tree.Precedence ambiguity is fixed by creating new nonterminals that push the higher precedence operators lower in the parse tree.Associativity
: The order in which operators are evaluated when they have the same precedence.Addition and subtraction are left associative, meaning they are evaluated from left to right.Exponentiation is right associative, meaning they are evaluated from right to left.Associativity ambiguity is fixed by creating new nonterminals that push the execution into the correct order.Left associativity is fixed by using left recursion, and right associativity is fixed by using right recursion.The facts and rules can be empty, but there must be at least one query also.The 'Schemes:' part is required for each of these 4
The function-like parts of the datalog program are called predicates.Gives meaning to the string
Prefix notation, infix notation, and postfix notation.The FIRST set of a nonterminal is the set of leftmost terminals that can be derived from that nonterminal.AKA, the first letter of the strings that can be produced from that nonterminal.The FIRST set of a string is the set of leftmost terminals that can be derived from that string.A grammar is LL(1) if
For all nonterminals *N*
the FIRST sets of all RHS of rules using N don't overlap (in other words, none of the productions have the same elements as their left most terminals)
LL stands for Left-to-right input, Leftmost derivation.Review the parts and rules of Recursion:
Parts of recursion:
Initial call
Base case check
Recursive call
Rules of Recursion:
  0.Assume the recursive call worked and solve the k --> k + 1 challenge.Make sure that the number of recursive calls is reasonable
  5.Show that if it works correctly for k, then it will work correctly for k + 1.The heading row is for the input character that is being read, and the heading column is for the nonterminal that is at the top of the stack.The cell at the intersection tells what to push onto the stack (and implies that the nonterminal of that row is to be popped off the stack).Trace
: A record/history of the productions used to parse a string.It is a record of the stack and the input string at each step.Head predicates have parameters of ID types, and queries and other predicates can have parameters of string or ID types.In Project 2, I'll have to develop a way to check if the next input character is in the first set of any of the productions for the nonterminal at the top of the stack.When I apply recursion, I'll have to use the input char being in the follow set of the nonterminal on the top of the stack as the base case.FOLLOW Sets
: A set of all of the terminals that can legally follow that production.Formal definition: FOLLOW(A) = *t* ∈ *T* : *S* =>* *wAtn*, where *w* is all of the terminals that preceed *A*, and *n* is all of the terminals that follow *t*.A lambda in the parse table indicates to pop the stack, and not advance the input.If *p* and *q* are statements, then *p* ^ *q* is the conjunction of *p* and *q*.If *p* and *q* are statements, then *p* v *q* is the disjunction of *p* and *q*.If *p* and *q* are statements, then *p* -> *q* is the conditional of *p* and *q*.Implication
: The relationship between the antecedent and the consequent in a conditional.Conditional-disjunction equivalence
: The conditional *p* -> *q* is logically equivalent to *~p* v *q*.Essentially, trade and implication for a disjunction (with a negation up front).If *p* and *q* are statements, then *p* <-> *q* is the bi-conditional of *p* and *q*.Compound Proposition
: A proposition that is formed by combining two or more propositions with logical operators (negation, conjunction, disjunction, conditional, bi-conditional).Tautology
: A compound proposition that is always true, no matter the truth values of the propositional variables within it.Contradiction
: A compound proposition that is always false, no matter the truth values of the propositional variables within it.Contingency
: A compound proposition that is neither a tautology nor a contradiction.Logical Equivalence
: Two compound propositions are logically equivalent if they are their bi-conditional is a tautology.Proof
: Identifying facts in the world, then using logic to derive from them new facts, and then concluding an interesting fact.In propositional logic, facts are represented as propositions.Regardless of what facts are true, a substitution using logical equivalences can always be done.Two Proof tools:
**Logical Equivalences**: Used to replace one fact with another, foundation is tautology.Common Rules of Inference:
Modus Ponens
: The basic concept of implication.Three proofs patterns:
Deduction: *Deducing* a conclusion from existing premises to create new facts, using logical equivalences and rules of inference.Contradiction: Negating desired conclusion, then deducing new facts until we reach a *contradiction*.If *p* V q and *~p* V r, then q V r. Works with multiple premises too.Using resolution and contradiction, anything that is provable can be proven.The triple quote operators `""" """` can be used on a string with new lines, to insert the newline characters when comparing a string.A new variable must be used to store the return when working with other iterables, like a set.Use this when dealing with iterables to have elements added to the list, rather than the iterable itself.To then assert the exception, use the `excinfo` fixture to access the exception object.Syntax looks like this:
Reminder: tautology is a proposition that is true in all possible worlds, contradiction is false in all possible worlds.Logical Equivalences can always be used and counted on, rules of inferenece can be used in worlds where the premises are true.State the contradiction
Rules for CNF:
Propositions are grouped by disjunction and separated by conjunction
Negations can only precede a propositional pariable
Implication not allowed in CNF
A *predicate* is a parameterized proposition.Values for which the predicate is true
The Schemes section of a datalog program declares the predicates, and the Facts section declares the values for which the predicates are true.In the rules section, the commas signify conjuction, and the colon-dash signifies implication (towards the direction of the colon).Universal Quantifier
: The symbol ∀ is the universal quantifier.If *P(x)* is a predicate, then ∀*x* *P(x)* is true if *P(x)* is true for every value of *x* in the domain.Existential Quantifier
: The symbol ∃ is the existential quantifier.If *P(x)* is a predicate, then ∃*x* *P(x)* is true if *P(x)* is true for at least one value of *x* in the domain.Universal and existential quantifiers have the highest precedence of all propositional operators.A variable is said to be *free* if it does not have a quantifier.De Morgan's Laws with Quantifiers:
Negation flips the affected quantifier when moved inside the quantifier to the proposition variable.There can be nested quantifiers, where there are multiple quantifiers consecutively.To jump from quantified variables to specific instances of them is called **universal instantiation**.Moving from specific to general is **universal generalization**.Nesting quantifiers is needed when dealing with multiple variables.The order of the quantifiers is important, and the order of the quantifiers is the order in which they are written.Repeat
When converting to CNF, getting rid of implications is a good first step.Using universal generalization in a proof usually includes universal instantiation, because most of the rules of inference don't work well with quantifiers.Also, when using universal instantiation, think of it as bringing in an arbitrary and specific instance of the domain, "*an* instance" not "*the* instance".Closed-world assumption
: Don't have the full definition, but Prof. Goodrich explained that this is what allows us to know that the bound variableds x and y have the same domain in the proof.I am still having significant challenges with the testing pane in VSCode.I think it has to do with the pytest config that I did in project 1, and where I have Pytest installed.I'm just using the terminal to run the tests for now, I'll figure out the testing pane later.DO NOT OVERLOOK***
The testing pane in VSCode was not working with my usual dev set up because it can't access the project directory to configure the pytests.To resolve this, open the project directory in a new window, and then the testing pane should work.For some reason, the definition of the `RelationTuple` type made it so that I couldn't use the standard `tuple()` constructor.This wasn't necessarily a problem, but I want to eventually try and understand why this is the case.An issue that I ran into later in `Interpreter.py` , was the relation that I was passing in was not actually populated with anything, it wasn't accessing the values in the Interpreter object's `relations` dictionary.I wanted to avoid memory issues, but I also needed changes to compound when I would have queries that require multiple relational operations.So, I assigned the relation I was passing to the methods to the deep copy I made, and then had it update itself as methods were called.Now the only bug I seem to have left is that the interpreter is not able to find answers to queries where every single parameter is a constant/string.I solved this one, by determining that in the case where the query was asking for all strings, and all the same strings, because I was using the `index()` list method, it was returning the index of the *first* occurence of the passed in value.I simply fixed this by using a variable to keep count of the number of variable that I was on for that query, and using that to correctly index the list of headers for the select operations.Now, it appears that the only other bug I have is with `select_eq_col`, returning too many matches, so that should be the last bug.Here is a useful reference of how to write the MyPy type annotations for different types: [Built-in Types](https://mypy.readthedocs.io/en/stable/builtin_types.html)
Something important I've learned is that sets are non-indexable.But, they are able to be made easily into lists, using the `list()` constructor.Tuples, although imutable, can actually be added using just the normal addition operator.Case 5  - two common headers with duplicate values
This case works, but I need to dig more to understand exactly why, I thought the .remove() method that I used with some lists would cause problems.I learned while working with `eval_rules()` about the StopIteration exception with iterators.My understanding is that these are thrown when `next()` is called on the iterator, but it has not yielded anything else.I learned that `yield` pauses execution and later resumes at the same point, while `return` ends the function and returns a value.So with iterators, The number of times `next()` can be called before a `StopIteration` exception is thrown is the number of times that a `yield` statement is executed, either through loops or subsequent calls to the function.I need to begin the habit of commenting my code as I go along, this will make things much easier.I went to the TA lab, and I started this last night but had it reiterated there, that clean code is important.In some cases the complex issues caused by unclean code are what break a program.The 2 specific principles that the TA taught me were 1: Don't define a nested method, just define it at the top level and use it in the method I need, and 2: don't define methods in for loops.One of the TAs helped me learn that the `-k` flag in pytest lets me run a certain case of a parameterized test.My big bug that was the majority of failures was the way that I was looping through the rules, using my "rule queue".The problem with that method was that if the initial evaluation does not change anything, it is never evaluated again, but the evaluation of a later rule could change the relation so that the initial rule would now be able to produce something.Or, to use more proper terms, if one rule was evaluated before its dependencies were, it would not be evaluated again after the dependencies were evaluated.I fixed this by using a while loop that would continue until the relation was no longer changing.Text for Project 5a:
Looping through a dictionary in Python is a bit particular.By default, when using `for entry in MyDict`, the loop will iterate through the keys of the dictionary.To iterate through the values, use `for entry in MyDict.values()`, and to iterate through both the keys and values, use `for key, value in MyDict.items()`.This class is meant to take me from saying to myself "I trust that this works, but I don't know how" to "I know how this works, and what happens behind the scenes".This will be accomplished for the topics in this class, but I will still always be in the "I trust that this works" stage for some topic until I learn it better.Abstraction
: A model of something that hides or ignores a certain level of detail.C's Data types are:
Whole number types:
`char`: smallest, enough space to represent a single character (all the ASCII characters), and numbers up to 256.Floating point numbers:
`float`: normal size
`double`: higher precision, due to more decimal places and more space.To output variables in C, format strings are used, unlike in C++ where variable names can be used.When this is done, the full string with the format string character is the first argument, and the variable name is the second argument.Common format strings:
`%d`: integer
`%f`: float
`%c`: char
`%s`: string
`%x`: hexadecimal
`%l`: double
`%p`: pointer
Instead of `cin` from C++, `scanf` is used in C to read from `stdin`.The arguments similar to `printf`, with the format string first, but then it needs the address of the variable to assign the value to, not the variable name itself.You can also scan in multiple variables at once, separated by commas.Heres an example:
A format code needs to be included for each variable being scanned in, regardless of if the data types are the same.Each data type in C has a specific size:
`char`: 1 byte
`int`: 4 bytes
`short`: 2 bytes
`long`: 8 bytes
`float`: 4 bytes
`double`: 8 bytes
Because of discrepancies in data type sizes across various hardware, the `stdint.h` library was created.This library contains data types that are guaranteed to be a certain size, regardless of the hardware.Examples:
`int32_t`: 4 bytes
`int64_t`: 8 bytes
> **Aside**: Processors are referred to by their _word size_, which is the size of an individual memory address, and how many bits are processed by the CPU at one time.Hexadecimal is often used to represent memory addresses, because it is more compact than binary, as 1 letter of hexadecimal represents 4 bits, _so 2 letters represent 1 byte._
Hexadecimal numbers usally have a leading `x` or `0x` to indicate that they are in hexadecimal.The most significant byte is stored in the highest address, and the least significant byte is stored in the lowest address.One important thing about arrays in C, is that nothing stops you from accessing indeces outside the array range.If you try and access an index outside the array, it compiles and will return a zero.Typecasting is done by putting the new type in parentheses before the variable, such as `(char *)ip`.In different bases, the value of a place is determined by raising the base to the power of the place.In base 10, you raise 10 to the power of the place, with the one's place starting at 0.Hexadecimal is 16^i (hex does get more complicated in other ways, though).The number in each place tells how many of that place there are.When converting from binary to hex, start from the right (least significant) and evaluate in groups of 4 into hex characters, through the whole binary number.When converting from hex to binary, expand each hex digit into the corresponding nibble.Modulo divide n by 2 (without changing n), the result is the bit for the next least significant digit.In addition, when you would need to "carry a 1" in base 10, you do that same thing in other number bases.Multiplication and division can be tricky when done arbitrarily, but multiplying and dividing by the base is easy, because you simply add or subtract a place to the number, adding a 0 to the right if multiplying by the base, or remove the rightmost 0 when dividing.Recap: **Bit-shifting** is multiplying or dividing by 2 in base 2.This can be done with the `<<` and `>>` operators, respectively.Using these operators in other number bases will simply multiply by 2, not add or remove number places like in binary.So far, we've just been dealing with unsigned data types when converting between decimal, hex, and binary.Now, we are going to focus on how to represent signed, or negative, numbers in binary and hexadecimal.We want to shoot for a method of represntation that **1**, does not waste or duplicate values, **2**, makes numbers easy to negate, and **3**, allows the addition circuitry to be used for subtraction.The representation method that computers use is called **Two's Compliment (2C)**.With this method, the most significant place value becomes negative, and the rest of the place vales are positive.Adding the values in the present place values gives the number being represented.To negate a number in 2C notation, flip the binary digits, and then add 1.When converting between signed and unsigned, the bits themselves stay the same, they just get interpreted differently.And, because 2C focuses on not wasting spaces, any bits for positive numbers will be unaffected.When an expression in C mixes signed and unsigned numbers, the compiler implicitly casts to unsigned number.This is a cause of lots of strange bugs and non-intuitive behavior.Conversions between sizes are referred to as _truncating_ for moving to a smaller type, and _expanding_ for moving to a larger size.When expanding, the lower order bits stay the same, and the higher order bits are filled in.With unsigned numbers, these higher orders are filled in with 0.So, if pointer arithmetic is being used to get a value from a certain address, the resulting dereferencing operator should be used with a cast to the type used in the type declaration of the variable being used.For example:
Another issue is overflow when doing arithmetic, and assigning results to intermediate values.This can cause overflow if data types aren't watched carefully.This can be avoided by declaring intermediate values as larger data types, or using one-line expressions that don't require intermediate values.This is equivalent to multiplying by 2^i, where i is the number of places shifted.Right shift `>>`
: Shifts the bits of a number _to the right_ by a certain number of bits.This is equivalent to dividing by 2^i, where i is the number of places shifted.Includes the logical right shift and the arithmetic right shift.The logical right shift fills in the leftmost bits with 0s, and the arithmetic right shift fills in the leftmost bits with the sign bit.Bitwise AND `&`
: Compares the bits of two numbers, and returns a number with a 1 in the place where both numbers have a 1, and a 0 otherwise.Bitwise OR `|`
: Compares the bits of two numbers, and returns a number with a 1 in the place where either number has a 1, and a 0 otherwise.Bitwise XOR `^`
: Compares the bits of two numbers, and returns a number with a 1 in the place where only one number has a 1, and a 0 otherwise.Bitwise operators are mostly used for 2 reasons: **1** to extract some of the bits of some type, or **2** to set some of the bits to a type.When focusing on a bit, there are 3 main goals I can have: **1** set the bit to 1, **2** set the bit to 0, or **3** leave the bit unchanged
To achieve one of these goals, use the AND + OR operators between the target variable and a mask to get the desired goal.To set all bits to 0, perform AND with something with the value 0.To set all values to 1, perform OR with something with the value 1.Example: you want to know what bits 0-3 of the following char are.So, bits 0-3 we want unchanged, and all others we want to be 0 So, this is what follows:
As opposed to the bitwise operators, there are the logical operators in C: `&&` for AND, `||` for OR, and `!` for NOT.Rather than working bit-by-bit, they treat the argument as a whole, and evaluate it as True (non-zero) or False(zero).It represents logic that all of computation can be built off of.It can be represented mechanically (hole punched or not, punch cards), and electronically (charge or no charge, etc).One of the physical building blocks of implementing boolean algebra is the electronic switch, which would open or close a gate based on if there is electric current.The next step after the electronic switch, as they were slow and failed with wear and tear, was the vacuum tube.ENIAC was a famous/historical computer that used vacuum tubes, and broke almost daily.Made out of semiconductors, able to switch 10,000 times per second, was smaller and cheaper.Moore's law: number of _transistors_ on a computer chip(CPU), doubles every 2 years
Today, transistors are < 50 nanometers thick, and can switch millions of times per second.Transistors will essentially, keep a gate open if voltage is high, or shut off the gate if voltage is low.From the boolean transistor gates NOT, OR, and AND, we can create all other functions needed.Y86-64 is an assembly language called "simple assembly"
X86-64 is "real assembly" or what all code gets compiled down to.Processor state: The current status of execution in the processor.Includes:
Memory: stores bytes for data, programs
Registers: The "scratch paper" for the CPU.Processor status: Includes:
Program Counter (PC); Address of next instruction
Condition Codes: 1-bit flags that show results of most recent math/logic operation.Include ZF (zero), SF (sign), and OF (overflow)
Program Status (Stat): Status codes for a program.Include: `AOK` (normal operation), `HLT` (operation halted by instructions), `ADR` (invalid address encountered), and `INS` (invalid instruction encountered)
Assembly instructions directly manipulate the processor state.It does this with actions like moving values, performing operations on values, writing new values, checking conditions, etc.All instructions are extremely simple, they do exactly 1 basic step.Some assembly notations:
`q`: quad-word, 64 bit/8 byte
`b`: byte, 8 bit/1 byte
`w`: word, 16 bit/2 bytes
`l`: double-word, 32 bit/4 bytes
Y86-64 only works on quad-word values.Each instruction has an binary encoding to perform the instruction in the hardware.Syntax for these instruction names encode **the source and destination, in that order.You can also specify an address a certain number of bytes away from something.One of the registers must be overwritten, the destination register is one of the sources.When using binary/bitwise operators, the destination, is the **left** operand.So, the right operand in assembly is the **left** operand in normal C code (in both bitwise and arithmethic instructions)
A bit more info about these:
**ZF**: results in 1 if the last ALU operation is exactly 0.To remember these combinations that allow for boolean logic, imagine that the flag is set to 0 or 1 based on the output of the last ALU operation.The destination, `Dest` can be a hardcoded address, like `24`, `8(%rax)`, or a label, like `loop`.Using the stack instructions, `pushq` and `popq` both have the same syntax and do 2 things.This will resume execution from where the function was called.Assembly source code essentially tells the simulator how to fill up memory.So, when I run instructions on the simulator and see the values pop up on the lines of memory where each instruction is on the right panel, those are the encodings for their encodings.Each register also has its own encoding, so that the registers being used can be included in instruction encodings in one digit.The sequential architecture is organized in **stages**, and each stage performs a part of the necessary steps for an instruction.These 6 stages are repeated until the `halt` instruction is given, or an error occurs.Details about each stage for different instructions are given in _computation tables_.These can be found in the textbook in chapter 4, and are a type of pseudocode for how instructions work at a lower level.To pipeline, registers are made to store values between multiple stages.This allows for multiple instructions to move through a pipeline at once.This helps processors work faster, but does have some challenges, like branch prediction (waiting on a previous instruction), and the fact that different instructions need different amounts of time to execute.Using a stack is useful for computer programs, because it naturally fits the procedural nature of programs.Whatever is stored in the register `%rax` is what will be returned at the end of a function call in Y86.However, all of the 32/16/8 bit registrs can still be accessed (this is why x86 is backwards compatible).For example, the 32/16/8 bit registers of `%rax` are `%eax`, `%ax`, and `%al`, respectively.If I change `%al`, I am actually also changing `%ax`, `%eax`, and `%rax`.This picture shows the corresponding alternate bit registers:
!Just `movq` with different types of operands, and x86 interprets what the operands are.Uses `b`, `w`, `l`, or `q` in place of `x` to compare values of the corresponding data size.So, `movq 6(%rdx), %ax` would move the value 6 bytes, or 6 memory addresses.Basically, an object in the struct of size _L_ bytes needs to be at an address that is a multiple of _L_.Gaps or empty memory locations are used to fulfill this requirement.Users can specifically design their inputs to attack a program.The key idea in these is, an input can line up its data with the return address of a function, it will then overwrite the return address with the input (often an address for some attack code).Another type of attack that builds off of buffer overflow attacks are **injection attacks**.These are similar, but they involve injecting executable code into the input, as well as overwriting the return address.Then the function taking input will return to the injected code, and execute it.In Dr. Mercer's words, the idea behind these attacks and "return oriented programming" is to "break up the continuation" of a program.The "continuation" of a program is where the execution continues to, and where it moves forward to after returning from a function call.My work in the attack lab will involve using buffer overflow attacks and injection attacks.Some programs put protections in place to guard against these attacks, such as randomizing the contents of the stack, or making code in the stack non-executable.However, through _return-oriented programming_, these protections can be bypassed.Input your attack string
With what I have learned so far, I know that the way that memory is accessed matters.A nested for loop that goes through a 2D array, for example, will be almost an order of magnitude faster if it iterates through row-by-row, rather than column-by-column.In computing, we want memory is fast, large, reliable, and cheap.The alternative then, is to mix memory with different characteristics, to make all memory seem to fit all of these.At the top is a little bit of fast and expensive memory, and the bottom is a larger amount of slow and cheap memory.Cache
: a place for hiding, storing, or preserving treasure or supplies.It stores some, but not all, of the data from the level below.Then each level can have a faster level than itself access the data it needs.Some caching terminology:
**working set**: all of the data blocks that are needed to run a program, or a part of it.Designers will often talk about miss rate, but the metric that is ultimately most important is _execution time_.There are 2 main types of locality:
**Temporal locality**: If a memory location/piece of data is accessed, it is likely to be accessed again soon.This means that the CPU can perform 2-3 billion operations per second.So, a latency of 1 billion cycles is really less than a second in our measure of time, but for a computer that is a long time.Some fundamental and enduring properties of hardware + software systems:
1.Faster storage tech virtually always costs more per byte, is smaller, and needs more power.General Cache organization involves sets, lines, and blocks.Each set consists of a certain number of lines, which each consist of 1 block.So the number of lines in a set = the number of blocks in a set.So, the size of a cache = #sets x #lines x #bytes per block.Each block also has some additional info besides just the data.It has a "valid bit" at the start, which indicates if the block is valid or not.For example, on boot, all the valid bits in all the caches should be 0, since nothing has been opened or accessed.Each selected address has a tag, set index, block index, which helps determine which block in the set is being accessed, which set in the cache is being accessed, and which part of a block, respectively.The tag combined with the set index is used to uniquely identify each block in a cache.A **direct-mapped cache** is a cache where each set has one line/block, or E = 1.A good example of this, and how to read/identify blocks from an address using tags, set indexes, and block indexes is in this image:
!There is something important about using lower bits in an address for the set index.I think it keeps the cache from becoming a copy of memory, but I'll need to ask about this to understand it better.When a set has more than one line/block, and a replacement needs to be made, different types of _replacement policies_ can be used.Some examples are:
Random line
Least Recently Used (LRU), replace the line that hasn't been used for the longest time
Least Frequently Used (LFU), replace the line that has been used the least often
There are more complex replacement policies out there, but they can become more expensive and cost more time to implement.But, they can be more effective, and lower down the memory hierarchy, they can be well worth it in the long run, since miss penalties are so much higher.The 1st dimension of the array is the sets of the cache, and the 2nd dimension is the lines/blocks in each set.The address of a something in a cache has a type of inverted index into this 2D array: `[tag][set index]` because the tag specifies the block in a set, and the set index specifies the set in the cache.On the lab, I struggled understanding how the miss rate of the C code was calculated.By default, the terminal wil always open in your home directory.Different flags used with the command dictate which of these it does.C programs need to be compiled, just like the other languages I've used that have a compiled runtime model (Java and C++).Here is a breakdown of the syntax for the `scp` command: `scp <source> <destination>`.If a file path needs to be specified for either the source or the destination, it should be in the form `<username>@<hostname>:<path>`.In the context of this class, that will usually look like `jtoosh@moat.cs.byu.edu:<path>`, when I'm using the remote machine.When my local machine is one of those parameters, a username and hostname is not needed, just a file path relative to the current directory.It has an optional argument of a directory that is put after the command.If no argument is given, it will print the tree of the current directory.This simply shows the full directory structure with the current directory or the argument directory as the root, and at the bottom print the number of subdirectories and files in the tree.Whatever is on the "greater than side" or "being eaten" is what is being redirected.The process involved the command `git remote add origin <url>`, to set the remote origin, and then `git push -u origin master` to push the local master branch to the remote origin.I ran into some issues and need to study git a bit more to understand how this process really works.The tutorial site is pretty extensive, and provides lots of good information, beyond the technical stuff, such as the history of C and is advantages and disadvantages.It attach the link [here](https://www.tutorialspoint.com/cprogramming/index.htm) for reference.It is used extensively in operating systems, language compilers, text editors, and other applications.It is a lower level language than others, meaning it interacts more directly with the computer hardware and memory.The UNIX operating system is written entirely in C.
C uses header files to include libraries, similar to C++.Some common ones are `stdio.h` for standard input and output, `stdlib.h` for standard library functions, and `string.h` for string manipulation functions.Like C++, C uses the `main` function as the entry point for the program.It returns an integer, which is the exit status of the program.By convention, a return value of 0 means the program executed successfully.Other functions are declared with simple signatures, just the return type and the name of the function.The implementation of the function is then defined later in the file.Key parts are the **dereferencing operator** `*`, and the **address-of operator** `&`.The `*` is used to declare a pointer, and it returns the value in the memory address a pointer points to.The `&` is used to get the memory address of a variable, and is used when defining a pointer.Strings are not a built-in type in C, but are represented as arrays of characters.However, strings don't have to be created only by making arrays of chars.They be created by making a char array, but then typing a string in double quotes, like other languages.When dealing with strings in I/O, the `"s%"` format string allows strings to be read in and printed out.However, the `scanf` function can still only assign read in values to char arrays, and a length of the array must be specified, and acts as a type of character limit.I want to figure out why the behavior for multi-word strings is the way that it is.Divide the decimal number by 16, then look at the remainder.The hex representation of the _remainder_ is the next least significant digit in the hex number.Now, divide the quotient by 16 again, and repeat this process with the remainder until the quotient of the floor division is 0.If I understand the project write-up correctly, the starter code contains a function for parsing the command line, and a skeleton of the `readAndPrintInputAsHex` function.This function calls 2 functions that I'll need to implement: `printDataAsHex` and `printDataAsChars`.Then I'll repeat the process for a `readAndPrintInputAsBits` function, which should use a `printDataAsBits` and `printDataAsChars` function as well.This project write up explained a couple of arguments for the `gcc` compile command.The `-o` argument allows you to name the resulting executable something other than `a.out`.This argument comes after the file name of what is being compiled, and simply type `-o <executable_name>`.Typing the `xxd` command in the terminal will prompt the user for standard input.After inputing from the keyboard and pressing return, the converted values will be output.Use the `-b` flag to output in bits rather than the default hexadecimal.Along with the redirect operators (see Lab 1 notes), the pipe character, `|` is useful for this lab and the project.The `cat` command reads a file sequentially and writes it to `stdout`.The `echo` command writes the provided operands to `stdout`
The hex and bit dumps for `xxd` have 3 parts.In other words, it is the memory address of the start of that line.So, the first line will always have a file offset of `00000000`.And if 16 bytes are on the first line, then the next line's file offset will be `00000010`.The hex dump appears as groups of 2 bytes, 2 hex digits per byte.The bits dump functions essentially the same, only instead of a max of 16 bytes (8 groups of 2 bytes/8 groups of 4 hex digits) the max per line is 6 bytes of binary.While there is a format character for chars and hexadecimal, memory contents cannot be naturally printed in binary.The write up shares a bit of an algorithm for prining out something as an byte (padding with leading 0s if necessary):
The `diff` command is essential for comparing larger outputs.Simply type `diff <file1> <file2>` to compare the two files.If there is no output from this command, then the files are identical.From the lab, I realized that each decimal number from 0-255 can really have 3 meanings: **First**, the actual decimal number value, **Second**, the hex number value (printing 16 with a `%x` format character yields 0x10), and the ASCII character that corresponds with that decimal or hexadecimal value (dividing 'A' by 5 equals 13, the same as dividing 65 by 5, 65 is the ASCII code for 'A').Something that I ran into in the lab, was realizing that to have the output for a byte look the way that I am used to, from right to left (with the least significant digit in the rightmost place), I need to store a byte in the opposite order that the computer normally would.The first digit of the byte, the least significant digit, needs to be at index 7, not 0, to appear on the rightmost place.Remember that strings print by encoding a byte one at a time until an ASCII `null` is found.Remember that `printf` does not take in a pointer, only `scanf` does.Remember to check typing with pointers and non-pointer values.These lab notes will serve as a sort of retrospective for the assignment.Questions 1 + 2 I did well with, Bit shifting is simple and something that I understand.Switching between signed and unsigned, and seeing how things loop when they overflow made sense too.I understood how truncating and expanding affected the data, depending on if it was signed or unsigned.And how logical operators behave if signed and unsigned are mixed (it casts the signed value to unsigned).And I found an easy method for 6, take the int that it wanted, and find the hex value for it by dividing by each significant digit.The biggest thing that I learned on the first try was that, when a different format character is being used than is the data type given (such as `%u` or `%d` for a `char`), it essentially expands or truncates the value, and behaves accordingly.Now that I type this, I think what may have thrown me off there is if the sign of a couple of the values would extend a 1 after expansion, since I didn't check what the most significant bit was.The biggest thing that got me here was not reading one of the problems closely enough, and I used all 6 bits when finding `char`s, when the question stated that `char`s are in 3 bits, so that got me.I also need to remember that when expanding a value, they extend the value of the most significant bit in signed values, and they extend 0s in unsigned values.Remember that when moving data to/from memory, I can't just reference labels, I would have to pass in the address of labels.When a label is moved to a register as an immediate value (without a `$` prefixing it), the _memory address of that label_ is what is placed in the destination register
I learned that the "return value" of a bit-shift operation is not the bits "shifted out" by the operation, but the result of the bits originally in question, after the shift has taken effect.Note that my PDF of the textbook is 28 pages later than what is noted on Canvas.Parameters x, y, and z are stored in registers %rdi, %rsi, and %rdx, respectively.Declaration of `NR(n)`: return 3n (look at line with `leaq (%rdi,%rdi,2), %rax`)
e. Declaration of `NC(n)`: return 4n+1
if NR == NC{
  NR >> 3
}
**Question 5:**  Given the code in problem 3.67:
A. Bro I don't want to draw the whole stack diagram.Basically it allocates 24 bytes for the struct `s`, then stores the first 16 bytes as the 2 elements of the `long` array, and the last 8 bytes as the pointer.Then it stores that in a register and moves it into memory in the corresponding spot.I enjoyed working in Vim for the whole project, and taking some good steps to learning that better.I think one of the biggest benefits of this project was it helped me straighten some things out in my mind, such as what one increment in a memory address means, and being conscious of dealing with the data a byte at a time, or a bit at a time, and what form I am putting it into.I had to think, compare outputs, write out and count things to really figure it out.I also went to the temple in between my main sessions of coding this, and during one moment while I was waiting in the temple, while thinking a bit about how to solve the padding partial hex dump lines, an idea came to me, and that ended up being the solution!I just needed to move a couple lines outside of the for loop.I think that shows that **pondering the code over time** and **making time for the Lord** will both lead to coding success.The offset for the 1st byte of the pixel array from the start of the file is an int, located 10 bytes into the bmp header, which is 10 bytes into the file.The width of the pixel array is found 4 bytes into the DIB header, which is 18 bytes into the file.The height is 4 bytes after that, 8 bytes into the DIB header, and 22 bytes into the file.I think I got the methods connected alright, but as of right now the images aren't being edited quite right.I think my for-loop logic is off, and maybe the way I am reassigning pixel RGB values for the threshold filter.I got the coloring figured out, I was thinking I needed to assign the colors of all three bytes in each pixel by doing the pointer arithmetic, but I just needed to do it once, because the pointer that I was using appears to have edited all 3 bytes.I needed to recruit ChatGPT for some help, but most of my problem constituted remembering the pointer I'm using is a 1-byte pointer, and the pixels are 3-byte units.The mental gymnastics of going back and forth between those made it tougher for me.Okay, I ran into the blue diagonal line issue on _only_ the fish image that some others in the class did.THe hint that Dr. Archibald gave proved correct, as the padding calculation/handling was what created the issue.This project was also not too strenuous, but in my opinion is was a bit more work than project 1.I had to similarly turn it in late, because I had a phase of 240 due on the same day.Luckily, the next project is due on a week that no 240 phases are due.Keeping track of whether I was moving by one pixel (three bytes) or one byte, and remembering that the pointer I'm drilling through 3 functions is a 1-byte pointer, and working out the needed math to make iteration correct was the most difficult for me.I was able to see more specific information, such as how many bytes my image was off from the correct image.I think something I could have done better to build my mental model and understanding correctly the first time, wis **commenting about the byte usage as I code** and **drawing out what I am trying to do**.As part of my working to understand this project, I'm going to write what I decipher the parameter names to mean (which correspond to the symbols on the computation tables in the textbook).In `utils.c` All the functions for the project are implemented.They are defined in `utils.h`, which is where I can look for more succinct definitions.Think of all the functions described there as my toolbox for the project.Now, my goal is to use those tools to implement the sequential architecture of the given operation.Now, my only question is how I am going to do this for different instructions, or in other words, make it modular.I didn't really understand the computation tables beforehand, and I kind of understood the execution stages, but after doing this project, I understand them both a lot better.This was a project so unlike the other things I've done before as a programmer that it was tricky to figure out how to approach it.I can give input via command line, or reading in a text file.This project was pretty good, but I feel like I wasn't as methodical about finding out what each phase was doing abstractly.In a way, I guess that isn't as important, but I feel that I was doing a lot of hunting and pecking instead of figuring out what things were doing.When I would start a new phase and start trying to decode it, I would step through really fast and not go as directly, instruction by instruction, as I maybe should have.In one way, there was maybe a bit too much to take in on the first pass, but I often found that when I was stuck, it was because I was stepping through the functions too fast and not figuring out what the parts that I didn't understand were doing.Writing out things that I found on a separate notes file helped a lot too.I also got help from _looking at the hints/dialogue from this and past semesters in the course discord server._
See project5notes.txt for my notes on this project.I think that this project was about as hard as the bomb project overall, but getting started was harder.I think one of the biggest keys for me was **remembering how the `call` and `ret` instructions affect the stack**.I was a bit confused about how the stack was going to affect execution, and how to push certain things onto the stack.I thought I had to figure out exactly what `getbuf` and `Gets` were doing to figure that out.Once I remembered that addresses for returning are stored on the stack, it became clear how to overwrite the return address and alter the continuation of the program.The other big help was **a prior thread in the discord** where one student was helping another one pretty hands-on.While it wasn't egregious with copy-paste code, it was a good guide that helped me catch onto the basic ideas when I got stuck.They can range from very informal ("Hey, can you take a look at my code?It can be intimidating, especially if you have to present the code to colleagues, but if I swallow my pride, I can learn a lot from them and grow a lot too.This means the frequency of code review is determined by the changes that are made, not a certain amount of time.The effectiveness of a code review was found to depend on the amount of time it takes.According to sources from the Wikipedia page, code reviews are optimal at a rate of 200-400 lines of code per hour.Obvious logic errors
Ensuring all of the requirement cases are fully implemented
Ensuring that the automated tests that are written are sufficient, or if new ones are needed
New code conforms to the style guidelines
Code reviews are especially useful for Agile development teams, that have the work decentralized across the team.Doing code reviews can share knowledge of the code base, and make it so that no single employee is the *critical path*.This will also allow for needed time off, and certain employees not being/feeling bound to a project.This distribution of knowledge leads to multiple people knowing the complexities, known issues, and concerns of parts of the code base, which leads to multiple informed opinions, creating more reliable and accurate estimations.Code reviews allow senior team members and opportunity to mentor junior team members, but also gives a fresh perspective to conventions that may actually be hindering the efficiency of the code base.To keep code reviews optimized for time, here are 3 useful practices:
  1.Share the load, requiring multiple reviews, from different parts of the team
  2.Require reviews before code is merged into the main branch
  3.Creating a culture of code reviews will motivate developers to write better code in the first place, as they know it will be reviewed.We handle slightly better being told about little errors by a computer than by another person.Settle style arguments with a style guide
  Don't bother arguing over style, as many different ways aren't necessarily better than others.Most all style only holds its value in its widespread maintenance.If an adjustment to it is needed, bring that up with the whole team.Start reviewing immediately
  Be considerate of the time of your coworker.Doing this promptly creates a virtuous cycle, where smaller changelists are given for review, which makes quicker reviews, which then encourages more smaller changelists.Start high level and work your way down
  Don't bother with the lower level stuff until the higher level and more important stuff is dealt with.If the smaller issues persist, then they can be addressed later.Be generous with code examples
   These help the author learn and grow from the review.Only use these for uncontroversial, certain changes, otherwise code examples can be seen as condescending.This makes the *code* review about the code, not the author.Frame feedback as requests, not commands
  Ask questions, rather than making demands.This also makes giving a response easier for the author, because rather than "pushing back" they are simply answering your question.Ground notes in principles, not opinions
  Give feedback, but then a reason for that feedback that is grounded in a good programming principle.This also make the author feel less attacked, and more like the feedback is an objective suggestion and a learning opportunity.This module will cover the tools Maven and Gradle, which are used to manage dependencies in Java projects.They download the correct versions of libraries and jar files for your project, and manage the dependencies between them.Maven
: A project management tool that is based on the concept of a project object model (POM; like the DOM in HTML).It maintains a set of standards for the project, and manages the project's build, reporting, and documentation from a central piece of information.It also manages dependencies, and can be used to build and run tests on the project.Using Maven allows a developer to ensure that their projects follow a consistent structure, more easily declare the dependencies of the project, and more easily manage the project's lifecycle.It also ensures that a project is *IDE agnostic*, meaning that it does not depend solely on any particular IDE and its features to be built and run.The POM file is an XML file that contains information about the project and configuration details used by Maven to build the project.It contains the project's dependencies, the plugins that are used to build the project, and what is referred to as the "coordinates" of the project, which are the group ID, artifact ID, and version of the project.These tests should not require the code be packaged or deployed
4.This repository is searched by Maven when it needs to download a dependency for a project.When I install Maven, it also created a local repository on my machine, in a folder named `.m2`.This is where Maven stores the dependencies that it downloads from the Central Repository, and where it stores the artifacts that it builds for the project.The dependencies are declared in the `<dependencies>` section of the POM file, and each dependency is declared with a `<dependency>` tag.Each `<dependency>` tag contains coordinates, just like individual projects do.A `dependency` also has a `scope`, which in what stage the dependency is used by the project.This will create a Maven project, and the POM file will be created for you.You can then add dependencies to the POM file, and IntelliJ IDEA will download them for you.Doing this will also create the directory structure that Maven expects automatically.In IntelliJ IDEA Ultimate, you can also view the dependencies of the project in a graphical view, which shows the dependencies and the relationships between them.Plugin goals
: Granular, specific tasks that are exectured by Maven Plugins.They can be attached to build phases (aka lifecycle phases) and execute upon running that phase, or they can be invoked directly from the command line.Rather than using XML, Gradle uses a langauge called Groovy, which is a variant of Java.Gradle is more flexible than Maven, and allows for more customization of the build process.It is also faster than Maven, and has a more concise syntax.Artifact
: A file that is produced by the build process, and is used as a dependency in other projects.It is usually a JAR file, but can be other types of files as well.Build file: Gradle uses a Groovy DSL (Domain Specific Language) to define the build file.This file is called `build.gradle`, and it contains the configuration of the project, and is both human and machine readable.Task Graph: Gradle builds a DAG (Directed Acyclic Graph) of tasks that need to be executed in the build process.Task Execution: Gradle exectures each task in order, recording the output.Because the output of each task is saved, for tasks that have repeat inputs, they are skipped if they are not needed to optimize the build process.Dependency Management: Gradle manages the dependencies of the project, ensuring that correct versions are downloaded.Repositories: Gradle uses repositories to store dependencies, and can use multiple repositories to download dependencies, such as central repositories, local machine repos, or others like those owned by an enterprise.Self-updating: Gradle can automatically update itself and the dependencies of the project.This plugin makes Gradle "Java aware" and allows it to build Java projects and define tasks specific to a Java project.These tasks include:
clean: cleans the Java byte code
compile: compiles the Java byte code into the source code
assemble: creates the JAR file
test: runs the unit tests
The plugin is added by adding `apply plugin: 'java'` to the `build.gradle` file.Gradle also manages dependencies similarly to Maven, and can actually use the Maven Central Repository to download dependencies.The dependencies are declared in the `dependencies` block of the `build.gradle` file, and are declared in a similar way to Maven, with the group ID, artifact ID, and version of the dependency.Here is an example of a dependency declaration in Groovy:
In the dependecies block, also declare the scope of the dependency, or in which task it is needed.Add the Java plugin to the `build.gradle` file, and then add a source code declaration, using the `sourceSets` block.Add the dependencies to the `build.gradle` file, using the `dependencies` block.The dependecy, like stated before, consists of the group ID (name of the organization), the artifact ID (name of the library), and the version of the library.Add the repository location for where Gradle will download the dependencies from.When using the IntelliJ new project wizard, you simply select "Gradle" for the build type.A few other settings are important: 'Use auto-import' should be checked, as that will allow IntelliJ to automatically import the dependencies that are added to the `build.gradle` file.The setting 'Create directories for empty content roots automatically' will create the directories that Gradle expects for the project structure, which is the same directory structure that Maven uses.For example:
Semi-colons are optional
Parentheses are optional for method calls with one or no arguments
Certain libraries like System, Math, String, and others are imported by default
Another important concept of Groovy is closures.Closures are blocks of code that can be passed around as variables, and can be executed at a later time.Here is an example of a closure in Groovy:
In this example, the same function is being exectued twice, but the code it is actually executing is passed in as a parameter of the type "Closure".This is similar to the idea of higher-order functions in other languages.Groovy does compile down to Java bytecode and is run on the JVM, so it is very interoperable with Java.In the build.gradle file, each of the blocks is actually setting a member variable of the `project` object.So, `sourceSets` is the same as saying `project.sourceSets`, and so on with the other blocks.The `gradle properties` command in the terminal will show the properties of the project, and the `gradle tasks` command will show the tasks that are available to the project.The second most important object in the POM, after the `project` is the `task` object.Each task is also a command that I can run on the terminal after the `gradle` keyword to run that corresponding task.Later down the line, I can investigate making my own tasks and other stuff about that.This will update the project with the changes I made to the POM file.It is hard to know if you actually have tested thoroughly enough.Automated testing can reach a far greater number of cases in a shorter amount of time.Automated testing can be run more frequently, and can be run repeatedly after changes are made, much more easily than manual testing over and over as changes are made.Functional testing (ensure the code *does* the right thing) is possible with manual testing, but performance testing (ensuring the code does the right thing *in a reasonable amount of time*) and load testing (ensuring the code does the right thing *under the expected amount of demand*) are much more difficult to do manually, and so automated testing is needed for those dimensions of testing.Here is an outline of the different types of software testing described by the article:
  1.Focuses on individual functions and methods of classes, and are cheap to automate and quick to run.More expensive to automate as they require multiple parts to be running simultaneously.Similar to integration tests, but rather than focusing on the interactions between parts of the application, it focuses on the actual output of the application, and ensuring that it is correct.These are very useful, but are expensive to perform and difficult to maintain when automated.They can be expensive to automate because they require the entire application to be running so that user behaviors can be replicated.Determines if performance requirements are met, loactes bottlenecks, measures stability of the application during peak traffic, and more.Useful to determine if mor expensive testing is warranted or if a new build is working properly.Automating tests can be done using different frameworks for different languages.Examples given included PHPUnit for PHP, Mocha for JavaScript, and RSpec for Ruby.Exploratory testing is manual testing that is recommended to try and find less obvious errors.Exploratory testing sessions should never span more than 2 hours.These are "white box" because they interact with the code directly.These should be "black box" because they interact with a live instance of the product, not the code.All of these layers are considered functional tests, because they verify functionality of the product.Placing more tests on the source code itself will catch simple bugs that are harder to recognize in integration or end-to-end testing.The article describes the **Rule of 1s**: unit tests take aout 1ms, integration tests take about 1s, and end-to-end tests take about 1m.Because of this, if a testing suite has a greater number of tests at the top level, it could take hours to run tests, which is not practial in today's development environment.These proportions should be a useful guideline, not a strict quota.Setting a strict proportion for different types of tests can encourage poor practices like skipping needed end-to-end tests and inflating the number and quality of unit tests.The following rule applies: **Each value within an equivalence partition must have the same output behavior as all other values in that partition.If the form requires a number between 1 and 100, the equivalence partitions would be:
1. the numbers from 1-100 (valid input)
2. the numbers less than 1 (invalid input)
3. the numbers greater than 100 (invalid input)
4. non-numeric input (invalid input)
These equivalence partitions represent all of the possible inputs from different types of output behavior.Boundary Value Analysis
: Testing values that lie at the extreme ends of a testing domain, with the assumption that most of the bugs lie there.These both are not the answer to every situation, but are valuable tools that have their use cases.It is part of the family of unit testing frameworks known as xUnit.Many other unit testing frameworks have been inspired by JUnit, such as NUnit for .NET, PHPUnit for PHP, and others.So, learning the ins and outs of JUnit will be useful for understanding other unit testing frameworks.JUnit promotes test-driven development, which encourages testing a little, then coding a little, then testing a little, then coding a little, and so on.This is known to increase code quality and reduce stress on the programmer.JUnit is highlighted by some of the following features:
Annotations to identify methods
Assertions for identifying expected results
Test runners to run tests
Test suites to group tests
Great simplicity and ease of use
Automated execution with self-checking test results and providing feedback
Progress bar with color indications
There is a recommendation of 2 unit tests for each requirement, one to test the positive case and one to test the negative case.These are used to set up resources for a specific test method, such as instantiating objects that are needed for the test.These are used to clean up after a test method, such as erasing any resources that were created by the `@Before` method and are no longer needed.These are used to set up resources that may be needed by all test methods, or other computationally expensive operations that only need to be done once.These are done to clean up after any operations performed by the `@BeforeClass` method.JUnit Basic Assertions:
There was an old list of assertions, where each case was a different assertion method (equals, false, null, is not null, etc).However, the `assertThat()` method is a lot better, because it can be used to test all of these, and it has better readability and give better failure messages.Its syntax is basically `assertThat(actualValue, is(method(expectedValue)))`.Some asserstions use what is called a hamcrest coreMatcher, which is a matcher that is used to test the actual value against the expected value.Here are some examples of coreMatchers:
allOf
any
anyOf
anything
both
containsString
describedAs
either
endsWith
equalTo
everyItem
hasItems
instanceOf
is
isA
not
notNullValue
nullValue
sameInstance
startsWith
theInstance
An example of this in use is `assertThat("theString", both(containsString("S")).and(containsString("t")))`.This one will cover the differences between JUnit 4 and JUnit 5.Some changes to annotations include:
`@Before` and `@After` are now `@BeforeEach` and `@AfterEach`
`@BeforeClass` and `@AfterClass` are now `@BeforeAll` and `@AfterAll`
`@Ignore` is now `@Disabled`
`@Test` can no longer have the `expected` attribute, and instead should use `assertThrows()` to test exceptions.It also cannot have the `timeout` attribute, and instead should use `assertTimeout()` to test timeouts.Assertion methods can be written with lambda expressions, and can be grouped together with `assertAll()`.Assumptions are now supported, which allow for conditional test execution.Lab Notes:
***Programming Principle:*** The principle of Cohesion states that unrelated things (bits of code, methods, source files) should not be located together, but should be separated by how closely they are related to each other.File structure and location is critically important in Java.Packages need to be in the correct place for the project to be able to build correctly.IDEs like IntelliJ have good tools to help with this, but it is something to be aware of.Also, the directory structure of the `main` and `test` directories inside of `src` must mirror each other, or the tests will not be able to find the classes they are testing.Use `@BeforeEach` methods to avoid code duplication in tests.This will pass if an instance of the specified class of exception or any of its subclasses is thrown.The class of exception is passed as a parameter with the lambda function that will execute.There is not a native function for testing if a certain type of exception does not get thrown, but this can be done by using a try-catch block in the test method and a fairly simple custom workaround.The arguments can come from various places, one of which is the `@ValueSource` annotation.This annotation takes an array of values that will be passed to the test method, but only works for one of four data types: String, int, long, and double.The test can also be given a name, by using the `name` attribute of the `@ParameterizedTest` annotation, placed in parenthesis after the annotation.Another argument source is `@EnumSource` which takes an enum class as a parameter, and will pass all of the values of the enum to the test method.The `@EnumSource` annotation also has a `names` attribute, which can be used to filter the values of the enum that are passed to the test method.The `@MethodSource` annotation is used to pass arguments to the test method from a method that returns a stream of arguments.The method must be static, and must return a stream of arguments.The method can be in the same class as the test method, or in a different class.The method can also take parameters, which will be passed to the test method.There are 2 types of CSV sources, `@CsvSource` and `@CsvFileSource`.Custom Argument sources can also be implemented, but are more complex.Parameterized tests are integrated with the full testing lifecycle, meaning that they use any `@BeforeEach` or `@AfterEach` methods that are present in the test class.This is useful for making the test results more readable and understandable.There are also `DisplayNameGenerator` classes that can be used to generate display names for tests.These can be used to generate names based on the method name, the class name, or other factors.A DisplayNameGenerator can be set for a test class by using the `@DisplayNameGeneration` annotation, and then implementing the `DisplayNameGenerator` interface, with various different Generators, such as one for converting underscores into spaces, or one for generating complete sentences.The default DisplayNameGenerator can be changed by setting the value of the `junit.jupiter.displayname.generator.default` configuration parameter to a "fully qualified class name" for a DisplayNameGenerator class.It is a way to group tests that have similar starting state to avoid redundant set up.In the example in the article, the input was 1 and 2 for half the test methods, and 1 and 0 for the other half.This was a situation in which using nested tests was useful.This is done by simply creating a new class containing the grouped tests, and placing the `@Nested` and `@DisplayName` annotations above the class declaration.Passing a value in for `delta` allows me to control the precision of a test.I was wondering why the test names and nesting wasn't displaying in IntelliJ, and I discovered that there is a button in the `Run` tab that focuses on passed tests, one for failed tests.I had the passed test button untoggled, so that's why they wouldn't show up when they passed
Code Coverage
: The measure of how much of the code is executed or "covered" by tests that are run.A metric that can be used to determine if there are enough tests written for that part of the code base.There are different types like:
*Statement* or *line coverage*, which just focuses on how many actual lines are covered.This is the ideal for testing, but also unable to be achieved because many functions have infinite distinct paths.This is the most stringent type of coverage, and is required for safety-critical software.This seeks to be thorough but also reducing the number of test cases needed.Test Criteria Subsumption
: The idea that a higher level of coverage will also satisfy a lower level of coverage.For example, branch coverage will also satisfy statement coverage because the requirements are stronger.It can be said that branch coverage subsumes statement coverage.There is no way to satisfy branch coverage without satisfying statement coverage.This also indicates when one level of coverage is higher than another, it is more expensive and slower, because more test cases will be needed to satisfy the higher level of coverage.What are the benefits of measuring code coverage:
Software that has high code coverage percentages is less likely to have undetected bugs stemming from code errors, poor coding practices, or overly complex code.High code coverage percentages *can* imply maintainability and readability, but not always.It provides a measureable metric for shareholders or others in communication who may not understand developer language.Generally, levels of 70-90% code coverage is considered good, but it is not a hard and fast rule.Provides and approximation value to use when defining "well-tested" code, especially in larger project teams.Mocking
: The process of creating stand-in or "dummy" instances of dependency units for units that are going to be tested, to the end of isolating the unit from those dependecies, while still being able to test its full functionality.We cannot control when we receive an answer, but we can control when we are seeking an answer.Answers often come when we are out in the sun working, rather than sitting in the shade.Mocking is difficult to do with static objects or singletons (I think this means pure functions), **dependency injection** is the usual way to make objects that are mockable.While unit testing is usually focused on **state-based verification**, mock testing focuses on **behavior-based verification**.State-based assert on the end status, or *state*, of the object, while behavior-based assert on if a certain method, or *behavior* was performed by the object, with the correct parameters.Common types of mock testing are *proxy-based mocking* and *classloader-remapping based mocking*.I'll have to read up and learn more to understand these better.Some best practices for mock testing are:
Don't mock external dependencies, only mock types that you own.One of the limits of mock testing is that is requires a good enough understanding of the dependencies, or the results won't be accurate.Overusing mocking can dramatically increase the needed maitenance of the tests.One limitation of mock objects is that for the mock test to be accurate, the behavior of the object being mocked must be well understood and established.Stub
: A piece of code that replaces a complex software component with a simpler, controllable, temporary standing.Use argument matchers to verify method calls with certain arguments.Use `verify(mockObject, times(#)).method()` to verify that a method was called a certain number of times
Lots of variations on this, `atLeastOnce()`, `atLeast()`, `atMost()`, `never()`, etc.Use `verifyZeroInteractions(mockObject)` to verify that no interactions were made with certain mock objects.The default return value for a mock is an "empty" value for its data type, like 0 for an int, or null for an object.Remember that you'd often keep using a mock even after the object you're mocking is created and operational, because you want to isolate the object from its dependencies and keep tests running quickly.When I had to write my own tests, one of the biggests challenges at first was understanding the interface between objects.I reckon that this will mostly be the case in the future as I work with Java.A mock, on the other hand, is an instance of a class that is a barebones version of the class, that simply tracks behavior.To create a spy, use the `Mockito.spy()` method, to create a spy out of the pass in object.This allows for methods like `verify(spyObject).method()` to be used to verify that a method was called on the spy object.To stub a spy, or give is a set return value, us the `doReturn()` method, followed by the `when()` method, followed by the method that is being stubbed.The article briefly explain the `NotAMockException`, but it is self-explanatory enough that I won't repeat it here.This also happens when calls to `thenReturn()` are chained, such as `thenReturn(1).thenReturn(2).thenReturn(3)`.Test-Driven Development (TDD)
: A software development process that relies on the repetition of a very short development cycle.The developer first writes a failing test
case that defines a desired improvement or new function, then produces the minimum amount of code to pass that test, and finally refactors
the new code to acceptable standards.As opposed to waterfall development which never works, and iterative development that often works, but often has bugs, TDD is a process
that often prevents hours-long debugging sessions by catching bugs early in the development process.He always knows exactly where my bugs are and how to fix them.If I will but ask him sincerely,
He will help me find them and fix them.He will always help me fix my bugs in the way
that will most benefit me as a person and a developer.Refactor the code and the test as needed, while maintaining passing of all tests.Repeat this process as needed
At first I did not understand why the code was written as simply as possible, even using hard coding, and then iteratively
refactored.But, thinking about it some more, that fits the idea of iterative development in the first place.Just writing
the unit test and then coding until it is exactly what is needed is more in the spirit of waterfall development, which is a poor practice.Writing
the test, and then writing the simplest code to pass, and then refactoring one step at a time to get cleaner and more matainable code is not only
more efficient, but simply easier to do.Improving code one step at a time is much easier than trying to write super clean code the first time.Something to consider in TDD is code-visibility, and how/if to access and test private data and methods.It is also important to consider testing
doubles, and how units will be isolated from their dependencies, using dummies, stubs, spies, or mocks.Another important principle of TDD is
keeping code units relatively small.This will reduce debugging efforts, as smaller units means less to look through, since tests should already
be isolated to a single unit.It also makes the test cases more readable and self-documenting.Some best practices include:
Having a good and consistent structure for the tests.A good structure to follow is: Setup, Execution, Verification, Cleanup.Treat test code with the same respect as production code
Test Oracle
: Whatever provides the information that determines correct outputs for a test case.Some practices to avoid:
Having "all-knowing" oracles, that are filled with information not pertaining to the test case.Causes spiraling false negatives and makes debugging harder.Having tests depend on system state that is manipulated by other test cases
Testing implementation details or precise execution specifics.Software to use with Testing and TDD includes the xUnit testing frameworks, which make automation much easier.The language-agnostic protocol Test Anything Protocol (TAP) is also useful for testing.For a comprehensive list of pros and cons, and the limitations of TDD, see the Wikipedia page.I'll list some of the major ones:
Advantages:
Greater confidence in the code
Less fear of change or experimentation, due to the large test suite
Comprehensive code coverage
Self-documenting code
Facilitates deeper understanding of the code, and the project requirements
Reduces debugging time
Greater emphasis on design and functionality.One of the rules is to **only write production code when a failing test requires it**.And, each cycle ends with a refactoring step where code is cleaned up and improved.Also, adding test cases that increase code coverage, even if new production code is not needed, improved regression detection.A regression is a change that breaks a previously working feature.Among the main types of testing, there are unit tests, integration tests, and end-to-end tests.This module will focus more on end-to-end testing, which consists of testing an application in a way that simulates a user interacting with the application.In the example of a web application, this would entail a browser being opened and navigating to the web application, and interacting with it using an automated tool.One that existed dominantly for a while was Selenium web driver, which I've used with Python.Playwright is another browswer automation tool that is more modern and has more features than Selenium, and will be what is used in this module.Quick note about HTML IDs, they can act as hooks in the URL, so if the url path includes an ID, such as `http://example.com/#id`, then the browser will scroll to the element with that ID when the page is loaded.It highlighted the need for testing automation on web apps, and gave descriptions of a few of the functionality tests that are needed on web apps.The article lists a few types of functionality tests:
Functional Tests
Usability Tests
Regression Tests
End-to-End Tests
Cross-Browser Tests
Performance Tests
[Playwright Intro](https://playwright.dev/java/)
Playwright tests can be written in several langauges, not just Java.If I need to use Playwright in the future, and need to refresh on the syntax, refer back to this lab.Performance Testing is a separate type of testing from functional testing, which includes unit, integration, and end-to-end testing.Performance testing focuses on the speed of the application, and how it performs under different loads.Testing different loads is called *load testing* and is a subset of performance testing.JMeter is an open source tool for performance testing that creates several threads, and then simulates a number of users interacting with the application at the same time, monitoring the performance of the application under different loads.It was originally designed for Web applications, but has since been expanded to test things like a Java object.It also uses the JavaDoc API to create a appealing report of test results.Core elements of JMeter:
Thread Groups: the collection of threads for a test, where each thread represents the user request.Samplers: informs the thread group of the type of request that JMeter will send to the server.Listeners: the way that JMeter displays the results of the test.Configurations: set up default values and variables to be used later in the test.Test plans: the overall plan for the test, that outlines the steps that will be taken when testing.It means reviewing the code without running it, hence the term "static".This is what is done by lots of the "linters" that I have run into so far.It checks for syntax errors, unused variables, indentation, and other things that can be checked without running the code.IntelliJ's static analyzer looks for syntax errors, dead code (unused variables), indenetation, potential runtime errors, and poor coding practices.It is run automatically when the code is saved, or by clicking **Analyze**->**Inspect Code**, and then following the prompts.Links for the file download and a quick start guide are on the Baeldung website linked above.The CLI command for the docker container is also on the webpage.Similar to platforms like phpMyAdmin, you need to login to SonarQube with an account, which has credentials initially set to `admin:admin`.To use SonarQube projects you'll have to get an access token, which is done on the account page (go to localhost, and whatever port the SonarQube server is running on), and after logging in navigate to the security tab, which is where tokens can be generated.Then make sure to add the plugin for SonarSource to the build file, whether that is Maven or Gradle.After running an analysis, issues are grouped into 5 categories: **Bugs**, **Vulnerabilities**, **Code Smells**, **Coverage Percentage**, and **Duplication Percentage**.Each issue can have one of 5 severities: **blocker**, **critical**, **major**, **minor**, and **info**.Another major feature that won't be covered in class, but that I found interesting, is **Quality Gates**.A quality gate is a set of conditions that project source code is tested against and must meet before it is deemed ready to be pushed into production.For example, the default quality gate that comes pre-installed is called *SonarQube way*.Coverage percentage of new code is at least 80%
2. percentage of duplication on new code is less than 3%
3. maintainability, reliability, and security rating is an *A* or better.If the code in question doesn't meet or exceed these conditions, it will fail, and be unable to be pushed into production.Another term defined here: **Leakage Period**: The period between two analyses or versions of a project.Quality Gates can be created, edited, and set as defaults from the "Quality Gates" tab of the SonarQube interface.A **Quality Profile** is a set of rules that are used to analyze the code.The default quality profile is called *Sonar way*, and it is a set of rules that are considered best practices for Java code.Quality profiles can be created, edited, and set as defaults from the "Quality Profiles" tab of the SonarQube interface.It underlines issues with blue squiggly lines when an issue is detected.SonarLint puts issues into 3 categories:
**Bug**: a coding mistake that can negatively affect performance or behavior
**Vulnerability**: a security issue that can open a window for an attacker
**Code Smells**: a violation of a clean coding rule that can affect maintainability and readability.Each of these issues has one of 4 severities: **minor**, **major**, **critical**, and **blocker**.Standalone mode is run with only SonarLint and no other peripheral softwares, but can only use the default rule set.Connected mode requires use of a SonarQube server or SonarCloud to access stored custom rules.SonarLint will flag things like always-true or always-false boolean expressions, redundant or verbose expressions, overly complex methods (evaluated using the Cognitive Complexity score that SonarLint calculates), and even some language/version specific things.Here is what I found that helped me to do so:
Run this command to check and see what versions of Java are actually installed: `/usr/libexec/java_home -V`
Then, this is the terminal command that will actually change which version is being used:
`export JAVA_HOME=``/usr/libexec/java_home -v <versionNumber>`
**Lab Notes:**
To run a SonarLint report on the whole project: Report -> File Icon -> Analyze Project with SonarLint
Remember to run SonarQube on Java version 17!Quality Profiles can be uploaded from XML files, like they had us do in the lab.This is done using the "Restore" tab in the Quality Profiles tab.To let SonarQube access my project, make sure my `<sonar.login>` and `<sonar.password>` are in the `<properties>` section of the `pom.xml` file.THen, running the maven install command, either in IntelliJ's GUI or in the terminal using `mvn install`, will run the analysis and upload the results to the SonarQube server.Connect SonarLint to SonarQube in IntelliJ by using the SonarLint settings found in the tab.Code profiling is a method of dynamic analysis that measures performance metrics, such as space and time complexity, time of function calls, and then gives suggestions for optimization.A code profile is generated by a code profiler, which is a tool that measures the performance of a program.Types of profilers (based on output):
**Flat Profiler**: computes the average call time, from the calls, don't break the call down based on callee or context.A profiler has various methods of reporting, including:
A statistical summary of the program's behavior, called a **profile**
A visualization of the program's sequential behavior, called a **trace**
A program can be modified to profile itself, inserting itself into a profiler at compile time.Some compilers provide profile-guided optimization, which uses the profile to optimize the code.Some notes about YourKit:
It can easily connect to an IDE via plugins.CPU profiling is needed to view how method calls affect CPU usage.This is done by changing the profiling mode from "sampling" to "tracing" on the play/stop button.Live view only show basic information, a snapshot is needed to see more detailed information.There are useful features like viewing code that is reported by something in the IDE, and YourKit will jump you to the exact line of code it is detecting a hot spot in.In the Callees List, the method that is being called is shown in a long form of dot notation, which shows the full path of the method, starting from the project root.Web programming is one of the most useful branches of skills and technologies to have in the world today.Learning these skills will open previously inacessible doors.Because the specifics technologies of web programming shift constantly, focus less on the specific technologies and more on the overall concepts and frameworks of being a web programmer in this class.Capable: know the technologies well, discern the meaningful tech from the fads, and know how to best leverage different technologies.Creative: scuplting and writing code to be neat and intentional is very much a creative process, and can be beautiful to those who recognize it.Collaborative: rarely are apps used by just one, and they are rarely built by just one.Remember, that in a team, 1+1+1 does not equal 3, but 4 or 5.Underlying all of these should be to be a Christlike software engineer, including:
Seeking divine help, including in coding, of what to do, where to go, and how to do something
Having an eternal perspective, looking past a project due date, a class, a job, or even this life.Some of the tools we will use include:
EC2: Elastic Cloud Compute - a virtual server in the cloud
ECS
**Console Application**
  A console application is an essential of web programming, and I need to make sure I have a POSIX compliant one.A console being POSIX compliant means that is supports a standard set of console commands.Some basic commands that I haven't run into yet include:
curl - Command line client URL browser
grep - Regular expression search
find - Find files
top - View running processes with CPU and memory usage
df - View disk statistics
cat - Output the contents of a file
**less - Interactively output the contents of a file** I want to explore this one more.Overwrites the file if it exists
`>>` - Redirect output to a file.Appends if the file exists
> Something that I loved from this assignemnt was figureing out that I could write a small bit of script, in the console, to manipulate a file, when I redirect the output of the script to the file using `>>`.I think that is connected to the `less` command, but I need to learn how it works more.Here is the line that was used:
While a repo can be made in a local directory using `git init`, it is easier to create a repo on GitHub and clone it to the local machine.To create a repository in GitHub, log into your account, select the `Repositories` tab, and press `New repository`.You then specify a unique repository name, give a description, indicate that you want it to be public, add a default README.md file, and choose a license.A repo clone is an exact copy of the repo with all of the commits, comments, and SHAs.It also configures the clone to know what the remote source is so that you can use Git commands to keep them in sync as changes are made.Pull the repository's latest changes from GitHub (`git pull`)
2.Push the changes to GitHub (`git push`)
⚠ Note: The first time you make a push request to a repository Git will ask you how you want to identify yourself and what credential (e.g.You will need to create a [Personal Access Token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token) and provide that as your password.The `git fetch` command will get the latest info about the changes to the remote source (GitHub) without making changes to the local repo.This info is displayed when running `git status` after running `git fetch`.Normally pushing and pulling between collaborators is just fine, until the same lines of code are edited in different ways, then a merge conflict happens.A merge conflict is resolved by editing the file in question, deleting the conflict markers, and then committing the changes.The `README.md` file is a markdown file that is displayed on the front page of the repository.To have it displayed on the front of the repo, it must be named README.md.It is similar to the `git clone` command, but it is creating a copy of the repo on GitHub rather than my local development environment.This is useful when I want to make changes to a repo that I don't have write access to.The forked repo still has a link to the upstream (original) repo, so I can pull in changes, and make pull requests to push suggested changes to the upstream repo.Personal access tokens are a way to authenticate with GitHub.They are used in place of a password when pushing or pulling from a repo.They are more secure than a password because they can be revoked at any time, and they can be limited to only the permissions that are needed.The two types described are fine-grain PATs and classic PATs.Fine-grain PATs are more secure because they can be limited to only the permissions that are needed.Classic PATs are less secure because they have access to all of the permissions that the user has, but they have some functions that don't work with fine-grain PATs.Read more about them [here](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)
**Pull requests:**
Pull requests are how you suggest/request a change to a respository.You can submit a pull request to push changes from your forked copy to the original repository.The owner of the original repository can then review the changes and decide whether or not to accept them.Pull requests are a way to collaborate with others on a project, and they are the way to contribute to open source projects.Pull requests are specifically for propsing changes to the main branch of code by merging in changes from a feature (other) branch.Pull requests can be reviewed and commented on by others in the repositories, and a pull request can have changes committed to it while it is still open and under review, and the files changed in the pull request can be viewed.A user can also make a draft pull request, which is a pull request that is still in progress and not ready to be merged.A lot more good info on pull requests can be found [here](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests)
**Lecture 9.5.24**
One of Prof. Ventura's biggest course objectives is for us to learn to think analytically and creatively.He also wants us to gain confidence, have fun, and of course learn web programming.Don't be trapped by the accumulation of ignorance, understand as I go.They are used to provide added functionality to an application.A `deployfiles.sh` script is used to deploy the application to the EC2 instance.I'll want to look at that in one of the versions of Simon and see how it works.Database: how any data that persists between sessions is maintained and reloaded.Peer-to-peer model: This is a model where each computer in the network can act as a server or a client.This is useful for sharing files, and it is more secure than a client-server model because there is no central server that can be attacked.It is also more efficient because the data can be transferred directly between the computers.Like with other projects I have had, remember to balance ambition with reality.I felt this when I had the idea to add the file input and quiz generation feature.I think I am going to still make the quiz/study app, but only do a simpler part of it for the project.Maintain the separation between the Development and Production environments.It is bad practice to do any coding/developing in the production environment.Development is the designated place for breaking and messing up.Also ensure that things being done in the development environment are being pushed to GitHub.Caddy
: a gateway that can be used for 3 main purposes: serving static files, web user authentication, and redirecting web traffic.DNS
: Domain Name System, and it translates domain names into IP addresses.In the case of AWS, the DNS is Route 53, and the web server is EC2.The unique combinations of IP addresses actually ran out in 2011, so now there are IPv4 and IPv6 addresses.IPv4 addresses are 32 bits long, and IPv6 addresses are 128 bits long.Other things that are being done to address the shortage of IP addresses are NAT (Network Address Translation) and CIDR (Classless Inter-Domain Routing), so I'll still have a "unique" location for my server so it will be secure.So if you try to set up the server in a different region, it will work, but I won't be able to use the AMI file that they provided.The `public_html` directory contains all of the static files that you are serving up directly through Caddy when using it as a web service.The `services` directory is the place where you are going to install all of your web services once you build them.There are different records that are associated with a domain name.The most common ones are:
A record: This is the most common record.This is the record that you will use to map your domain name to your EC2 instance.It is used when you want to map one domain name to another domain name.MX record: This is used to map a domain name to a mail server.It is used when you want to receive email at your domain name.TXT record: This is used to store text information about a domain name.It is used for things like SPF records, DKIM records, and DMARC records.Caddy cannot use HTTPS with just an IP address, it needs a domain name to do that.Something that was a stumbling block for me was remembering the syntax of the ssh command.The `-i` flag is for the key file, and the `ubuntu` is the username of the server, which would change depending on the server.Prof. Ventura mentioned that class may not seem very hard right now, which is fine, it'll get more difficult as time goes on.Make sure to take notes of things as I do the HTTPS assignment reading and the HTML readings.HTTPS, or Hypertext Transfer Protocol Secure, is a protocol that is used to secure the communication between a web server and a web browser.It is an extension of HTTP, or Hypertext Transfer Protocol, which is the protocol that is used to transfer data.HTTPS uses encryption to secure the data that is being transferred.This encryption is done using SSL, or Secure Sockets Layer, which is a protocol that is used to encrypt data that is being transferred.HTTPS is used to secure sensitive data, such as credit card numbers, passwords, and other personal information, that is being transferred between a web server and a web browser.Web certificates are generated by trusted 3rd parties, and then given to the owners of a domain.The certificate can then be served from the web server, and those who want to access the domain can do so using the certificate and the public/private key encryption system.Web certificates used to be very expensive to get, and there was quite a monopoly on them.This monopoly was busted by 2 Mozilla employees when they started a non-profit called Let's Encrypt.Let's Encrypt dynamically provides and renews free, credible web certificates to anyone who wants them, using their IETF standard ACME protocol.This has made the internet a much safer place, because now anyone can have a secure website.Caddy uses Let's Encrypt to provide HTTPS certificates to websites.It does this by automatically requesting a certificate from Let's Encrypt when the website is set up, and then automatically renewing the certificate when it expires.This makes it very easy to set up a secure website with Caddy.To implement HTTPS on my website, I need to do the following:
1.Set up a domain name for my website by leasing it from a DNS, such as Route 53.Create an A record in the DNS that maps the domain name to the IP address of my web server.In the EC2 instance, with Caddy installed, edit the Caddyfile to include the domain name.Do this by editing the section that says `:80{` (meaning port 80) to be the domain name (`mnemonicstudy.com{` in my case).To edit the color of a `color` input element, use the `value` attribute to assign it a hexidecimal color value.According to Prof. Ventura, the best resources are MDN Web Docs and W3Schools.Attributes: `src`, `controls` creates a visual representation of the audio, `autoplay` beings playing as soon as the `audio` element is rendered (strongly discouraged), `loop` replays the audio when finished, and others.Attributes: `src`, `controls`, `autoplay`, `loop`, and others.These have the same functions as the corresponding attributes in an `audio` element.Another one is `crossorigin`, which is used to specify how the element should handle CORS requests (cross-origin resource sharing).This is needed when the video is hosted on a different domain than the webpage.SVG is a vector image format that is used to create graphics that can be scaled to any size without losing quality.SVG images are created using XML, and they can be manipulated using CSS and JavaScript.SVG images can be used to create logos, icons, and other graphics that need to be scalable.The HTML element itself creates the canvas, but JavaScript is needed to actually draw on it.Don't worry about making the pieces look like the finished product, just make sure that all of the pieces are present in my HTML.In the Simon example code, most of the pieces don't look or behavior _anything_ like the final product, but they are all there.Some of the most common ones are: `:hover`, `:active`, `:focus`, `:visited`, and others.Use a `@font-face` rule in the CSS file, and in the `src` property, set the value to the url.Use and `@import` rule in the CSS file, and in the `url` property, set the value to the url of the font file.This is for if you are using a font from a font service, like Google Fonts.The `animation-name` property is used to specify the name of the animation, and the `animation-duration` property is used to specify the duration of the animation.These are placed on the elements which will have the animation applied to them.Then, the `@keyframes` rule is used to define the animation, with the name of the animation matching the value of `animation-name`.The `from` and `to` keywords are used to define the starting and ending states of the animation.Here is an example of a simple animation:
> The following is courtesy of Copilot lol.To remember the order of the properties in the CSS box model, from outside to inside, you can use the mnemonic:
**"My Big Purple Cat"**
**M**argin
**B**order
**P**adding
**C**ontent
Review of `display` property values:
`block`: The element will take up the entire horizontal space of the browser, and it will start on a new line.Grid and flex are a bit more complex, so here is some more info on them:
[Grid] (<https://www.w3schools.com/css/css_grid.asp>)
[Flex] (<https://www.w3schools.com/css/css3_flexbox.asp>)
Bootstrap is the most popular CSS framework, and Tailwind CSS is a newer one that is gaining popularity.All CSS frameworks are built on top of CSS, and they provide a set of pre-built styles that can be used to create a website.They are useful because they save time and effort, and they provide a consistent look and feel to a website.They are also useful because they make responsive web design much easier.The downside to Bootstrap is that sites styled with it are pretty uniform, so like all programming tools, it is not an end-all-be-all.Bootstrap is far and away the most popular CSS framework, but its major success is also its weakness: Bootstrap styling has become the de facto look of the web, so using Bootstrap can deliver a reliable and comfortable user experience, but it won't be able to stand out and be unique.Tailwind CSS is the next closest framework as of recent years, emerging in the last 4 years.It allow for different "looks" and avoids having a single distinct style.Bootstrap is a mobile-first framework, meaning that it is designed to work on mobile devices first, and then scale up to larger devices.It can be used by downloading the files and using a package manager, or by their content delivery network (CDN) Here are their official CDN links:
CSS: <https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/css/bootstrap.min.css>
JS: <https://cdn.jsdelivr.net/npm/bootstrap@5.2.3/dist/js/bootstrap.min.js>
In Caddy, different from Apache, the `publichtml` directory is the root directory of the website.This is where the `index.html` file is located that gets displayed when no subdomain is specified.The HTML for any subdomain is located in the `services` directory.Remember when using the `deployFiles.sh` script, to be in the directory that you want to deploy.The colors for Bootstrap is present in their CSS using a Sass map.If I want, I can make my own Sass file and customize the colors to my liking.Or, I can just use other assignments for different purposes, like not using the 'danger' color just for warning components.When debugging CSS, the `Styles` tab in the browser inspector is my best friend.It will only show the styles that are being applied to the element that I have selected, so I need to select one first.In `Styles` I'll see all of the styles applied, and then the ones that are overridden will be crossed out.I can also add new styles to the element to see how they will look, or toggle styles using the checkbox next to them.The `Computed` tab will show me the final styles that are being applied to the element, including the ones that are inherited from the parent elements.UX has become a complex subject, and in the setting of designing a good, mature application, serious consideration must be given to UX.Includes `click`, `mouseover`, `keydown`, `load`, `submit`, and others.Handling an event is done by executing certain code when the event happens.Listening for an event constitutes observing a specific element for a specific event, and then executing code when that event happens.The `debugger` command in JS will also function as a breakpoint.Other formatters are %d or %i for integers, %f for floating point numbers, %o for DOM objects, and %O for JavaScript objects.Objects that are instances of a class and have methods, such as String, Array, and Date, and others.Objects that are defined with key value pairs, such as `{name: 'John', age: 30}`, and don't require instantiation.One of the core things that I've noticed as I've used Bootstrap for this is the placement of most of the elements into some type of container, referred to as `components` in their docs.A lot of the stylistic choices include choosing the right component for the job.Also, alot of the box model classes are similar to the way that they are in Tailwind.The `p` classes are for padding, and the `w` classes are for width.The `bg` classes are for background color, and the `text` classes are for text color.Other common ones that I found were `fw` for font weight, `fs` for font size, `text-center` and others for text alignment.I'm thinking that to achieve my specific needs and ideas, I'm going to have to use a combination of Bootstrap and my own defined CSS.My idea for the flashcards is to have a flashcard class that has a animation defined for it that will flip the card and toggle the visibility of the corresponding side.The `input` element has an attribute called `placeholder` where the string value that you give it will appear as placeholder text (which disappears when something is typed) in the input field, when the input type is `text`.Like Tailwind, Bootstrap has some classes that can apply generally to everything, like `my-` for margins on the y-axis.Most of these classes are 'utility' classes, and can be found in that section of the docs.The Array object has several interesting static functions associated with it.Falsey and truthy values are only functional with the loose equality operator, but they do work with comparison operators.With a switch block, remember to use the `break` keyword to prevent the code from falling through to the next case.An anonymous function is one without a name in the function declaration.They can be called using a name if they are assigned to a variable, however.Unlike other languages, Js allow parameters to be unused and the function still called, that parameter will just be `undefined`.With arrow functions, a return nor curly braces are needed if the function is only one statement.When a function is returned from another function, the returned function is called a closure.The closure has access to the variables in the outer function, even after the outer function has finished executing.There are `for of` loops which iterates through each value (the iterator needs to be instantiated) and `for in` loops which iterates through each key in an object.A **spread** operator, `...` is used to spread, or add, the elements of an array into another array as individual elements, not as a nested array.Here is the link to the class [Javascript Demo](https://htmlpreview.github.io/?https://github.com/webprogramming260/.github/blob/main/profile/javascript/introduction/jsDemo.html)
Modern web application have very few pages.This is because rather than making lots of separate files, the DOM is manipulated using JavaScript.At some point, I want to try this, maybe during Christmas break.Remember that using the DOM, a new element is created using `document.createElement()` method.This only creates an instance of an element object, however, it isn't attached to the DOM.To attach it to the DOM, use the `appendChild()` method of the parent element.Browser rednering is single-threaded, so typical sequential execution can cause the browser to hang.They are used to handle asynchronous operations in JavaScript.Promise
: an **object** that represents the eventual completion (or failure) of an asynchronous operation, and its resulting value.Asynchronous operating allows the program to work on other tasks while waiting for another part of the program to finish.It is like when I go and do another task while waiting for something in the oven to cook.W3Schools talks about the idea of "producing code" and "consuming code".The producing code is the code that takes some time, and the consuming code must wait for the results of the consuming code.Anatomy of a Promise:
`new Promise(executor)`
Where executor is a function to be executed by the constructor, which should take two functions as parameters, `resolve` and `reject`.The `resolve` function is used to return the result of the promise, and the `reject` function is used to return an error.When the `resolve` function is passed a primitive or object that can't have `then` called on it, it is instantly fulfilled.A promise also has 3 important methods: `then()`, `catch()`, and `finally()`.The `then()` method is used to handle the fulfilled state, the `catch()` method is used to handle the rejected state, and the `finally()` method is used to run code after the promise has been settled.Using this is referred to as the "promise chain" or "promise syntax".In this syntax, the code here in the promise chain is the consuming code.The `async` keyword is used to define an asynchronous function.An `async` function always returns a promise, and the value of the promise is the value that the `async` function returns.The `await` keyword is used to pause the execution of the prefixed function until the promise from the `async` function is settled.This is because `await` will pause the execution of the function until the promise is settled, and if the function is nested, then the parent function will also be paused.If a nested function needs the `await` keyword, then the parent function needs to be an `async` function.Basically `resolve()` is used to return the result of the promise, and `reject()` is used to return an error.I learned the "producing code" and "consuming code" way of thinking about promises is useful for me.When I tried to put the next promise inside of a function called by the `.then()` method, it didn't work.It only did when I placed the promise function directly inside of the `.then()` method.What I did learn is that Promises are objects that take in a function to produce something, and then with the promise syntax, the promise chain consumes the product of the producing code.In doing these assignments, I decided to read up on callbacks, since that kept being mentioned.A callback in JS is a function that is passed to another function as an argument.In JavaScript, functions are executed in the order that they are called.Hence the name callback, because a function can be defined and passed as a parameter, but isn't called until the function that it is passed to is called, and it "calls back" to the callback function.When using async/await, remember that the `await` keyword can only be used inside of an `async` function.Thus, async is only needed when I want to (1) return a Promise, or (2) use the `await` keyword.The method to get and the full year from a date object is `getFullYear()` and `setFullYear()`.The method for the month is `getMonth()`, but it is zero-indexed, so January is 0, February is 1, and so on.All of the Math operations beyond the primative operators are in the `Math` library.The JS ternary operator is the same as that in C++, `condition ?In a `switch` statement, the `default` keyword is what denotes a block that will execute if none of the cases are true.JS's version of `for in` loops is the `for (x of array)` loop.Modules
: A file that contains code that can be exported to other files.The `export` keyword in front of a function signfies that said function is to be accessible by .js files that are trying to import.The `import()` function is used to import a function, and the `from` keyword is used to specify the file that the function is being imported from.To import all of the functions from a modules, use the `*` wildcard.You might also be able to use `import 'module` to import the entire module, I'm not sure though.Node.js is a runtime environment that allows you to run JavaScript on the server.It is built on the V8 JavaScript engine, which is the same engine that is used in the Chrome browser.Some Node things to know:
**NVM** - Node version manager
**Node** - JavaScript runtime
**NPM** - Node package manager
Don't push the `node_modules` directory to GitHub, because of how large it can get.The `require()` function is used to include a module, passing in the name of the module as a parameter.Stuff from lecture to look up later (Web service functions):
`require()`
`createServer()`
`writeHead()`
`write()`
`end()`
`listen()`
React is a Web Development framework.Frameworks focus on the DRY principle, simplfying and bundling common patterns and components.They also strive to improve performance and increase device coverage.The three most used web frameworks are **React**, **Angular**, and **Vue**.It is a syntax extension for JavaScript that allows you to write HTML in your JavaScript code.React works quicker than native JavaScript by using a virtual DOM stored in memory.When a change is made to the DOM, React updates the virtual DOM, and then compares the virtual DOM to the actual DOM.If there are differences, React only updates the parts of the DOM that have changed, rather than re-rendering the entire DOM.React is not a page-centric or file-centric system like native HTML, CSS, and JS development.Components exist as either **functional** or **class** components.Functional components are just functions that return JSX, and class components are classes that extend the `React.Component` class.Class components are becomeing deprecated because of hooks, so avoid using them.When using a component in JSX, the component is used like an self-closing HTML element, with the name of the component as the tag name.Props are passed to a component as attributes, and they are accessed in the component using the `props` object.They are essentially parameters for the behind the scenes function of a component.Destructuring is a way to extract values from an object or an array and assign them to independent variables.A **hook** is a function that allows you to access and use the state of other React componenets.It takes an initial value as an argument, and **it returns an array with two elements**: the current value of the state, and a function that can be used to update the state.I can list them as a parameter inside {braces}, or uses the props object and refer to the properties of `props`, using the name of the prop passed in the component call.For example: `<Demo who = "James">` could have the `who` prop accessed in the component as `props.who`, or as `{who}` if that was how it was passed in.To do inline styling with React, use the `style` attribute in the component's return HTML.The value of the `style` attribute should be a JavaScript object with the CSS properties as keys and the values as values.The keys should be camelCase versions of the CSS properties.Anytime a change is made to the state or props of a component, that component's `render()` method is called, and the component is re-rendered, making the change visible.Another key to reactivity in React is making components return HTML with event listeners.Two that I used for the assignment were `onChange` and `onInput`.The latter is used for input elements, and the former is used for other elements.Something from the assignment that I didn't understand yet was the attributes used to get the value of the input element.What appeared in the code was `e.target.value`, and I'm thinking `e` was just the parameter used for the input data, but I'm not 100% sure, and I'll need to find out exactly what `.target.value` is.React routing is what allows the application to navigate between different "pages" without actually changing the URL or html file.The router finds the correct component to render based on the file path given, and then renders that component.Some important components for this: `BrowserRouter`, `Route`, `Routes`,`NavLink`, and others.The toolchain used to push a React app into production includes Babel (transpiling JSX, polyfilling) and Minify JS for compressing the JS files.An application called Vite is used to bundle the files, and then the application is deployed to a server.An important difference between JS and JSX syntax is that in JSX, the `class` attribute is replaced with `className`.This is because `class` is a reserved keyword in JavaScript.Things to look for in router assignment: relationship between `NavLink` component's `to` attribute and the `Route` component's `path` attribute.There are 3 things that need to be done to use React Router:
1.Just import the `BrowserRouter` component from the `react-router-dom` package, and then wrap my entire application in the `BrowserRouter` component.Following the standard nomenclature, This would look like wrapping my `<App />` component in the `<BrowserRouter>` component.Import the `Route` and `Routes` components from the `react-router-dom` package.Each individual `Route` component has 2 essential props: `path` and `element`.The `path` prop is the URL/file path to element in the `element` prop, and the `element` prop is a component to be rendered.It functions like an anchor element, and the path to the component to be rendered is passed in the `to` prop, not in an `href` attribute.Links are used outside of the `Routes` component, because only the `Routes` components get re-rendered when the URL changes, and we generally want nav bars to be static across pages.Routes are navigated by matching the `to` prop of the `Link` component to the `path` prop of the `Route` component.If I want to render a page/component for individual products, to keep the code DRY and avoid hardcoding, I can use a dynamic route using a colon followed by name for the dynamic parameter.Route priority is determined by rules similar to those that dictate CSS specificity.The more specific the path, and the less dynamic parameters, the higher the priority.Also the `*` character can be used as a wildcard to match any path that hasn't been matched by a more specific path.The big thing to remember is that, as long as I put the path name in the `path` prop of the `Route` component, and then the same path in the `to` prop of the `Link` component, the router will handle the navigation.Here are some notes from my studying React on W3 and porting my startup app to React:
The command to create a React app is `npx create-react-app my-app`.The command to start a React app from the root directory is `npm start`.Though parameters aren't required in arrow functions in ES6, the () are still required, so `() => {}` is the correct syntax, regardless of the presence of parameters.When destructuring from an array, use `[]` with a comma for each element to skip.The name put in the brackets is the name of the variable that the element will be assigned to.The spread operator `...` copies all or part of an array or object into a new array or object.Remember, the ternary in JS is the same as in C++: `condition ?This differs from Python, where the ternary is
  `value if condition else otherValue`.JSX syntax includes replacing where parenthesis would be used in JS with curly braces `{}`.And functions passed to event handlers are passed inside of curly braces `{}`.This is used to keep track of which items have changed, and which items are new.Something that I ran into often as I was porting was forgetting to import things into `.jsx` files.I'll need to get into the habit of doing that with every `.jsx` file I make and work in.Describe using Context API to manage state between App, Study, and Flashcard components.One of my biggeset takeaways from this stage of the startup was that it took much much longer than I anticipated or gave it time for.I think experiencing this will serve as a good learning step for me.In the future I'll devote a lot more time to the design and planning to also get a feel for how long a project like this will take.One challenge that I encountered was comparing object equality.Remember that data is stored in localStorage as JSON, so it needs to be parsed when it is retrieved, and stringified when it is stored.Potential rework would include rendering flashcard, quiz, and respective edit components in the study component, rather than separate URLs.A router is a special kind of computer, so routers connecting to routers is how the internet works.It is used to connect a computer to the internet using the infrastructure of telephone lines.Fetch is a function that is used to make network (HTTP) requests in JavaScript.It is used to get data from a server, and it returns a _promise_ that resolves to the response from the server.The `fetch` function takes a URL as an argument, and it returns a promise that resolves to the response from the server.The response object has a `json()` method that is used to parse the response as JSON.The `json()` method also returns a promise that resolves to the parsed JSON data.Because of the promise returned, the `fetch` function is asynchronous, and its results are used with the `then()` method to handle the response from the server.The default method for a fetch call is `GET` but `POST` can be used by using the optional argument.Vite is a toolchain for web frameworks that bundles code quickly, and allows for features like minification, JSX, polyfilling, and others.It is used to bundle the code for a React app, and then deploy it to a server.This is implemented for my startup in the deployment script.Vite is installed using npm, and the commands `npm run dev` and `npm run build` are used to make development and production builds, respectively.After some more reading, the class notes for Express and middleware were the most helpful for me.Using `require('http')` returns an object that can be assigned, with methods.The major one is `createServer()`, which takes a callback function as an argument.The callback function takes two arguments, `req` and `res`, which are the request and response objects, respectively.The `req` object contains information about the request, and the `res` object is used to send a response to the client.The `createServer()` method then returns a server object, which has a `listen()` method that is used to start the server listening on a specific port, by passing in the port number as an int, and an optional callback function.This is code on the server that can be triggered/interacted with via HTTP requests.When an endpoint cannot be found is when a 404 error occurs.Express
: A Node.js module used for manipulating and routing HTTP requests, generating HTTP responses, and using middleware to increase functionality.Express works similarly to managing HTTP with Node, but rather than creating an `http` object, an `express` object is created, which has `get()` and `listen()` methods.The `path` argument is optional, and defines the path of the endpoint to trigger the callback on.Functionalities are added in Express middleware using the `next()` function, which is used to pass control to the next middleware function on the stack.When no more middleware functions are left, the response is sent to the client.Express allows variables to be passed in the URL, and these variables are accessed using the `req.params` object.The `req.params` object is an object that contains the values of the variables passed in the URL as attributes.PM2 is a process manager that uses daemons to keep node services running.Terminal commands can be used to do various things related to PM2 on a server where it is installed.Dr. Ventura reminded us that having a big picture mental model of the system/project is super important.It is critical to understanding how it should work and what architecture it should have.To debug the backend code, it can be a bit trickier, since it technically runs on a computer in Virginia.Luckily, Vite allows for debugging by proxying into the port that the backend is running on.This is done using a `vite.config.js` file, where this port is specified.This file is placed in the root directory of the project, and frontend code can be stepped through in the browser dev tools as normal, and the backend code can be stepped through in VSCode.A good way to think of a closure is when a function is returned and all of its state is returned and accessible as well.Cross Site Request Forgery (CSRF) is a type of attack that tricks the user into performing an action that gives attackers access to their information, by using a fake website to send a fetch request to the real website, and access personal data.Cross Origin Resource Sharing (CORS) is a security feature that restricts what resources can be accessed by a web page from another domain.So, in this example, the Wells Fargo would only allow requests with an origin of their own domain.Single Origin Policy (SOP) is a security feature that restricts what resources can be accessed by a web page from another domain.I'll link to the files [here](../notes/cs260.github/profile/webServices/dataServices/dataServices.md).For accessing data that is user-specific, Dr. Ventura recommended creating a collection for each username, and only access the collection tied to that username.Rainbow tables are a type of attack that uses precomputed hash values for common passwords to crack hashed passwords, and then map those hash values to the original passwords to find matches.My understanding of endpoints is you define a path/URL that triggers a certain function when visited.Then, on certain components on the frontend, when I want to access that data, I use a fetch statement that is connected to that component to get that data.Design overview:
**Objectives:**
Create user objects and store them in memory of the backend service.Store the study objects of each user in the backend service.Create user objects and store them in memory of the backend service.Create a User class with a constructor that takes a username, password, and study materials as arguments.Create a set of users to store the user objects in memory and disallow duplicates.Create methods for adding users and transferring data from the backend to the front end.Create Login and Authentication components, add those routes to my frontend.For both of these, copy the Simon code, since Dr. Ventura encouraged us to do so.Make sure I read and understand all of that code as I do so, however
3.Store the study objects of each user in the backend service.Create an endpoint for updating the User object in the Users set with the study materials.Create an endpoint for getting the study materials of a user as they are updated on the frontend.Create an endpoint for getting flashcard suggestions when using the Help button on the front end.I'm going to start with the 2nd objective, since it seems to be a bit of a prerequisite, and should be the easiest.After having Micahel help me, I got a few things cleared up mentally.The Vite server simulates the front end, serving all of that up.The port that the Vite server runs on and the backend port need to be different.The port listed in the `proxy` object in `vite.config.js` for a particular endpoint is the port the backend is running on.Remember that, as far as my understanding, a node package needs to be installed in the same directory as the file it is being used in.Because I needed a consistent data structure for Decks between the front and backend, I got to tackle the task of sharing the classes between the front and backend.I made the Card and Deck classes into separate `.js` files, and then put them in a `shared` directory in the root of the project.Continuing from the last point, I found some trouble with importing the classes into both the frontend `.jsx` files, and the backend `.js` node files.I learned that the `import` statement is used in the frontend, because `.jsx` files use ES6 modules, while the node.js backend files use the CommonJS syntax for including modules, which is done with the `require()` function.So, to make this work for the full stack, I changed the `type` of the node app to use ES6 modules.I did this by adding the line `"type": "module"` to the `package.json` file.This allowed me to use the `import` statement in the backend files as well.However, then I had to change and update the `require()` statements for the other node modules I was using like `express` and `uuid`.I was able to do this with identical functionality by doing `import express from 'express'` and `import { v4 as uuidv4 } from 'uuid'`.Some important Object methods that I learned about are `Object.keys()` and `Object.entries`.I ran into a challenge with `Object.entries()` where I thought just the entry was in the array, I didn't know it was an array of arrays with the array index and the entry.So I just had to alter the code slightly to access that data correctly.It takes 2 parameters, a function to run, and an array of dependencies.The function runs when the dependencies change, after the DOM updates with those changes.The passed in function returns a cleanup function, which is used to clean up the effect before the next DOM update.Understanding this hook and using it properly was extremely useful.After taking a look, I think I am going to have to move around my shared Card and Deck .js files, or experiment with editing the deployment shell script so that the build version can access those correctly.Dylan helped me find the error that I had in accessing the Object passed from the backend to the front in the HTTP response.I was calling `res.json({ "userObject":user})`, which was returning a nested object, with an attribute of `userObject` that contained the object I had stored in `user`.So by instead just using `res.json(user)`, I was able to access the object directly in the front end.The class repo notes on Troubleshooting 502 errors was really useful.One of the most helpful things was remembering that I can run the backend script on the server itself, and see what errors are being thrown, `node index.js`.Get correct data handling for exisitng pages (flashcard, flashcard edit)
Main implementations for Startup Login:
Cookies
Store decks in DB now, can keep front end methods the same.Notes:
To safely establish a database connection, import `MongoDB` library in to your backend file.Then put your database credentials in a separate .json file, _**and make sure that that .json file is in your .gitignore file, DO NOT commit or push those credentials**_.Then call the MongoDB method to establish a connection, using the info from the .json file.The data are stored as JSON objects, but also the queries to access the data are in a JavaScript-esque style.Each "table" is called a collection, and each individual entry in a collection is called a document.The object model of Mongo starts with the `MongoClient` object, constructed using a url that connections to the MongoDB server, which contains the username, password, and host name, such as this: `mongodb+srv://${userName}:${password}@${hostname}`
A `db` object is created from the `MongoClient` object, and then the `db` object, which is used to create and access `collection` objects.If a database or collection does not exist when called, then MongoDB will create it.This method takes an object as an argument, which is used to filter the results of the query.An example for a collection of rental houses would be `collection.find({bedrooms: { $gte: 3}})`, which would return all of the documents of the collection that have 3 or more bedrooms.The method for inserting a document into a collection is `.insertOne()`.This method takes an object as an argument, which is the document to be inserted into the collection.The method returns a promise that resolves to the result of the insertion.The [reading](../notes/cs260.github/profile/webServices/webSocket/webSocket.md) for this was pretty helpful.WebSocket connections move from client-server to peer-peer communication.The main difference in between these two is that client-server can only ever be initiated by the client, while peer-peer can have communication (after the initial) initiated by either end.On the backend, import the `ws` node module, and create and instance of the `WebSocketServer` class.The `on` method in WebSocketServer objects and other WebSocket objects, sets up a sort of event listener.So a WebSocketServer called `wss` with the method `wss.on('connection', (ws) => {console.log("Hello webSocket")})` Will log out that string on a client connection to the server.The `.on('message')` method will execute the passed in callback function when a client message is received.On the frontend, WebSocket capacities are built in, so you can simply create an instance of the existing WebSocket class.The frontend WebSocket object also has a type of event listeners, with slightly different syntax (`.onmessage()` instead of `.on('message')`).When WebSocket capacities are only wanted for some of the data traffic, a non-webSocket server can be made, as well as an instance of WebSocketServer, with the property `noServer` set to true.Then connections can be specified to be upgraded or not, and the backend can have event listeners for those upgrades.Because all of the Class instantiation happens on the front end, I don't need to worry about that on the backend at all.The main thing will be identifying which endpoints need to access data, and how to get the data from the DB for those end points.Data-needing endpoints:
Login endpoint
Create user endpoint
Logout endpoint
These will follow the same general structure as the Simon example, but slightly different because of the data structure.I think that the there is no way to use a mix of the two, only ever one or other other.I realized that because the file structure that led to the Card and Deck classes are odd, I just defined the classes again in the backend file, and because that was the main reason I was using ES6 on the backend, I found it easier to just use CommonJS for the backend.I also learned that because ES6 modules and Common JS use different methods of exporting, that a class or method that is being exported can only be exported via one of those two methods, not both.I had to understand `cookie-parser` a bit better for this one too.I learned that when the express app is using `cookie-parser` the `req` object has a property called `cookies` that is an object that contains all of the cookies that are stored in the browser for the site.The cookie-parser is what parses and populates this property of the request object.Remember that `bcrypt.hash()` requires the string to hash as the first argument, and the salt rounds as the second argument.The commond `lsof` shows what ports are being used on MacOS.I was running into a problem of trying to debug the backend index.js file, but it kept saying that the port was in use.I didn't realize that because the bash script runs `pm2 restart startup`, that the index.js file was already running using pm2.So I had to stop the pm2 process, and then run the index.js file in the terminal to debug it.That honestly should come earlier in the hierarchy before googling, because the info an questions in the Discord are specific to the class' context.I was able to find that other people had already had the exact same problem as me, with the server not being able to access the Database despite the IP address being whitelisted, and the fix was to change the MongoDB version to an earlier one.There are 2 ways to think of peer-to-peer communication: First, the server and the client being direct "peers" where their communication is bidirectional.Second, different clients are "peers" to each other via their common connection to the server.A client doesn't communicate directly to another client, but it communicates a message directed for a client to the server, and the server sends that message to the receiving client, with the server initiating the communication.This object is a WebSocket connection object that is created by a client connecting to the server.So, the WebSocket object is instantiated at connection, not by the callback function.If no dependencies are passed into a `useEffect()` hook, then all of the state variables of the component are its dependencies.To set no dependencies, pass an empty array `[]` as the second argument of the `useEffect()` hook.The WebSocket deliverable will deal alot with event handlers.On a different note, a popular platform for testing web apps on different devices is called BrowserStack.There is also UI and Endpoint testing, check the class readings for useful resources there.A good place to see notices about recent data breaches is on Tech.co, [here](https://tech.co/news/data-breaches-updated-list).Security is a massive concern that should always be considered.Some major types of security threats are:
Injection attack: When an attacker sends malicious data as part of a query to a server, and the server executes the query with the malicious data.Cross-Site Scripting (XSS): When an attacker sends malicious scripts to a server, and the server executes the scripts.Social Engineering: When an attacker tricks a user into revealing sensitive information.Denial of Service (DoS): When an attacker sends a large number of requests to a server, overwhelming it and causing it to crash.The "Line of Death"
Don't unconditionally trust anything below the URL bar, because it can be faked.OWASP, the Open Web Application Security Project, is a non-profit organization that provides resources for web application security.They have a list of the top 10 security risks for web applications, which is a good resource to check out, [here](https://owasp.org/www-project-top-ten/).Juice Shop is a web application developed by OWASP that is intentionally insecure, and is used to teach developers about web application security.One way to get some useful data like protocol, port #, and hostname is to use the `location` object in the browser.It is accessed as a proptery of the `window` object, so `window.location` will return an object with all of the data about the current URL.The location object has properties like `protocol`, `port`, and `hostname` that can be used to get the data needed to connect to a WebSocket.Here are ideas and insights that I got from people's demos:
The deck of cards workout app was a really cool idea.Abe said that API ninja is what he used to get the recipie data for his app.URLs to keep:
<https://startup.mymovieknight.com/>
<https://startup.subak.click/>
<https://startup.deckofdeath.click/>
<https://startup.homeworkhub.click/>
Test Review:
Port 80 is for HTTP, Port 443 is for HTTPS.Cookies allow for the server to store data on the client's machine.Remember that `fetch` can be used on the frontend and the backend
As I was working on MnemonicStudy a bit today (19.12.24), I did a little more digging and learning about the Context API and how it works.The [react docs](https://react.dev/reference/react/createContext) are pretty useful.What I learned is that a more standard way to include dynamic values in the context object being used is to wrap all the components in a `<SomeContext.Provider>` tag, rather than making a separate function called SomeContextProvider, like I was doing before (I think what I was doing before was something generated by an LLM anyway).The `<SomeContext.Provider>` tag takes a `value` attribute, which is the object that is being passed to the context.So, I simply set a state variable and setter that had the same default value as the context's default value, and passed the state variable and the setter as the `value` attribute of the `<SomeContext.Provider>` tag.This is essentially the same as what the provider component functions were doing before, just a different way of packaging it.I had a weird issue while working with Modals on the quiz page.When I was trying to handle the show state of the modal, rather than just using the setter that I associated with the state variable for the click event listeners, I had to define seperate handler functions that used the setters from the state variables, and for some reason those handlers that I defined work, but using the setters directly does not, as the console says that causes an infinite loop.I'm curious to learn why the handlers do not cause an infinite loop.I had to relearn the difference between `name` and `id` attributes in `input` elements.So, when form data is sent to the server, the keys for the values taken from the input elements are the `name` attributes of the input elements.And I've confirmed that for the flashcards, the editing fields to get correctly re-rendered when advancing or retreating cards, so I just need to find out why the quiz edit page is not re-rendering correctly.An input field that uses `defaultValue` is uncontrolled, while fields that use `value` are controlled.The problem with uncontrolled fields, or at least those that use `defaultValue`, don't update when the state changes.Use `value` instead of `defaultValue` for the input fields, and use an `onChange` event listener to update the state when the input field changes.Use a `key` attribute in the input field that interacts with state variables, when using `defaultValue`.When I select one of the radio options, that selection persists across all of the questions.So, if I select B on the first question, the next question renders with B already selected.Then, if I select A on the second question, then go back to the first question, A is now selected.I'll need to find a way to make the radio buttons independent of each other.So, I needed to develop some way to store the selections throughout the quiz.It had been so long since running this that I forgot how to run the backend and the vite server lol.These are such basic things that I am documenting them:
To run vite the command is `npm run dev` in the root directory of the project.The `vite.config.js` file is used to configure the vite server, such as the port it runs on, and the proxy attribute for the backend port to connect to.Running a backend that uses node.js is done using the command `node <file path>` using a path that leads to the main backend file, generally something like `index.js`.I'll be honest, there is a lot of stuff to do to make this project better.Here is a small outline:
[ ] Clean up file structure and architecture of project.This is all a lot of stuff, so I am going to try and focus on one at a time and take some small steps to keep myself motivated to keep working on the project.First steps:
[ ] Message michael about his suggestions for file structure and architecture.Michael also suggests **motion.dev** for animations, [Docs](https://motion.dev/).Not including a version will use the latest version installed.Use `--no-errors-on-unmatched` to suppress errors on unmatched files.Typescript enforces added protection against errors caused by potential null values, usually giving a message along the lines of "`object` may be null".This can be overcome using the Null Assertion operator, `!`, Optional Chaining `?` (won't try to access attributes if an object is null), or Type assertions, `var as MyType`.As I worked on changing the title card component `DeckCard` to receive props from the `study` component, I learned that each prop needs to have the correct type annotations.I also learned that declaring the type annotations can be done a bit cleaner than the all-in-one line method by using an `interface`.TS interfaces work similarly to Java interfaces, but they are more to define the structure and types of a class, functions, or array, rather than the method signatures.Here is the what I used in `deckCard.tsx`
I was working with toasts to test them out today, and wrestled with them for a bit.I learned that on top of the shad/cn component, I had to install the toast library `sonner` and then put a `<Toaster/ >` component in the `__root.tsx` file of my app.I worked on changing the main Flashcard component to modularize it a bit more.This way I can pass the study contents to it, or the edit contents to it, instead of duplicating it for both.The tricky part was that the children can't be distinguished between, if there is more than one passed in then the parent component simply gets an array of children.There is a `Children` API in React to map this array and access individual children, but React's docs discourage using it.So, instead of using children in the traditional way, I made a `front` and `back` attribute for the Flashcard component, and passed in components for the front and back, because the flipping animation wouldn't work with both in there as `children`.This maybe isn't ideal, but it beats all of that code duplication that would come from reusing the flashcard comp for an edit page but with slightly different inner workings.But, this did help me get exposed to `children` and how to use them in react, and how passing things as props also fulfill a parent-child role in relationships between components.Another obstacle that I had was getting the `cardIndex` state that I was tracking to match with the index of the Carousel map function in the shad/cn component.Apparently there is a `setApi` prop for the component that I was able to manipulate.The reason I decided to do this was because there is a method in this prop that allows me to set the direction back to forward facing when cards are changed, which was a really handy feature I didn't want to pass up on.Variables must be typed, but don't need to be declared with a var keyword like JS.Multiple variables can be assigned in one line, like `int x = 5, y = 6, z = 50;`.They can also all be assigned the same value, with assignment operators in between each variable.Type casting is done with `()` around the type, like `int myNum = (int) "0";`.When moving from a smaller primitive type to a larger one, it is called a Widening Cast, and is done automatically.When moving from a larger type to a smaller one, it is called a Narrowing Cast, and must be done manually.In Java, the length of a string is a method of the string (`.length()`), not a property or a separate method.Finding the index of the first occurrence of a char (or array element for that matter) is done with `.indexOf()`.When using `.indexOf()`, on a string, the char you are searching for is placed in "", not ''.Apart from the normal way with the `+` operator, Strings can be concatenated with the `.concat()` method.And because strings are immutable, concatenation does not alter any existing strings, but creates a new one.Escape characters are handled like normal, with a `\` before the character.Add the `-a` flag to `git commit` to commit all changes, without needing to add them first.This is a shortcut for `git add .` and `git commit -m "message"`.Java has a few main reasons for its use: **Portability**, **Object-Orientation**, **Garbage Collection**, **Maturity**, **Rich Libraries**, and **Large Community**.Remember, instance methods operate on actual objects of a class, while static methods operate independent from any instances of a class.This good for utility methods, or for declaring a "singleton" which is allocated to a class, not an instance of a class.Most classes in Java need to be constructed, which is done using the `new` operator.The `new` operator creates a new object of a class, and returns a reference to that object.For example, `new Random();` constructs a new `Random` object.Some objects are ready to use without constructing them, like `System.out` or `String`.An alternative way to produce an instance of a class is called the _factory method_.An example is `RandomGenerator generator = RandomGenerator.getDefault();`.This is a static method that returns an instance of the class.It is useful for testing small code snippets, and for learning Java.It is run by simply typing the command `jshell` in the terminal.This is why memory addresses start is `0x`, because they are hexadecimal literals.Some special floating-point values are `Double.POSITIVE_INFINITY`, `Double.NEGATIVE_INFINITY`, and `Double.NaN`.To check if a value is `NaN`, use the `Double.isNaN()` method.Special `char` literals include `'\n'` for newline, `'\t'` for tab, `'\b'` for backspace, and `'\r'` for carriage return.Sometimes the type can be inferred by the compiler, so instead of a type, the keyword `var` can be used.To mutate a string throughout a program, there is the `StringBuilder` class, which includes familiar methods like `.append()`.Because of string immutability, concatenating strings with the `+` operator can be really inefficient, so try to use `StringBuilder` when possible.Strings have plenty of useful methods, reference the reading or Java docs for more info.Some of the more useful ones include: `.length()`, `.charAt()`, `.substring()`, `startsWith()`, `split()`, `replace()`, and others.The static method `main`, which is the entry point for the program, takes in the `String[] args` parameter.This is an array of strings that are passed in as command line arguments.Accessing command line arguments is done by simply indexing the `args` array.This does not create an array, but a reference that can point to one.An array is created with the `new` operator, like this: `int [] intArray = new int[10];`.The array is initialized with values like this: `int [] intArray = {1, 2, 3, 4, 5};` (list initializer syntax) or after the creation of the array, `intArray[0] = 1;`.If values inside of an array are primitive, the values are stored directly in the array (directly in the allocated memory).If the values are objects, the values of the array are references to the objects, not the objects themselves.Multi-dimensional arrays are created by nesting the `[]` brackets.For example, `int [][] intArray = new int[3][3];` creates a 3x3 2D array.The same syntactical rules apply for declaration, creation, and initialization of multi-dimensional arrays.In Java, mult-dimensional arrays don't have to me matrices, or square in dimensions.They can have uneven lengths of rows and columns, referred to as a "ragged array".A package is a way to encapsulate code, specifically classes, into logical groups.The name of a package should match the directory structure of a project.All of the code in the same package has access to each other, because all of that code is in the `package scope.` Code that is available outside of the package is in the `public scope`.The `public` keyword is used to declare a class or method as public.If a class is not declared as public, it is only available to other classes in the same package.Packages can have sub-packages separated by dots (`java.util.date`).The package name becomes part of the class name when you place a class in a package.For example, a class named `Date` in the package `java.util` would be referred to as `java.util.Date`.To refer to a packaged class, you must use the fully qualified name, with the package name and its sub-packages, OR use the `import` keyword.The `import` keyword is used to import classes from other packages.This is done at the top of the file, before the class declaration.This provides a shorthand for referencing packaged classes, allowing reference using just the class name.The wildcard `*` imports all of the classes in the package, but it is not recursive, meaning it doesn't import classes from sub-packages.The `java.lang` package is automatically imported, so classes in this package do not need to be imported, and don't need to be referenced with their fully qualified name.When using 3rd party libraries, or utilizing code external to the current class, the `java` command needs to be given the classpath value, which is given with the `-cp` flag in the command.When specifying mulitiple classpaths in one command, separate each path with a `:` on Linux/Unix systems, and `,` on Windows systems.CLASSPATH is an environment vairable that lists all of the directories that contian .class files, package base directories, or other resources the application needs to access.IDEs like IntelliJ and Eclipse manage the classpath for you.Both are useful classes with methods that perform essential functions.A `Scanner` is an object that implements an interator, and has a method, `hasNext()` to return if a next object exists, and `.next()` to yield that next object.This is part of the reason why programs are compiled into byte codes that are run on the Java Virtual Machine (JVM).This allows Java programs to run on any machine that has a JVM installed.The `java` virtual machine launcher is invoked with the name of a class, with dots separating the package segments, and no extension.Java will internally check if the string already exists in the pool, and if it does, it will not create a new string, but instead return a reference to the existing string.By contrast, using the `new` operator will always create a new string object on the heap.For example, `int[] primes = new int[10];` creates an array of 10 integers, named `primes`, all initialized to 0.This second way of initializing an array uses what is called the "list initializer syntax", which uses the {} braces.In Java, the variables of a class are called the `fields` and the functions of a class are called the `methods`.One way of thinking about _Encapsulation_ is only exposing code on a need-to-know basis.This is done by using private fields, and **getters and setters**.Common practice is to make all fields private, and then create public methods to access and return the field values, and modify the field values as needed.Sometimes this will seem redundant, but the value becomes apparent when the field values or program behaviors become more complex.Sometimes the getters and setters are as simple as this:
When creating my own classes, it is generally a good idea to _overwrite_ a few of the built-in Java methods: `equals()`, `hashCode()`, and `toString()`.This is because the default implementations of these methods generally do not have the desired functionality.When overriding a built-in Java method, the method is prefixed with the `@Override` annotation on the line above.Overriding these methods is a good practice, because it ensures that hash codes are generated with all the fields being considered, and equality is being determined by all the fields as well.Overriding the `toString()` method is especially helpful for debugging.This reference is stored in the variable that the develolper passes in.To reference an object inside of itself, the `this` keyword is used.Generally, when referencing an objects fields or methods, Java will assume that the reference is to the current object without the `this` keyword, unless there is a naming conflict.Instance methods and variables are associated with and operate on a specific instance of the class.Static methods and variables are declared with the `static` keyword, while instance methods and variables are not.Then, the constructor will be called whenever the class is instantiated with the `new` operator.Constructors can be overloaded, meaning that multiple constructors can be written with different parameters.The constructor that is called is determined by the parameters passed in when the class is instantiated.Another common use of constructors is to make a "copy constructor", which takes in an object of the same class and copies all of its fields to a new object.If a constructor is written, the compiler will not write a default constructor.It is required when there is a naming conflict between a parameter and a field of the class.It is optional when there is no naming conflict, because Java can infer that the reference is to the current object.This is done by using the `this` keyword like a method call, typing `this()` and passing in the parameters for the corresponding constructor.This process, of calling another constructor in the same class, is called constructor chaining.It represents a group of constants, so the values are immutable.An `enum` is declared with the `enum` keyword, and the values are declared in a list, separated by commas.An example of an `enum` is:
  
  > The values of an `enum` can be accessed with dot notation.To access the values of the `Level` enum, you would use `Level.LOW`, `Level.MEDIUM`, and `Level.HIGH`.It is most useful to use an `enum` when you are working with a set of values that you know will remain constant.The standard order of elements in a Java class
  > Here is an answer provided by GitHub Copilot:
  > In the context of the notes in the file, "The standard order of elements in a Java class" refers to the conventional sequence in which different parts of a Java class are typically organized.This order helps in maintaining readability and consistency across Java codebases.In object-oriented programming, it is fairly common to create a class just for representing a collection of data.These instances of data field collections are referred to as `data objects`, and are fairly common in Java.Ideally, a data object should have the following characteristics:
Is immutable
Has getters for all fields
Overrides the `equals`, `hashCode`, and `toString` methods to match the fields of the object
This can be done with a standard class, but it adds to the amount of boilerplate code that is needed.Because of how common data objects are, Java introduced the `record` keyword, that can create something functionally equivalent, but much, much more concise.To create a record, use the following syntax:`record Person(String name, int age) {}` Record that are created this way will meet the three characteristics mentioned above, as they are immutable, have getters for all fields, and override the `equals`, `hashCode`, and `toString` methods to factor in all of the fields of the record.Polymorphism
: A blanket term in computer science for taking one object and morphing it to fit in to many contexts.In Java, this is accomplished primarily through _inheritance_, _interfaces_, and _abstract classes_.The central idea around an `interface`, is being able to define a class, and what its methods do, without defining how they do it.It is defining functionality without defining implementation.An interface defines methods that a class implements using a normal method signature, including the method's access scope, return type, name, and parameters.When a class implements, or uses, and interface, the class declaration uses the keyword `implements` such as `class MyClass implements MyInterface`.The class needs to declare and implement the methods defined in the interface.So, in the "implementing class" is where the functionality of the methods are actually defined.A class that implements an interface can implement other methods than the ones defined in the interface as well.If any of the methods defined in the interface are not implemented in the class, the class must be declared as `abstract`.When writing the implementation for the methods defined in the interface, some programmers like to put the `@Override` annotation before the method in the implementing class.This is not technically necessary, it will function fine without, but it means to add a bit of clarity to the code.When this is done, that variable can have a value that is of any of the subtypes of that interface.So, if a variable is declared as having the type `Collection` the variable can be assigned a value of any class that implements the `Collection` interface, such as `ArrayList`, `LinkedList`, or `HashSet`.There are built-in JDK interfaces, that certain classes implement, and the developer can create new classes that implement built-in interfaces too.This is done similar to writing a class, but with `interface` as the key word, instead of `class`.Here is a sample:
Later on, Interfaces were updated to allow the use of `static`, `default`, and `private` methods.This allows for methods to be actually defined within an interface.When a class "inherits" from another, the inheriting class is called the `subclass` and the class that gives things to be inherited is the `superclass`.A subclass is able to access and use code from a super class as if it was written in the subclass itself.In Java, the keyword for defining a class as a subclass of another is the `extends` keyword.An example:
In this example, the subclass, Dog, is able to access all of the code for the `Pet` class, and then define its own code.This perspective makes the `extends` keyword make sense, because the class `Dog` is taking all of the functionality of `Pet`, and then **extending** past that with whatever members are defined in `Dog`.If an implementing class only implements some of the methods defined in the interface, or if it defines additional methods without providing their implementation, then that class is an `abstract` class.Both the class needs to have a signature of `abstract` and the methods that are left unimplemented.Then, similar to implementing classes of interfaces, a subclass of an abstract class is what will implement the abstract methods.Similar to interfaces, any variable whose declared data type is that of an abstract class, can hold a value of any of its implementing subclasses.Lastly, the keyword `final` can be applied to method signatures or field declarations.If a method is declared as `final`, then it cannot be overridden by subclasses.If a field variable is declared as `final` then its value become immutable.Normally, errors are thrown to the caller of a method, so errors must move up the call stack, and generally only involve returning an error code.Instead, throwing exceptions allows specific _handlers_ to take over control when an exception is thrown.Really, all exceptions are thrown at runtime, but the objects of the `RuntimeException` class are not checked by the compiler.So `Runtime Exception` means that they are first _detected_ at runtime, makings its subclasses _unchecked_ exceptions.All other exception types are checked by the compiler, thus their designation as _checked_ exceptions.Any method that has the potential to throw a **checked** exception must have it declared in the method signature, after the method name.If an exception is thrown while any of the code in the `try` block executes, then control will be passed to the handler that is defined in the `catch` block.If no exception is thrown, then the `catch` block is skipped.The `finally` block is always executed, regardless of exception throwing.Often times a exception being thrown will prevent resources from being correctly deleted or closed from memory, which presents a clear issue.The syntax is the same as a try block, but before the curly braces `{}` any resources that need to be closed are declared, separated by semi-colons if there are morn than one.Once an exception is thrown **or** the try block executes successfully, the resources are closed.Example of these three:
Things can get a bit trickier if the `close` method for one of the resources is what actually throws an exception.If just the close method exception gets thrown, then in it thrown to the caller.If another exception is thrown, and then the close method, which is executing because of the prior exception, throws an exception, the close method exception can safely be ignored for now.When this second case happens, the first exception gets rethrown, and the second exception is added as a _suppressed_ exception.These can be accessed with an exception object's `getSuppressed()` method.Because the resources that can be passed to a `try-with-resources` block are limited to those of the class `AutoClosable`, the `finally` block is still necessary for other types of resources.Two important notes about `finally` blocks are 1, to avoid putting return statements in `finally` blocks, and 2, to avoid throwing exceptions in `finally` blocks.Because a `finally` block is always executed, a return statement in a `try` block will not be executed until the `finally` block is executed, so that return statement will be lost to the one in the `finally` block.Throwing an exception in a `finally` block will override any other exceptions that are thrown, and the exception thrown in the `finally` block will be the one that is caught, and the other exceptions will be inaccessible, as the suppression mechanism only works with try-with-resources blocks.Generally, just avoid any code that will alter the control flow in a `finally` block.As of now, I don't quite understand the why behind rethrowing, so I'll need to pay attention to that in lecture.From what I do understand, one purpose of rethrowing is to **chain** exceptions, which allows you to change the class of an exception to provide more meaningful information to the caller and/or stack trace.Some classes of exceptions have a method to get the cause to then display it, generally `.getCause()`.For those that don't have a cause as a constructor parameter, the `.initCause()` method can be used to set the cause of the exception (the argument passed to `.initCause()` must be a `Throwable` object, generally it is the exception that caused the current exception).When an exception is never caught, a _stack trace_ is given.This is a list of all the methods that were called up to the point of the exception being thrown.Most exception classes also have a `.printStackTrace()` method that will print the stack trace to the console.Along this vein, the Object class has useful methods that can throw exceptions, such as `.requireNonNull()`.If you see that this method is the latest on a stack trace before an exception, you can much more easily detect the bug.It also can take a message string as an optional 2nd parameter.Some alternatives to these methods can be useful too, such as `.requireNotNullElse()`.The `Collection` interface is the built-in method for implementing some common data structures in Java.Here is a chart that shows the inheritance of various types:
  !A useful method that the book mentions:
  > the method `Collections.nCopies(n, o)` returns a `List` object with `n` copies of the object `o`.That object “cheats” in that it doesn’t actually store `n` copies but, when you ask about any one of them, returns `o`.This is an example of one of the many useful methods of the `Collections` utility class, whose methods operate on any `Collection` object.It has some useful methods for getting and setting the first and last elements of a list, returning the list in reversed order, and other useful methods.The method is simply called `iterator()` and returns an iterator for that type.Like typical in most languages, 2 of the core methods for Java iterators are `hasNext()` which returns a boolean, and `next()` which advances the iterator.Whenever a "for-each" loop is used, `for (String element : collection)` What is actually being done is an iterator is being used under the hood to iterate until the end of the collection is reached.The `remove()` method of the Iterator interface removes the _previously visited_ element, not the one currently being pointed to.For more details, [here](https://docs.oracle.com/javase/8/docs/api/java/util/Iterator.html) is the doc for the Iterator interface.To do a conditional removal of an element, the collection method `removeIf()` is much easier than iterating through and removing.Part of this is due to the nature of iterators, in that the corresponding collection can't be mutated in between iterator instatiations, otherwise a `ConcurrentModificationException` is thrown.These are thrown to make sure that the iterator that you instantiate doesn't become invalid.Sets have no index, no concept of "this value is 'at' this location".There is also `LinkedHashSet`, which is a hash table combined with a linked list.A HashMap hashes the keys, which is efficient but traversal visits the entries in an unpredictable order.A ConcurrentHashMap allows safe concurrent updates
JSON is handled in Java using Google's open-source `GSON` library.To create a JSON string, you must create a `gson` object, which is done using the `GsonBuilder` object.Simply call `new GsonBuilder();` and then to create the string, just use the `.create()` method on the GsonBuilder object.The `.setPrettyPrinting()` method makes the JSON string more readable.To create a `gson` object for parsing, just create a new `gson` object, and then use the `gson.fromJson()` method with the needed arguments.The main methods used are the `.fromJson(jsonString, classType)` and `.toJson(object)` methods, which are used for deserialization and serialization, respectively.A brief history of Java:
A guy left Sun Microsystems because he didn't like it there.The boss, Scott Mcneely, went to some of his senior engineers and asked them to try an find the new hot thing in tech.James Gosling was one of the engineers, and their project led to creating Oak, a language designed for communication between devices.Mosiac and the web launched, and around 1994, the Green team (the group from Sun) realized they accidentally made a language that was perfect for the web.They changed the name to Java since they couldn't trademark Oak, and made the HotJava Browser, allowing dynamic content which was a big deal at the time.Netscape announced in 1995 that they would support Java, and that is kind of where it took off.Oracle then bought Sun in 2010, and also gaining Java in the process.Java uses references instead of pointers (pointers are evil according to Dr Wilkerson).Data types are the same size in Java, in C++ they can vary based on hardware.In C and C++, code needs to be compiled and linked, while classes are dynamically linked at runtime in Java.Compiled vs Interpreted code:
Compiled code is less portable, but faster.Java is a hybrid of the two, with the code being compiled by the same compiler, regardless of platform, and the same Java Byte Code is run on any hardware, and then each platform has its own JVM that interprets the Byte Code.This means that the Java Byte Code is compiled to machine code at runtime, and then that machine code is run.This allows the JVM to optimize the code for the specific hardware that it is running on.The JVM is also a Hotspot VM, meaning it can dynamically recompile at runtime, while other compilers can't do that.The reason that using a `PieceMovesCalc` interface, with a set of implementing classes, works slightly better has to do with serializing and deserializing the ChessPiece objects.Because ChessPiece is what will be serialized and stored, the move calculation logic doesn't necessarily need to be stored.So, if it doesn't need to be stored it is better to leave it out, so that serialization and deserialization is a bit simpler.So, making an interface that `ChessPiece.pieceMoves()` will call, and then having the implementing classes have the logic for the moves, this will leave the move logic out of the serialization process.Not all classes have main methods, and in fact most don't, as they are meant to specifically only be accessed from other classes.Primitive data types, refer to [Ch 1 Reading](./notes.md#key-things-to-understand)
For a lot of the other content in lecture today, reference [Java Basics Section](./notes.md#java-basics)
Although subclassing seems intuitive with `ChessPiece`, because there is already a way to determine the piece type, with the ChessPiece field `type`, subclassing would give a redundant way to determine the piece type, which is a code smell.And the reason to not put all of the behavior in switch cases in `ChessPiece` is the **Single Responsibility Principle**.This is a software engineering principle that dictates that each class handles one responsibility only.Remember that references and objects are different, and only the `new` operator creates objects, references are created with the `Date dt` syntax.The main takeaway from Dr. Wilkerson's spiel about the programming exam is that spending adequate time preparing is the key to passing.I'm going to do the whole 4 hours at least once as practice, even though it'll be time consuming.Contrary to the Java convention, the automatic getters generated for records simply use the name of the field for the method name, such as `.name()`, rather than `.getName()`.Records are immutable, but can have methods defined within them that go beyond the automatically generated ones.If you want to mutate a record, you'd have to create an entirely new record.What the readings and the slide say about exceptions in Java holds for the majority of languages, with a few nuances here and there.In Java, something unique to it is the Handle or Declare rule.This is, when a checked exception is found in compilation, the JVM forces a handler (try-catch block) to be made, or the method to be declared (in the signature) as throwing that checked exception.If one of these is not done, the JVM will not allow the program to compile.Dr. Wilkerson's advice is to not write handlers for _unchecked_ exceptions, because these types of exceptions are generally fixable bugs.Writing a handler for these unchecked exceptions can hide the bugs from the programmer and keep them from improving the program.Always create Exception instances on the same line that you throw them.The "collections API" refers to a group of classes and interfaces, mostly those shown in the chart from chapter 7 of the book.An important detail is that a `Collection` **cannot** store primitives, only objects.There are wrapper classes for primitives, like the `Integer` class, that can be used to store something functionally equivalent to primitive values.By "hiding" the specific type that you implement later, you can change it more easily later.He mentioned a common theme in Computer Science of "what you hide, you can change".Some interfaces not mentioned in the book:
**Queue:**
This interface is designed for holding elements prior to processing.Methods include `add(value)` to add to the end of the queue, `peek()` to view the first item but not remove it, and `remove()` to pop out the first item in the queue.Methods include `addFirst()`, `addLast()`, `peekFirst()`, `peekLast()`, `removeFirst()`, `removeLast()`
[Javadoc](https://docs.oracle.com/javase/8/docs/api/java/util/Deque.html)
Implementing classes
ArrayDeque (resizable array implementation)
LinkedList (linked list implementation)
The **stack** interface in Java is deprecated, because it does not work very well.Stacks are still a very useful data structure, and can be successfully implemented using a deque, using the corresponding peek, remove, and add methods to enforce a LIFO insertion behavior.This hash code is modulus divided by the length of the list storing items, and that result if the index for that item.If there is a hashcode collision, each element of the hash table (the list being inserted into) can be a list, containing all of the items that have a hashcode that results in that index.This needs to be overridden to compare equality by value instead.An object in a collection implemented with hash tables also uses identity by default for the Object.hashCode method.If `equals` is based on value, then `hashCode` should be too, and both methods should use the same fields in their calculations.An example for this class is when determining if the King is in Checkmate.Dr. Wilkerson recommends copying the board and trying a move when doing this.Shallow copy: Simply creates a new reference to the original object and its values (Not recommended usually).Deep copy: Creates an entirely new object with separate identity, but the values are made equal to those of the original.Example: makes new Linked List with new nodes, that have same values as original.Immutable objects don't need to be copied, can be referenced without worry.Copy constructors entail making a constructor that taking in an instance of the same class as a parameter.Making a `clone` method involve implementing the `Cloneable` interface, and overriding/using the `clone` method.Inner classes are useful for situations in which different classes need different implementations of an iterface.An example is the `Iterator` interface, and its implementation in different `Collection` types.The implementation for an `Iterator` in a `List` is different for that of a `Set`, and even each distinct type of `Set` will need a different implementation of `Iterator`.A good way to do this is by making inner implementing classes of `Iterator` where needed.Anything from the outer class that the inner class will handle must be passed to it.This makes the inner classes a bit easier to find and improves some code readability.It also allows the inner class to use local variables of the method, and in some cases allows for no constructor to be used.However, because the lifetime of the inner class's objects are usually longer than the lifetime of the local variables, there is a protective restrction, which is that a local inner class can only use local variables that are _final_ or _effecively final_, which means that they are not mutated if they aren't declared as final.This provides the same benefits as the other types, namely access to local variables and class members, but it is declared even closer to where it is used.Keep in mind that Inner classes were originally developed and added to Java for event handling, so that is where they really shine.They certainly have other uses, but that is their primary one.It can be thought of as focusing on SKUs, retail prices, inventory items, etc., before thinking of data structures, class and interface relationships, and algorithms to use.It doesn't work to design everything before doing any implementation, and niether does trying to implement without doing any designing.The best pattern to follow is to move naturally between both several times.Programming languages provide low-level classes for dealing with simpler tasks.Handling complex tasks in terms of just these built-in, low level classes would carry too much cognitive burden, so the developer creates higher level classes out of these lower level ones, to abstract out details and make complex systems more understandable.Abstractions generally represent real-world objects and their relationships and actions, whether those objects are internal to the program (`HttpServer`, `Database`), or a representation of an external object (`User`, `Car`, `Store`).Spiritual thought: Prof. Wilkerson had a friend and member of his ward that had some heavy questions about polygamy.His friend took a doubtful approach, was exposed to false information online, and discounted his existing faith because of faith he didn't yet have.He left the church and Prof. Wilkerson had to cut off contact with him.Prof. Wilkerson decided to find these answers, using a faithful approach.He started by studying the whole of the triple combination, focusing on anything about polygamy.Then he read just about every faithful and informed publication about polygamy.It is important to note that he didn't doubt his existing faith in the process.It is also important to note that he didn't get an answer any of his questions for 1.5 years.After 3.5 years he got about 4/5 answers, but even now he doesn't have all of the answers, but he still has a strong testimony.Includes the [InputStream](https://docs.oracle.com/en/java/javase/22/docs/api/java.base/java/io/InputStream.html) and [OutputStream](https://docs.oracle.com/en/java/javase/22/docs/api/java.base/java/io/OutputStream.html) interfaces, and a host of implementing classes like `FileInputStream`, `URLConnection.getInputStream()`, and others.I'll need to fact check, but I believe `Reader` and `Writer` are other implemeting classes, but I'll need to check.In any case, `Reader`s and `Writer`s are for I/O of characters/text formatted data.There is a 1-1 correspondence between `InputStream` implementing classes and `OutputStream` implementing classes, having a matching type for each one.Different Stream objects can be linked together to manipulate data during the entire read/write process.A `FileInputStream` can be connected immediately to a `GZIPInputStream` so compressed files can be read in and decompressed all together, and compressed and written out all together.These input and output streams with different features are called **Filter Input Streams** and **Filter Output Streams**.These are created by creating an instance of a `InputStream` or `OutputStream` implementing class, then passing that instance into the constructor for one of these Filter Streams.A standard in I/O is to use `BufferedInputStream` instances, instead of a basic `FileInputStream`.This is because a `BufferedInputStream` will go to the file/byte source, and grab a chunk of bytes at a time, storing them in an array.When all of the stored bytes are read in, the stream will grab another chunk.This prevents having to read from the file an excessive ammount.A note for `BufferedOutputStream` is that if _the last group of bytes to be written out_ is less than the size of storage in the `OutputStream`'s array, it may not get written.Streams can be converted into `Reader`s and `Writer`s using the `InputStreamReader` and `OutputStreamWriter` classes.The method `.hasNext()` returns a boolean if the end of file has been reached or not, and `.next()` returns the next token.Whitespace characters are the default delimiters, but custom delimiters can be passed in as a regex string to the `use.Delimiter()` method.Questions for phase 1:
When checking for CheckMate: First check if the king is in Check.Then get all of the moves for all of your pieces, and iteratively perform all of them and if just 1 gets out of check, then return false.There are three main ways to get out of check: **1** capture the threatening pass, **2** block the threatening piece's path, or **3** move the king out of danger.Most languages provide built-in JSON parsers, so I don't have to make my own.DOM Parsers
Convert the JSON text into a DOM tree, then traverse the tree to extract the data wanted
2.Stream Parsers
Tokenizers that return one token at a time from the JSON data file
3.Serializers / Deserializers
Use a libraries to convert JSON to Java Objects, and vice versa
Gson and Jackson are popular choices
Using `.class` returns a `Class Object` which is an object describing the structure of the attached class.The `@SerializedName("STRING")` annotation can be used before an object field to provide what the name of that field's key will be in the JSON.Certain aspects of each JSON property can be controlled, such as decimal point lengths.This is done using a variety of tools, but an essential is the `.registerTypeAdapter()`
Generic programming is a way to make classes and methods functional for a wider range of classes and data types.Simply stated, a **generic class** is a class that takes in type parameters.These are given in the class signature, inside of angle brackets, like this: `public class Entry<K, V>`.Example: `Entry<String, Integer> entry1 = new Entry<> ("JST", 2234);`.A method can be made generic inside of generic or regular classes.The type parameters are given in between the method modifiers and the return type in the signature.Example: `public static <T> void swap (T[] array, int i, int j){` Unlike generic classes, the type being used for certain type parameters does not need to be stated at all, and can be inferred.If a `Date` array is declared and then passed as an argument to this `swap` method, the compiler can infer that `Date` is being given for the type parameter `T`.When type parameters are employed in methods and classes, they can be utilized anywhere that a type declaration would be used, such as field instantiation, method signatures, and parameter specification.Type parameters in generics can also use bounds and wildcards.To add a bound, simply include it in the angle brackets, such as `<T extends Person>` or `<T implements Collection>`.A type parameter can have unlimited interface bounds, but only one class bound, and it must come first in the list of bounds, like this: `<T extends Person & implements Collection>`
Wildcards are used to accept the passed in class and any of its subclasses or parent classes.A **lambda expression** is a block of code that can be stored, passed as a parameter, and executed later.The syntax is the following:
Each lambda expression starts with a _parameter list_, which is comma-separated and placed inside of parenthesis.Next, the arrow token, `->` is required, followed by either a single line expression to execute, or a code block placed inside `{}` braces.If only a single parameter is used, parenthesis can be omitted.Type annotations are not needed on lambda expressions, because they can be inferred by the JVM.To create a lambda expression and pass it as a parameter or store it as a variable, it must be stored or passed as a data type that implements a **functional interface**.A functional interface is one that has **exactly** one abstract method.It can have any number of static or default methods, but there must be exactly one abstract method to be a functional interface, and for lambda expressions to be created with that type.The way that lambda expression work behind the scenes, is that the JVM looks at the type assigned to the parameter or variable containing the lambda expression, and will make an in-memory instantiation of that class, and the code given in the block part of the lambda expression will be the implementation of the functional interfaces abstract method.The design principles that phase 3 focuses on are **SRP**, **DRY/Avoid duplication**, and **Encapsulation/Information hiding**.One way to avoid duplication is to make a separate class for shared methods, and import and call to it.To encapsulate well, _keep class fields private_, and also name classes, methods, and variables in a way that highlights role, function, or behavior, not implementation.The reasoning here is because if you decide to refactor and use a different data structure, you have to worry about renaming as well to keep things clear and readable.The main components of the server are: **Model classes**, **Data access object classes (DAO classes)**, **Service classes**, **HTTP handlers**, **Request/response classes**.So, _ports_ are assigned to each network connected program on a device.So, using an IP address and a port, you can connect to a specific program on a device.Here is a list of some of the parameters to use with `curl`:
| Parameter | Description |
|-----------|-------------|
| `-X` | Specifies the request method to use (e.g., GET, POST, PUT, DELETE).Place it in `server/resources`
**Spark Java**:
This is an open-source framework for making Java web apps and APIs.It creates routes and handles HTTP requests, and uses Java Lambdas to do so.Simply import Spark, and then it uses methods for the HTTP methods.For example, to handle a GET request, use `Spark.get(<path>, <handler function>)`.Passing in the handler functions can be done in a few ways:
Most related to JavaScript, and the standard is to use a lambda.This uses syntax `className::methodName`
A third major option is making a routing class and calling the method.When matching routes, they are matched in the order that they are defined.Response: `body(...)` (sets response body), `status(code)` (sets the status code).This _must be done before mapping/creating routes_ to correctly serve the files.I need to study a bit more how the location and file path gets mapped.This is done using `before(handler function)` or `after(handler function)`.Filters can also receive an optional parameter of a pattern to restrict where the filter applies.Create a Maven project, add the dependency to the pom.xml file.Create a Gradle project, add the dependency to your build.gradle file.Pass Java request objects from handlers to Service classes, which should return Java response objects.It is best to create a separate class with fromJson and toJson methods instead of calling them from gson directly.Instantiate each class one layer above, ie, instantiate all of the service classes in the Server as fields, and instantiate the DAOs in each service as fields, to avoid making a new object with every single request.Areas of potential code duplication:
HTTP Handler classes
Service classes
Request/Result classes
DAO classes
5.Use the `java.util.UUID` package, and the `.randomUUID()` method to make authTokens.Get test app to work **first**
The autograder for code quality represents the minimum bar.They cover some of the biggest sources of bugs in programming:
**Strong Cohesion**: The behavior of each method should be very related and not overextended.If the method returns a value, naming the method after the return value is also acceptable.Methods should be able to have their behavior well-described in the name, but avoid too much abbreviating (make the names long enough).I ran into this in phase 0 and 1, making submethods and calling them inside different nesting levels is much better.Declare and initialize as close to where data is used as is reasonable.One of the keys is to **pick a style, and stick to it consistently**.First follow your organization's style, and if there isn't one, then follow the conventional style of the language.Put **methods** into "paragraphs", separated by 1 or more blank lines.In **expressions**, place spaces _around_ operands, operators, and parenthesis.Put separate **conditions** on new lines (even better is to put complex conditionals, or even just parts of them, into submethods).Only declare one variable per line, don't do multiple assignment.There are some cases where short variable names are okay, such as loop iterators, temporary variables, or naturally short names (x, y, z for coordinates).Variables and methods are generally in lower CamelCase, and Class names in Upper CamelCase.And constants are generally all caps, with underscores for spaces.Avoid characters that are hard to distinguish, like 1 and l. Avoid using "Dr. Seuss" naming.When you have to: remove non-leading vowels, or just use the first few letters of a word.Unit tests should be **fast**, **cohesive**, **independent**, and **unique**.In Phase 4 when I write Database unit tests, I'll have a couple other things to make sure I'm aware of.I'll need to put the database driver JAR file on the class path (I don't know what this means yet).I'll need to start each test with a pristine database, so that the **independence** characteristic can uphold.This can be done by re-creating tables before each test, or having set-up methods "rollback" the effects of previous tests.Another good guideline is to have 100% branch coverage on non-IDE generated code.Relational DBs are managed using Database Management Systems (DBMS).DBMS can have a couple of different models: Embedded or Client/Server.The **Embedded model** means that the database and corresponding files are stored locally, and programs will interact with DBs via local file access.In a **Client/Server model**, the program will make a network request to a DB server, which then accesses the local files of the DB.Interactive involves an end user utilizing a management console to directly query a DB.Programmatic involves a program using a DB API and DB driver to access a DB.A **primary key** is an attribute in a table that is used to uniquely represent each row.A **foreign key** is an attribute in a different table, T1, that stores the values of the primary keys of another table, T2.Example: the `Book` table has attributes for `Category` and `Genre`.The values in each of these constitute the primary keys in the `Category` and `Genre` tables.A primary key whose significance only exists in the context of the DB is called an **artificial primary key**.Conversely, a primary key that holds significance in the world outside of the DB context is called a **natural key**.When a table does not have any one attribute that adequately acts as a primary key, but 2 or more attributes, that when observed together can uniquely identify each row, create a **composite primary key** or **compound primary key**.Each SSN assigned to exactly 1 person, each person has 1 SSN.In these relationships, store a foreign key of the other in each table.The `Person` table will have a `ssn` attribute, and the `SSN` table will have a `person` attribute
One-to-Many Relationships
Examples: Categories.Each category has exactly one parent, but each category may have many child categories.Each state has many counties, but each county belongs to exactly one state.How to model: Rather than having an attribute for the "many" in the one table, to save space, just have a foreign key for the "one" in the table for the many.The `State` table _won't_ have a `counties` attribute, but each row(object) in the `Counties` table will have a `state` or `state_id` attribute.Many-to-Many Relationships
Examples: Books and Club Members.Each book may be read/owned by many club members, and each club member may own/have read many books.How to model: Have a **join table** (also called an _intersecting entity_) that contains 2 attributes, one that represents the primary key of each table in the relationship.Make a `Books_Read` table, which has a `member` attribute, and a `book` attribute.In an ERD, many is represented with a "crow's foot" and one is respresented with a straight line.Optionality in ERDs is shown with a circle next to the crows foot or line.Vertical Partitioning: Each class, abstract or concrete, is mapped to a table, with a foreign key referencing its parent.The command for creating tables is straightforward: `CREATE TABLE <table name> (fields)`
Deleting a table is done using the `DROP` command, which has virtually identical syntax: `drop table <table name>`.You can also add conditional clauses like `if exists`
When creating tables, you must specify the different columns of the table.When specifying the column you must include the name and data type, but can also include other useful modifiers like `not null`, which requires a value to be not null, or `auto_increment` which will automatically track and increment that value each time a row is added to the table.There are also modifiers to specify primary and foreign keys.To specify a column as the table's primary key, just use the keywords `primary key`.The syntax for specifying a foreign key is the following: `foreign key(<col name>) references <table>(<field of table>)`
**Foreign key constraints**
If foreign keys are not handled correctly, some problems can occur.For example, in the book club example, we have a table `members`, a table `books`, and a join table `books_read`.Imagine `memberID` is made as a foreign key in `books_read`, and is the primary key in `members`.If a member is deleted from `members` then there will be a number of rows in `books_read` that have a foreign key that relates to a row in `members` that no longer exists.Another issue can occur if the foreign key is updated in its primary key location, so `memberID` is updated in `members` to be something different, like `member_id`.Some configurations for foregin keys exist to help mitigate these bad effects.A foreign key specification can include `on update` and `on delete` clauses, followed by a number of modifiers like `restrict` (blocks the operation from happening), `cascade`(will propogate any changes to a primary key to foreign keys that reference it), `set null`, `set default`, and `no action`.The default behavior for a foreign key is `on update cascade on delete restrict`.This is common in scripts that set up databases, because SQL will throw an error if programmed to create a table that already exists, or drop a table that does not exist.The `*` wildcard character can be used to select all of the columns from the queried table.AND can be used in WHERE clauses to avoid Cartesian products and ensure join
Inner join is also useful, syntax is `INNER JOIN table ON cond`
transactions
sometimes you need multiple statements to pass together or fail together
default is each statement in sql is its own transaction
use BEGIN TRANSACTION to start then COMMIT or ROLLBACK TRANSACTION
JDBC stands for Java Database Connectivity.Here is the "order of operations" for Database Access from Java:
1.Close the database connection
To load a DB driver, there is a legacy method that involves `Class.forName(<fully qualified class name>)`.The class `DriverManager` and its method `.getConnection()` is passed a connection string, and both loads a db driver and opens a db connection.Make a plain string of the SQL query, then make it a Statement using the `connection.preparedStatement(<string>)` or `connection.plainStatement(<string>)` methods.I will almost always want to use a prepared statement, because they project against SQL injection attacks.Then, on the Statement object, use the `.executeQuery()` method.The javaDoc for `ResultSet`s is pretty helpful, but essentially it is a set that has a "cursor" pointing to a specific row, from which methods can be called to access the data.When updating, you can create the PreparedStatement and use `?` for all of the values to assign, acting a placeholders.Then, statment objects have a `.set()` method for each data type, that takes in the column index of that value, and the value to set it to.So for an int in the 1st column, you'd use `stmt.setInt(1, 34);`
Then, the `stmt.executeUpdate()` method executes the update and returns either a 1 or a 0.Sanitizing DB inputs and preventing SQL injection attacks is one security measure basic enough that I will be expected to always protect against it, and not doing so will get me fired from future jobs.Connection objections have methods, `.commit()` and `.rollback` that perform the corresponding DB actions.In practice, never hard code credentials into connection strings.Another method is to sign in with admin permissions first, then create a user that has permissions to access the database.The `DriverManager.getConnection()` method can also take a username and password String parameter in addition to the connection string parameter, which is another method to avoid hard coding credentials.Dr. Wilkerson found enough common code between his 3 SQL DAO classes that he found it practical to make a Parent SQL DAO class, so keep that in mind.Loading the DB does not create tables, so Dr. Wilkerson recommends making a `createTables()` method, and call it in the createDatabase method that is provided.I'll need to download the community server, and I can install a client shell to administer the DB via the command line.Here is a table of the essential client shell commands
| Command                   | Description              | Example                       |
|---------------------------|--------------------------|-------------------------------|
| mysql -u `<username>` -p  | Login to the shell       | `mysql -u root -p`            |
| help or ?There is also a `Bcrypt.gensalt()` method to generate a salt string for you.Then, to check passwords, use the `Bcrypt.checkpw(<password to check>, <stored hash>)` method, passing the password string to check, and the stored hash as arguments.A lot of these tips are things that I already covered in CS 202, but that I can use a good review of and implement more often.I am going to make a goal of **effectively using at least 1 conditional breakpoint, and one watch in the remaining Phases of the class**.A second goal: **commit to memory how to access breakpoint settings in IntelliJ**.Right click on a breakpoint and click 'more', or access through "Run > View Breakpoints"
Dr. Wilkerson's spiritual/life thought: When you feel yourself avoiding a certain task or project because of its difficulty and/or magnitude, as soon as you recognize that avoidance is when to start working on it.The things that I need to do for Phase 5 are namely:
Draw menus and handle input
Draw ChessBoard
Invoke Server API Endpoints (Just like in 260, send a HTTP request to the url)
Write tests
A few tidbits that can be helpful:
Use `System.exit()` to quit the program.When listing the games on the Client UI, don't just show the gameIDs.Since those are artificial primary keys, they shouldn't be shown to the user.Have the numbering of games on the UI be independent of the gameIDs, and start at 1.Dr. Wilkerson's recommendation for structure: Make a `ui` package, and make a `Client` class, which is responsible for drawing menus, and a separate class, `ChessBoard` for example, responsible for drawing the chess board.Also make the the `Client` class depend on the `ServerFacade` and a `Client Communicator` type of class.Bascially, DON'T mix menu drawing-logic and board-drawing logic.Create in Phase 5 the logic to look at each square and determine to draw a piece or not.This is more than what is required for Phase 5, but will help with Phase 6.Similar to the handlers being the only objects on the backend that deal with HTTP and JSON, this will be the only class on the frontend that deals with HTTP and JSON.This is accomplished primarily by using ANSI Escape Codes to set the text and background colors to certain colors.Many terminals can also display Unicode characters, which has Chess piece characters which can look a bit _fancier_.However, the chess piece characters might be wider than other charactes, causing them to misalign.To fix this, change a few settings in IntelliJ, described in the slides.ServerFacade - This will have the 7 methods for the corresponding endpoints
4.ClientCommunicator - Dr. Wilkerson recommends using this class to store the code for Client GET and POST methods, to maintain SRP.Then `ServerFacade` will simply call the GET and POST methods from this class when it needs them in those 7 endpoints.The Java class we'll be taught in this course for making HTTP client requests is `HttpURLConnection`.A different class that will be taught in the fall is `HttpClient`, which can be a bit easier, if I want to learn it later.We are learning `HttpURLConnection` because, historically, it has been the standard.An `HttpURLConnection` object is instantiated by casting a `URL` object with a connection.A `URL` object is created by passing a String into the URL constructor.Then, that `URL` object has a method `openConnection()` that opens a connection to that URL.Below is an example:
The javadoc on `HttpURLConnection` describes all of the methods and fields, but here are a few fundamental ones:
`.setReadTimeout(int milliseconds)`: sets the timeout limit, parameter is in milliseconds
`.setRequestMethod(String method)`: sets the HTTP method that the request will be using
`connection.connect()`: Connects to the URL that the connection was constructed with.The slides have a helpful set of "steps" to both an HTTP GET and POST interaction between client and server, consult that when needed
Rather than alternating between putting in and taking out lots of `println()` statements, logging is a better way to work with and handle errors.Logs provide needed info to devs, sys admins, and customer support reps, while not intimidating the user with that info.Logging is a universal programming idea, but implementation varies in each language.Loggers have certain levels that they are set at, and only errors that are the specified level or _higher_ (i.e.For example, `ConsoleHandler` will send log messages to the console, and `FileHandler` will send messages to a designated file.Further, each handler has a "formatter" which defines the format used to encode log messages.Configuring a logger is done 2 ways: **Programmatic Configuration** and **File Configuration**.The standard practice is file configuration, but programmatic configuration is the way to learn about configuring at first.Log config files use what is referred to as "properties notation" to define the configuration.Config settings include useful settings like how much data to write before **rotating** files, how many files to have in the rotation, and the logging format.Here are variables that can be used in the logging format configuration:
%1$ The date/time the message was created
%2$ The method that called the log method  
%3$ The name of the logger
%4$ The level the message was logged at
%5$ The message
%6$ The throwable
There are a variety of logging methods that add certain things to the log, and are extremely useful, such as logging method entry/exit, logging the throwing and catching of an exception.Because Sun took a long time to build logging into Java, an external library was made called `Log4J`.It became so popular that when Sun finally added logging, they essentially copied Log4J, but people liked it so much that they didn't switch, and Log4J is still more widely used.Defensive Programming is a style of coding that leads to more reliable code with less bugs.It hinges on 2 practices: **Making assertions** and **Parameter checking**.As I code, I frequently make asssumptions about the program state.The way that I program things will often contain implicit assumptions, such as, this parameter will not be null, this data will be sorted, this string will have this many words, etc.When these assumptions are incorrect, however, that leads to bugs in the code.And when these assumptions are only implicitly contained in the program, they are extremely difficult to identify as the source of the problem.This can be resolved by making these assumptions explicit in the code, rather than implicit.These `assert` statements are built in to Java, and function slightly different.You simply make an assertion about some value, and if the statement is true, the code will continue, and if false, and `AssertionException` is thrown and the program crashes.Example: `assert listToSearch != null;` or `assert isSorted(listToSearch);`
Sometimes, an assertion may call a method that takes a fair bit of time and resources to compute, such as determining if passed data is sorted or not.Running these assertions constantly is inefficient, so by default, assertions are disabled when Java code is compiled and run.Thus, assertions are mainly used for observation in development settings.To enable them in these settings, use the flag `-enableassertions` or `-ea` when compiling from the command line.When running in IntelliJ, edit the Run Configuration and edit/add "VM Options" and add one of these flags to enable assertions.When assertions fail and AssertionExceptions are thrown, do two things: **1**, examing your assumption represented in the assertion statement, and ensure that it is correct for that point of the program.Parameter checking involves ensurine that parameters are in the correct state when passed into a method.This catches errors closer to their cause and leads to easier debugging.Parameter checking is one of the ways that assertions are used.It can also be done with if-else blocks and throwing exceptions.Which method of parameter checking can be determined by whether or not the programmer has control over the calling code, or the code that calls the method and provides the parameters.If the programmer does have control of that code, assertions should be used.If not, throwing exceptions inside of if statements should be used.HTTP does have some workarounds for p2p, like _short polling_ (pinging the server repeatedly over and over), and _long polling_ (pinging the server less often over longer intervals of time).However, these workarounds are pretty inefficient, and have lots of HTTP overhead, among other shortcomings.Once ws is established, the client _or_ server can initiate communications.Is an upgrade of HTTP (its built on top of it)
Ping/Pong for detecting dropped conns.Methods with the `@OnWebSocketMessage` annotation _must_ be inside of a class with the `@Websocket` annotation.The library that we're using for WS Client side will make the initial HTTP request to upgrade for us.All I need to to is use a URI with the `ws` protocol specified.Before connecting and getting a Session, I must get a WebSocketContainer, and use the container's `.connectToServer()` method.Using `.Part` will break the String into arbitrary chunks, rather than keeping it as one string.What Dr. Wilkerson did for Phase 6 was rename his `ClientCommunicator` class to `HttpCommunicator` and then created a `WebsocketCommunicator` class.A main question of this phase is: how to get the WS message with the updated board from `WebsocketCommunicator` to the `ChessClient` class?In Dr. Wilkerson's method, he uses an interface called `ServerMessageObserver`.Then, the `WebsocketCommunicator` takes in a `ServerMessageObserver` parameter in the constructor, and then will only have access to the `notify()` method.The `ChessClient` will still be passed to the `WebsocketCommunicator`, but as an interface, avoiding a _bad dependency._ **Keep this technique in mind, it can be handy for avoiding bad dependencies.First, deserialize to a parent Message type that has "type" attribute, which will then allow you to determine the corresponding sub type.However, this method can be computationally expensive in larger systems.A better way to handler this for all system sizes is to **use a Gson type adapter**.This will essentially signifiy to Gson that when it encounters a certain type, it should use a certain adapter to deserialize it.Among the 3 types of type adapters that Dr. Wilkerson mentiones, the one he recommends for this use case is the Deserializer type adapter.RuntimeTypeAdapter and StandardTypeAdapter are faster, but more complex.Bad actors try to compromise systems in many ways, such as gaining unauthorized access to data (in rest or in motion), gaining unauthorized access to computers, or disable a system so it can't be used (DDoS).Security is an entire discipline, so this lecture will only focus on some essential topics.Verifying actions)
**Foundational Concepts:**
Secure communication with HTTPS
Secure storage of passwords (hashing)
Secure data storage
These have a few essential characteristics:
**One-way**: The input cannot be derived from the digest.A small change in input should produce a large change in the output.However, they have both been cracked and should not be used.Git uses SHA-1 for generating commit IDs, but it is not a security risk because the commit IDs are not used for authentication or integrity checking.SHA-3 is a newer family of algorithms that is not as widely used yet.That is the kind of mistake that can and should get you fired.Some problems with storing password hashes:
If two users have the same password, they will have the same hash.This is a problem because if one of those hashes is compromised, the other user is also compromised.Common passwords can be determined using a rainbow table attack.This is where a bad actor hashes some of the most common passwords, puts them in a table, and then compares the hashes of the passwords in the table to the hashes of the passwords in the database.If they match, they know that the password is one of those common passwords.This means that even if two users have the same password, they will have different hashes because the salt is different.This also means that rainbow tables are not effective, because the salt is random and not known to the attacker.Password hashing algorithms are designed to be slow and computationally expensive.The longer it takes to hash a password, the longer it will take to brute-force it.This is also why many authentication platforms will "throttle" the number of login attempts.This means that if a user tries to log in too many times in a short period of time, the system will lock them out for a certain amount of time.Current recommended password hashing algorithms include: _Argon2_, _bcrypt_, and _scrypt_.Because of salting, hackers have moved from determining the password from the hash to trying to get the plaintext password from the user.This is done through **1** phishing and other deceptive methods, and **2** keystroke loggers, which are programs that run in the background and record the keystrokes of the user.This is a problem because it is very difficult to detect, and can be used to steal passwords, credit card numbers, and other sensitive information.This is similar to hashing, but differs in some important ways.Encrypted data is eventually meant to be read and used, so it needs to be decryptable.Modern encryption algorithms are divided into two main categories:
**symmetric key** ("secret key") algorithms and **asymmetric key** ("public key") algorithms.However, larger keys also mean slower encryption and decryption.Symmetric key algorithms use the same key for both encryption and decryption.This means that the sender and receiver must both have the same key, and it must be kept secret.Examples include AES (Advanced Encryption Standard) and, historically, DES (Data Encryption Standard).AES includes 128, 192, or 256 bit keys, and a 128-bit "initialization vector".Other algorithms include Blowfish, and Twofish, which support 32-448 bit keys, or 128, 192, and 256 bit keys, respectively.Asymmetric key algorithms use two different keys: a public key and a private key, that have a special mathematical relationship.One key is used for encryption, and other key must be used for decryption.This means that the sender can encrypt the data with their private key, and only the receiver can decrypt it with the sender's public key.Examples include RSA (Rivest-Shamir-Adleman) and ECC (Elliptic Curve Cryptography).It is much slower than symmetric key encryption, and can only encrypt data less than the key size.One of the most common uses of public key encryption is to encrypt and a symmetric key, which is then used to encrypt more data in a faster way.This has made public key encryption **one of the most important inventions in the history of computing.A TLS (Transport Layer Security) handshake, the process of establishing a secure connection, involves a client and server exchanging random numbers and public keys, and ensuring they can decrypt each other's messages.Along with the public key, clients and servers send "certificates", to verify their identity.Certificates are granted by authorized third-parties, such as Let's Encrypt, DigiCert, and others.When you need a certificate, you generate a public/private key pair, keep secure your private key, and then send identifying info and your public key to a Certificate Authority.This interaction is called a Certificate Signing Request (CSR).Software devs will also at times create "self-signed" certs to use for testing and dev environments.SSL (Secure Sockets Layer)
How do CAs, or others that need to, create a digital signature that is unique and not easily forged?The sender creates a **hash** of the original data, and **encrypts** it with their private key.The sender sends the original data and the digital signature to the receiver.The receiver decrypts the digital signature with the sender's public key, and creates a hash of the original data.The receiver compares the two hashes(the hash of the original data, and the decrypted digital signature, which is an encrypted hash of the original data).If they match, the data is authentic and has not been tampered with.If they do not match, the data has been tampered with or the sender is not who they say they are.Digital signatures are used by all CAs to verify their identity.Bitcoin and other cryptocurrencies use digital signatures to verify transactions.Digital signatures are also used in many other applications, such as email and software distribution.Spiritual thought: Dr. Wilkerson got much more out of Isaiah by reading it more, not less.It is an "execution context" that allows the CPU to work on a program, track the exact point of execution that it reaches, than move to another thread and then come back to that thread later.This is done by splitting the program into multiple threads, and allowing the CPU to switch between them as needed.This can help program efficiency, and it can also cause problems too.Java has a `Thread` class that is used to create and manage threads.The `Thread` class has a number of methods that can be used to control the thread, such as `start()`, `join()`, and `interrupt()`.This is the most basic form of execution, and is the easiest to understand.The CPU will switch between threads as needed, but they are not running at the same time.This type of computing is done with a computer with a single CPU core, or a single CPU.This is what is done on machines with multiple CPU cores, or multiple CPUs.Thread synchronization is the process of coordinating the execution of multiple threads to ensure that they do not interfere with each other.This can be done in Java by using the `.join()` method from the `Thread` class, (or by using the `synchronized` keyword ~ CP).Thread synchronization bugs are some of the **nastiest bugs that developers run into**
Some processes need to use multiple threads fairly frequently, such as web APIs and other types of web programming.Rather than generating the overhead and cost of creating lots of threads, the practice of _thread pools_ involves initializing a number of threads before they are needed, and then storing them, using them as needed, and then reusing them later.If you need a new thread, look for a free one in the thread pool, and if there are no available threads, add the task to a "task queue".Then when a thread is free, check the task queue and assign any waiting tasks to the free thread.Java can utilize thread pools with the `ExecutorService` class.This class has a number of methods that can be used to create and manage thread pools, such as `newFixedThreadPool(num)` creates a pool of `num` threads that can't increase or decrease, `newCachedThreadPool()`, and `newScheduledThreadPool()`.You can **1**, implement the `Runnable` interface, or **2**, implement the `Callable` interface.The main difference between these two is that `Callable` can return a value, while `Runnable` cannot.Which to use will mainly depend on if you need a return value or not.These are bugs that are dependent on the relative timing of the threads (the results of the "race" between threads).This makes the execution, and thus the appearance of a bug, non-deterministic and potentially extremely difficult to analyze and debug.Because the uniqueness of these bugs, they have their own category, hence the distinct name.One way to write thread-safe code in a database context is to use multi-query transactions, rather than committing after each query.A critical section of code is a section that only one thread should be allowed to run at a time.Generally, race conditions appear because a critical section is not properly protected.This can be done by in Java adding the `synchronized` keyword in the method declaration, which will ensure that only one thread can run that section of code at a time.Synchronized code can cause **deadlocks** at times, which is when threads are waiting on each other and cannot proceed.This is done by using the `synchronized` keyword on an object, which will lock that object and prevent other threads from accessing it until the lock is released.This can be done by using the `wait()` and `notify()` methods, which will allow threads to wait for a certain condition to be met before proceeding.Getting and setting requiring multiple operations is what creates critical sections (if that code is running/runnable from multiple threads).With Atomic variables, since accessing and setting is done in one operation, there is no need for critical sections.Atomic variables are created using the classes in the `java.util.concurrent.atomic` package, such as `AtomicInteger`, `AtomicReference`, and others.View the Javadoc for more details on the classes and their useful methods.Although I don't explicitly use Thread objects in my code, the Spark library, and the Websocket library on the Client-side both use threads, which means that there _is_ potential race conditions.Command line builds are a way to automate this process, using tools like Maven and Gradel in Java, and others in other languages, like npm for Javascript.Command line builds are generally preferred to manual builds.Things like GitHub actions are a form of continuous integration, where changes are built and tested on a git commit.Something like pre-commit, which we used in 236, is similar.There is also continuous deployment, which is taking continuous integration a step further, and automatically deploying the code to production after it is built and tested.As I worked on this phase, I realized I had to pause fairly frequently and think about how to design the classes and methods.I've had to do that in past classes, but not as much and not as early.I think this is a good part of this class being a stepping stone for me.The methods used are a bit different than other languages, but still fairly intuitive.There is also the `.clear()` method and the `.size()` method.For iterating through a `HashMap`, use `.keyset()` to access the keys, and `values()` to access the values.I talked to the TA Michael for a little bit, asking him about my challenge with the HashMap's equals method in comparing 2 `ChessBoard` objects.I learned a few things:
A map would work fine for the ChessBoard, but it doesn't work great with the library that we use later in the project to serialize and store the ChessBoard objects in a database.This reminded me that when designing data structures, I need to keep a wider perspective, and remember all of the ways they will be utilized, as that can affect which design choice will be best.Python is one of the only languages where that is a built-in data structure, so just keep that in mind.The `equals` method of a HashMap works by executing the `equals` method of all of the keys and values in the map.So, if the keys and/or values have `equals` methods that need to be overridden to be more accurate, failing to do so can cause the `equals` method of the map to return inaccurate results.Storing the pieces of a ChessBoard in a 2D array may work better, so I'm making a branch and trying that.Array syntax is a bit different than I'm used to, coming off of Python and Javascript.Remember the syntax is `type[] name = {value, value}` or `type[] name = new type[size]` to initialize as empty.To make it multidimensional, just add another set of brackets, like `type[][] name = new type[size][size]`.Right now I am trying to figure out how to make a 2D ArrayList to store all of the valid moves, and keep the loop fairly abstract, to avoid repeating code.Michael's advice to find the furthest possible square in each direction eventually led me in the right direction.I tried to do recursion like he told me at first, but then figured out how to do it with iteration more effectively.I found the Single Responsibility principle to hold true in this case, as making 4 different methods for each of the directions that a piece could move, rather than one with far greater complexity, was much easier, even with the shared code between the 4.Later on I will investigate if there is an effective way to do this, or maybe ask Dr. Wilkerson.I should consider: _How could I have used records in my implementation of Phase 0?_
Something that helped with abstraction was learning this technique: When the body of a loop is the same and the only thing that differs is direction of iteration in 2 different uses, abstract the loop body into a method, and pass in the iterator variable as a parameter.Then just have the loop body be calling that abstracted method.I ran the code quality check as well, just out of curiousity, and got 70%, which I'm very satisfied with, since we haven't lectured on that yet, and I'm not graded on it for this phase.The details said I did well with _Naming_, _Code Decompisition_, and _Package Structure_, but need to improve on _Code readability_.Some of the conditional statements that I used are pretty unpleasant to the eye.I want to start this new practice that I just thought of, which is documenting/journalling a short retrospective of my experience with the project.I think I'll outline the biggest obstacle(s) and how I overcame them, and also what helped me out during the project.Because I've never done projects like this, with this much free rein, I felt a bit lost at first.Eventually **the readings and Dr. Wilkerson's commenets in lecture about design suggestions helped me out**, but I struggled to think of some ideas on my own.In retrospect, using a 2D matrix for a chess board seems obvious, but I should give myself some slack, since this is my first significant experience with software construction.I think that this was because of this: I am at the point where some of the lower-level tasks I can handle just fine, but the higher level things like design choices, such as for storing the board data, or traversing the board for movement rules, I am just beginning with.The aforementioned resources proved helpful because they targetted these things: design suggestions and tips.The **Javadoc from Oracle** also proved pretty helpful and comprehensive.Something else that helped was "lurking", per se, in **the Slack channel**, and reading up on threads from TAs and fellow students who had similar ideas, thoughts, and questions as me.As I was wrangling with copy constructors and such for copying the `ChessBoard` for checking game status, I discovered the `System.arraycopy()` method.It takes 5 parameters: `src`: the source array, `srcPos`: the starting position in the src array, `dest`: the destination array, `destPost`: the starting position in the destination array, and `length`: the number of array elements to be copied.Okay, one bug that I found was that my method for looping through the board and checking the opposite team's pieces to see if the king was in their possible moves (indicating the opponent being in check), called `inCheckLoopBody` was that I was calling piece moves on the unmodified field `board` not the copy of the board with the move made.Making this change worked, but now I'm getting concurrent modification exceptions in my for-each loop, because I'm trying to remove the invalid, check-inducing moves before the iterator working in the for-each loop is finished.I think this highlights the upside of TDD, and exposes a lack of its implmentation on my part for this phase.Going forward, **I can work in a more explicit TDD manner**.This helped me make realizations that made the design/implementation stages easier.I think that this highlights the good effects and, really, the **necessity of studying and understanding well the product's behavior**.If I get a 50 on the first submit, I have 1 week to adjust, resubmit, and get up to 100.I need to read and understand the phase 3 spec to do phase 2 effectively.Here is my personal TA feedback:
authToken is passed to Logout, ListGames, CreateGame, and JoinGame but never used.It may seem redundant, but I need to make clear that each of those needs to verify authentication to be performed.On clearData, because UserData, GameData, and AuthData are going to be on 3 separate SQL tables, it is accurate, and better practice, to have 3 separate clear methods, and call each of those, rather than one method that tries to clear all three.My understanding now is that it only exists on Classes, and is a sort of static field that can be referenced.To store/access a Class itself (I _**think**_), use the `Class<classname>` syntax.So `Class<T> genericClass` is a sort of static object of that class that can be passed around.As I wrote my service test for the `clear` endpoint, I found it hard to instantiate the `ClearService` in a state that could be tested in isolation, rather than instantiating the full, end-to-end process.I wonder if I might need a `testUtilities` class like the passoff tests for the past phases.Maybe what Dr. Wilkerson mentioned about static utility classes could help.I just need to implement 2 more, then run passoff testing and debug accordingly.I also want to see if I can implement inner classes for the Request and Response classes, to apply some of what I'm learning.Pushing myself to meet the quality code standards led me to trying something new, creating the `handleResponse` method in my Server class, which used lambda expressions as one of the parameters, which required making an interface to assign as the type for those lambdas.It was a good stretch that helped me try and learn new things.James the TA helped me to get the history of all of the commits from my old chess repo with some handy git commmands.I'm going to begin by identifying relationships and characteristics of my database, and what my tables should be.Games, containing GameData JSON strings, and Auth, containing AuthData JSON Strings.I am having an issue inmplementing SQL while maintaining modularity and minimizing dependecies in my code.My issue stems from the fact that my DAO interface and service classes don't do a lot of exception handling, that is done mostly in the ServiceHandler lambda in the Server class.I need to try and find a way to appropriately handle the exceptions in each DAO, while maintaining code cohesion and leaving things "hidden" to use Dr. Wilkerson's terms.Here are questions to consider:
What methods do I know of to handle exceptions that avoid me editing any code further up the object model?Are those methods helpful or harmful in the broader scope of my application?Is not editing code further up the object model a sure possibility, or does it appear to be maybe a light necessity, with how I constructed the object model?Is there a way to simply slightly adjust the error handling I already have to account for these `SQLException`s and `DataAccessException`s?My temporary fix for this issue was to make DataAccessException inherit from RuntimeException instead of exception, because that is what all of the other exceptions do that I handle.My current status is that I have enabled the server to create the DB and tables on startup if they don't already exist, and I have one method in the SQLUserDAO passing its positive test.I just need to keep developing the other methods, driven by the tests.The big obstacles I found was formatting the SQL commands correctly, there were some details that slipped my mind.That may have been due to my watching the SQL videos while on the treadmill.Some obstacles included forgetting to include columns after SELECT, forgetting closing parenthesis in long table creation commands, and some others.I also got tripped up thinking the Result set was observing a _column_ at a time, but remembered that the cursor points to a _**row**_.It humbled me a bit and remineded me that these projects are not things that I can whip up in 3 or 4 days, but that need to be worked on for an hour or so each day for a week or two.I think the experience that I got with SQL was good, especially the practice of accessing the DB programmatically.Most of the bugs that I ran into were due to small syntax errors in my SQL commands, where I got most of the syntax right, but missed a few things.Or others were due to foreign key constraints and the way I chose to construct my tables.I think that these obstacles are the type that are simply overcome by time and experience, as the details of syntax get mastered with time, and a sense of how to best design my DB schema will also come over time.This allowed me to have quick access to highly specific and relevant information about things in my exact context.Something else that helps **particularly when it appeared that all of my DB code was correct, but the program was not behaving as expected** was _to drop the tables and recreate them_.This acts as a refresh for the DB, and I learned that any changes I made to the DB being created/set up, were not immediately reflected in the program's behavior, unless I do an ALTER TABLE command.A good way to think about and understand phase 5, specifically the menus, is as 3 different REPLs that the user will move between.There is a depth to the REPLs as well, as they can only exit the program after exiting the first REPL.If they exit the third REPL, it moves them to the second, and exiting the second they move back to the first.In the Client communicator, make the arguments for the 2 methods correspond to the endpoints that will use them, and pass unused params as null.I ran into a decent snag on the negative tests for ServerFacade.The errorStream for most of the error HTTP requests is going to be null, so just input something else as the error message.Right now, I am trying to figure out my starting point after copying the starter code.I am going to review the spec to see what to do with the User game commands and server messages.To accomodate for square highlighting, I had to add a parameter to the `ui.Chessboard`'s `draw()` method to take in a string of the square for which to highlight moves.Then I made a method in `ui.Chessboard` to decode that string into a "tuple" of indexes for the 2D array of squares.I also had to change the `board` parameter to have a `ChessBoard` type, rather than `ChessPiece[][]`, in order to accomodate calling `.pieceMoves()` in the `ui.Chessboard` class, since that method takes in a `ChessBoard` type, and I didn't want to bother changing that and all it's dependencies.Current progress as of 4.1: I have the highlighting down for when the board needs to be from the white player/observer's perspective, but there is a bug with highlighting from the black player's perspective.The board will load from the black player's perspective as expected, but then when I run the highlight command, the pieces get flipped.The ranks (numbers) are flipped, but the files (letters) are not.I think the bug may lie in the GameData that I am passing to the game menu.Upon creating a game, and then joining it, the game gets updated, but nothing gets returned.Thus, the gameData that I am passing to the `GameMenu` is the original gameData, not the updated one with the userName updated.So, when I am trying to go from the Black player's perspective, the method to check the usernames always returns false.Because I haven't fixed the bug preventing players from leaving and rejoining a game, I can't get back into a game, but I am feeling that if I could, I would be able to see the black player's highlighting work correctly.I've found another bug, where when I observe 1 game, make a highlight, and then leave, and observe another game, the highlighting from the first game is still there.The fix that I found for the first bug was that, the gamesList that I was searching in the JoinHandler was what needed to be updated after the `serverFacade.joinGame()` call.This resolved the issue of joining a game, and the gameData not having the player's usernames.For the second bug, the cause was rooted in my use of static variables.Because the variables for the piece to highlight and the arrayList of its legal moves were all static, whenever they were changed, that change persisted because it was performed to the class itself.This caused me to reflect on if making the `ui.Chessboard` class static was the best move in this context.On the one hand, the need to clear out certain variables each time, and make use of several parameters indicates the need for instance-specific data, making the class be non-static seem logical.On the other hand, with the sheer amount of Chessboards that may be drawn during a given match of chess, it would be very memory inefficient to create a new Chessboard instance every single time it is drawn.If I wanted to take some time later to optimize, finding a way to store and reuse instance of Chessboards would be a good middle ground that would allow the benefits of both routes.So, I've had to wrangle with type adapters to correctly serialize `ServerMessage`s and `UserGameCommand`s, and I've learned a couple of important things, one about Gson type adapters and one about Websocket with jetty and javax:
1.There is a subtle bug that can cause a Gson type adapter to recurse infinitely and cause a stack overflow.In my case, I was dealing with _deserializing `UserGameCommand` JSON strings on the server side_.Since `MAKE_MOVE` is the only commmand type that is unique in its parameters, I simply had the other three command types deserialize to a `UserGameCommand` object with the command type set to the correct type.However, the deserializer inner class that I was writing, like most deserializer type adapters, implements the `JsonDeserializer<T>` interface.Because I had implemented this interface for the `UserGameCommand` class, then calling `context.deserialize(jsonElement, UserGameCommand.class)` would cause the type adapter to call itself.So, even though the other three game command types didn't have any unique fields, because the `commandType` field needs to be different, and this Gson subtlety, I had to create a `UserGameCommand` subclass for each of the other three.For whatever, reason, _on the client side_, when adding the `WebsocketCommunicator`'s message handler, following the IntelliJ suggestion to change the anonymous inner class to a lambda caused the message handler to not be called.I had to change it back to an anonymous inner class, and then it worked.I don't know why this is the case, and I frankly didn't even know I was using an anonymous inner class.In the future, I want to examine this a bit closer and understand the inner class better.I am finding that following PetShop's example is useful here, making some separate classes in a websocket passage to keep things cleaner.When I was working on the `LEAVE` websocket interaction, the thing that caught be was using an if-else if block for the null checks on white/black usernames.As far as I understand, because the values for the other usernames can sometimes be null, doing `game.blackUsername().equals(userName)` will throw a null pointer exception.If I want to write slightly more efficient code, I'll need to do `if (username.equals(game.blackUsername())` instead.A bug that got me for a good few minutes was making the Websocket endpoint URL one that was not what the spec for the test and Test website expected.I was getting a `ConcurrentModificationException` on the `broadcast` method.I thought it was because I was using a `TreeMap` instead of a `ConcurrentHashMap`, but it turned out it was because I was removing closed connections in my for-each loop.This was making the iterator that the for-each creates behind the scenes invalid, causing the exception.Right now I can make moves just fine, but the notification is only being sent on the first move, not on all subseqent moves.The tests are also struggling to get a Chessmove from the client.Desired features of the application:
List of groceries is inputted, if certain brands a desired, those are listed as well
Store is selected
Store website is opened and crawled for the groceries
Prices are compared with 3 options, and the cheapest is added to a dictionary with the grocery as the key and the price as the value
Aisle numbers of the groceries are added to the dictionary.Watch at least 10 minutes at a time, and then try an replicate what has been coded, or take notes on what you have observed.This helps you acquire understanding rather than rotely copy down notes.Remember that struggle is good, and I will get stuck at points in this course.Use the tools that I know are in my toolbox to get a solution.Don't skip any of the tutorials, but feel free to speed them up if needed.Search StackOverflow
Ask someone, using a 4 step approach
    1.Any computer that a user engages with is a "client" and the 24/7, big computers are "servers".A request goes from a client to an ISP (internet service provide) and then to a DNS (domain name server).Websites on browsers are just HTML, CSS, and Js files that are rendered locally using the browser.I can edit the HTML, CSS, or Js for a website that I am viewing for my browser, and those changes are local, because the browser page I currently have open is essentially my local copy of the website files.From what I have picked up, `index.html` is the standard name given to the homepage of a website.And HTML element is the entirety of the object, from the opening tag to the closing, and all of the attributes and content in between.Good practices: Don't have more than one `h1` element in an HTML doc.Use [Lipsum](https://www.lipsum.com) to get placeholder text.There is also funny spin offs, like [Broipsum](https://www.broipsum.com).Some elements have tags that are self closing, or are "void".Two examples are the `<br/>`, break element, and the `<hr/>`, horizontal rule element.The break element creates a break in a paragraph, placing the following text on separate line.These should not be used to create separate paragraphs inside of one paragraph element.When new paragraphs are needed, to help screenreader functionality, use a new paragraph element every time.The horizontal rule element creates a line horizontally to divide elements in the doc.You can validly write void elements like this: `<br/>` or like this: `<br>`.HTML 5 can recognize the void elements and doesn't have to read for the forward slash.Writing the void elements with the forward slash is beneficial, however, for maintaining readability and comprehension of code.The command `Emmet: Wrap with Abbrevation` in the VSC command pallet allows me to place the selected on the _inside_ of an HTML tag.Anytime that you save in VS Code, it automatically re-indents the code to be the most useful.Example: `<a href= "mylink12345.com"> This is a link </a>`
There are attributes that are specific to certain tags, but there are also global attributes, which are accessible by every HTML element.To look at them some more, go [here](https://developer.mozilla.org/en-US/docs/Web/HTML/Global_attributes)
Quotations are always used for the URL in the `href` attribute of `<a>` tags, to despecialize the text that is given.When dealing with element attributes that have boolean values, simply including them sets them to true, such as with `reversed` for `<ol>` tags.This is only true for attributes where the default value is false.You can make an image the source of a hyperlink, by simply placing the `img` element in between the tags of the anchor element.The href of the anchor element is used as normal to link to the desired place.This is what a standard boilerplate looks like:
The `head` element is for declaring important information about that page that will not be visible to the user.For example the `title` element is what displays on the tab in the browser that is rendering the page.The `meta` element is what dictates other important info, such as the character set used.Another important `meta` element is:
`<meta name = "viewport" content="width=device-width, initial-scale=1.0">`
This will render the page at the width of the device, regardless of what it is.Web Hosting is the process of taking the files that a website is made up of, and placing them on a web server that is connected directly to the Internet.This allows anyone connected to the Internet to access that webpage.GitHub repos can be used to host websites, using a feature called "GitHub Pages".When creating the repo, simply choose public instead of private.After creating the repo, go to 'Settings' and then to 'Pages' on the left sidebar.Under the 'Build and Deployment' section, make sure that the source is set to 'deploy from a branch'.Then set the Branch to deploy from to 'main' instead of 'None'.This process can take up to 10 minutes, so keep refreshing and checking back until a URL appears, and it works correctly.Style Sheet is a type of language, just like Markup Language is (the ML in HTML).Other types include Sass (Syntactically Awesome Style Sheet), Less (Leaner CSS), and others.CSS is needed because handling the styling using HTML elements clutters up the HTML documents a ton.Keeping the HTML focused on content, and designating the CSS entirely for styling keeps things cleaner.Here is an example: `<html style = "background: blue"></html>`.Inline CSS is useful when adding CSS properties to a single element, or when doing testing to see what a certain CSS property value will look like.This method, and External CSS, because they apply to more than on element, must include a 'selector' which designates which HTML elements the CSS code applies to.Internal CSS is good for when you want to apply CSS to all the elements in just 1 HTML document, but not for multipage websites.The `style` element is placed in the `head` element of the HTML document.This file contains CSS code which follows the same syntax as that of Internal CSS.The file is attached and applied to the HTML documents by including a `link` element in the `head` element of each document it applies to.The `link` element is self-closing, and has two important attributes: `rel` meaining relationship, which explains the role of the item being linked, and `href` which gives the location of the linked item.The .css file is then attached by making the file path the value of the `href` attribute, and the `rel` attribute is usually set to "stylesheet" or something similar.External CSS is most common in web development, and is best for when working with multipage websites, as it applies the CSS code to entire websites.Element Selectors:
    This is the most common type of selector, with the syntax of `element type {property:value;}` This will apply a CSS rule to all of the HTML elements of the designated type.Class Selectors:
    The `class` attribute is a global HTML attribute, which can be set to anything.Whatever the `class` attribute is set to is a class that is created.Then, a class selector can be made, which applies the CSS rule to any element, regardless of the type, that is of that class.The class name as declared in the HTML element's opening tag does not include the period in front of it.ID Selectors:
    The `id` attribute is another global HTML attribute, which can also be set to anything.This behaves the same as the `class` attribute, except that there can only be one element with each value of the `id` attribute.So, CSS rules declared in an ID selector will only apply to 1 element in a single HTML document, as opposed to class selectors, which can apply to many elements across many documents.Attribute Selectors:
These are sort of a modification of Element Selectors.They allow you to apply a CSS rule to only elements that posses a certain attribute, or that posses a certain attribute with a certain value.The value will have to match exaclty with what it appears in the HTML, inlcuding quotes.Universal Selectors:
    Apply a CSS rule to all of the elements in an HTML document.The syntax is `*{property: value;}`
---
If I am unsure about what a property does, check the MDN Web Docs.If you use `background-color` on a single element, that will just change the background color for the area of that element, not the whole webpage.There are lots of "named colors" in CSS, which can be applied by simply typing their name.You can also set a color by typing in a hexcode for an RGB value.Or, many IDEs let you adjust the RGB values and colors yourself using text input or GUIs.All of the colors include their hexcode, which can be copied to apply that color easily to the CSS.Methods of measuring font **size**:
Pixels, denoted `px` is 1/96th of an inch.Example: `font-size: 20px;`
Points, denoted `pt` is 1/72nd of an inch.It functions the same as em, but is relative to the root element rather than the parent element.There are also some named font types that can be applied, such as `small`, `large`, or `xx-large`.Font **weight**:
Font weight deals with the boldness of the text.There are three main ways to declare font weight in CSS:
Keywords, such as `normal` or `bold`
Relative to parent, `lighter` or `bolder`
Specific value `700` or `400`.When using relative weighting, `lighter` just decreases the child element's weight by 100, and `bolder` increases it by 100.Font **families**
A font family is a way of selecting the typeface or appearance of the font, where the specific preferred font is typed, and then a generic, backup font is given after.An example of this CSS property being applied is:
This would attempt to apply the typeface "Helvetica" to the h1 elements, and if that is not possible, it would apply the generic "sans-serif typeface.Text **Align**
This property can have values assigned like `left`, `center`, `right`, or `start` and `end`.To inspect the CSS on a webpage, it is the same as inspecting the HTML.You can right click something and then hit "Inspect" or "Inspect Element" and that will bring up the developer tools.In the 'Elements' tab, you see the HTML on one side, and the CSS on another side.When you click on an element in the HTML, you can view on the right of it the applied CSS.You can look at it using the 'Styles' tab on the CSS half, which shows all of the CSS rules and where they come from.Some will be crossed out, because they are being overridden by another source.To view just the CSS that is being applied, switch to the 'Computed' tab on the CSS portion.When using Chrome Developer Tools, there is also a handy tool called 'CSS Overview'.This is accessed by clicking the three dots > clicking 'More Tools' > then clicking 'CSS Overview'.This will automatically generate an overview of the colors and fonts used, giving names and/or hexcodes, and showing the frequency of their usages.Five properties make up what is called the CSS Box Model, an important concept in Web Design.The height, width, margin, border, and padding of some content is what makes up an element's "box"
`margin` is the distance on each side between the edge of the browsing window and the border.The `border` is set all the way the content, with its type, color, and thickness declared in the CSS.The `padding` is the distance between the border and the content within the border.Different elements can be grouped together in "boxes" manually, using the HTML `div` element.This will group pieces of content together and apply CSS to them similarly, and is often done when two items go together contextually.When doing arithmetic to figure out box layouts with CSS, remember that paddings, margins, and borders will almost always apply on both sides of something, so amounts being added will often need to be doubled.CSS has a sort of order of operations, or a hierarchy that dictates which rules get applied to the HTML elements when there are conflicts or collisions (to use the hashing term).This is described as the "cascade" of CSS (Cascading Style Sheets).This is a description of the cascade:
Position: The CSS rule that is lower down is what will be visible after execution
    
    In this example, the color blue would be visible on the live HTML document for the li elements.Specificity: The CSS selector with the highest level of specificity is what will be visible and be the overider amongst conflicting rules.This is the order of specificity, from least to greatest:
Element selectors
Class selectors
Attribute selectors
ID selectors
Type: This one is similar to specificity.The method used to implement the CSS, or the type of CSS that it is (External, Internal, or Inline) dictates which one is active.This is the hierarchy, from lowest to highest:
External CSS (lowest)
Internal CSS
Inline CSS
Importance: The importance of a CSS rule among conflicting rules is the most important factor.Importance is denoted by using a CSS keyword, which is `!important` in between the property value and the semi-colon of a CSS rule.There are a few different method for combining CSS selectors, to target specific groups of content in your HTML document.Group:
    This will simply apply the following CSS rule to all of the indicated elements, delimited by commas.Here is an example:
Child:
    This will apply the CSS rule to the indicated element that is a direct child of the indicated parent element.Here is the syntax:
Descendant:
    This method of combining selectors has the following syntax:
    
    This will apply the CSS rule to any element that is a descendant of the specified element on the left side, regardless of how many generation deep.Chaining:
    Here is the syntax for this method of combining selectors:
    
    What this method does is apply the CSS rule where all selectors are true.So, combining an element selector, to only get elements with a certain id would look like this: `li.to-doItems{`.Or combining element, class, and id selectors would look like this: `p#id1.classname{` When chaining, always place the element selector first if one is being chained, because id and class selectors have the special characters to help denote themselves.The position attribute is a CSS property that is applied to HTML elements to determine their placement on the HTML document, and thus the webpage.There are 4 main types:
Static:
    This is the default position for all HTML elements.It simply places an element below the proceeding element, in sequential order following the HTML syntax.Relative:
    This is positioning relative to _that element's static positioning_.So `p{position: relative; left: 50px; top 50px;}` would place the paragraph element 50 pixels from the left and down from where it would be in static positioning.Absolute:
    This positioning places an element relative to the closest positioned ancestor, or at the top left of the web page.An element is considered "positioned" as long as the position property is set.Thus, the CSS rule `h1{position: absolute; left: 25px;}` would place all h1 elements either 25 pixels left of the position of their closest ancestor element with the position value set, or 25 pixels to the left of the top left of the web page.So, if the closest ancestor of that h1 tag, with a position set, is 2 generations up in a div tag, the position of the h1 is relative to the position of that div tag.It places the element the specified amount in the top left of the browser window, and stays there no matter where you are on the webpage.One important attribute to make note of with positioning is the `z-index` attribute.This dictates an element's position on the z-axis, which will determine if it is placed in front of or behind other elements on the page.One interesting thing that I found was that I found a way to get the desired outcome, but it was significantly different from the provided solution.What I found as interesting was that if I tried to take parts of the CSS from the provided solution, and replace the parts of my own CSS that served the same function, it wouldn't work the same.I think the point that this may prove is that working with the CSS is very much like a scripted program or a larger scale function, not everything is just plug-and-play, because of interactions with other pieces and dependencies that are established.The Display property affects how different elements can display on a rendered webpage.There are three values for the display property: `block`, `inline` and `inline-block`.Here is a description of each:
Block:
    Block elements will have their "box" take up the entire length of the the webpage, and no other other elements can be on the same line as them, regardless of the length of the content.Inline:
    Inline elements will only allow their "box" to take up the amount of space needed for the content of the element.The width and height of inline elements cannot be adjusted (they can be actually, they simply have no effect when rendered, however).Inline elements do get placed on the same line, given there is room enough for them.Thus, 2 `p` elements could be placed on the same line, appearing as if they are two sentences of the same paragraph, if they are inline elements.Inline-block:
    This is essentially a hybrid of the two previous display types.Elements will be placed on the same line if there is enough space, but the height at width can also be altered, regardless of the size of the element's content.This property is applied to certain elements, and dictates where other content is positioned relative to it.For example, if the float property on an `img` selector is given the value of `left` then the image will be positioned to the left of other content, and the other content, like text for example, will wrap around that image on the right side.To make an element disregard another element's float, to position it below in a new block as normally would happen, use the `clear` property.This property, if given a value of `left` for example, would cause the selected element to disregard all left floating elements, and render as normal.The same applies to elements with clear set to `right` with right floating elements.To make an element disregard all floats, use the value `both`.In the past, float properties were manipulated to create complex website layouts, but now tools like Flexbox, Grid, and Bootstrap exist to hand the more complex layouts, so using the float property for those are less effective.Using CSS, it is possible to create webpages that are responsive to the size of the client device being used.That is, the same website, with the same html files can be rendered differently, depending on if it is rendered on a smartphone, tablet, laptop, or desktop.External Frameworks, like Bootstrap
**Media Queries**
These are written in the CSS file in this way:
The `@media` identifier is followed by what is called the "breakpoint" which is the condition on which the application of the following CSS depends on.The basic idea is that the container of a grid, which is created as a `div` element, can be given the `display` attribute of `grid` rather than block or inline.This allows the number of grid columns and rows to be set using attributes such as `grid-template-columns`, `grid-template-rows`.The elements inside of the `div` element that acts as the container become the contents of the columns and rows, and the properties of those rows and columns can be edited, such as changing the span of `grid-column` or `grid-row`.This method essentially works one-dimensionally, where the width or the height of the content can be set to fulfill certain ratios, which by nature makes the page responsive,
as the dimension that is controlled by a flexbox is a ratio of the total avaliable dimension, rather than a set number.This is implemented similarly to CSS Grid, where a parent `div` element will be given the value of `flex` to its `display` CSS attribute.Then, the children elements will have their `flex` attribute given a numerical value.A `flex` value of 1 will give that element the same dimension as all of the other elements, or it will evenly distribute the dimension among all of the elements in the flexbox.A `flex` value of 2, would give that element 2x the dimensional value as the standard amount (however much an element with a `flex` of 1 would have).The Bootstrap Framework is a library of prewritten CSS code, that is implemented by giving elements certain types of classes, such as `card`, `row`, or `container`.This will then apply the corresponding CSS rule when the Bootstrap framework is active.Bootstrap is built off of CSS Flexbox, so the responsiveness works similarly.It uses a 12 div system, so an element is given its ratio of a dimension by declaring how many of those 12 sections the element should use.For example, an element with the class `card-col-6` would use 6 of those divs, or a 50% ratio of the dimension, depending on the screen size.The description for how media queries work is given in part above.Essentially breakpoints are defined in the CSS file using different keywords to create overriding rules for certain screen sizes/orientations, or other use cases.For example, `max-width` will apply the corresponding CSS rule to all screens that are less than or equal to the list specified with this keyword.There is also the `orientation` keyword, which can apply CSS depending on if a device's screen is in portrait or landscape orientation.Different keywords can be combined using `and` to target or exclude specific screen sizes and types as well.These are described really well on the MDN Doc for media queries.Something that I noticed with this project, is that I felt a bit stymied in taking my own approach and being creative with the project.I was very focused on getting the CSS to work, and simply ended up copying the solution code.I tried to make sure I understood what each line did as I included it, but it still wasn't as deep of a learning experience as it would've been had I followed my own idea.I think what caused that hesitancy to participate was a lack of knowledge and experience.On the next project, I am going to try intentionally to do things independently, and get things to work on my own, without looking at or copying the solution code until I am completely finished.I also was reminded that sometimes the simplest little errors can cause headaches.I could not get my images to resize correctly when on larger than mobile sized screens.After lots of static observation, I found out that it was because when I was declaring the `height` for the images in the CSS, I had typed `200` instead of `200px`.As CSS developed, people found complex and roundabout ways to create website layouts using the 3 basic `display` attributes: `block`, `inline`, and `inline-block`, as well as the `float` attribute.Now this practice is discouraged in Web development, as much more sophisticated methods are avaliable for website layout, such as Flexbox, Grid, Bootstrap, and Tailwind.The `float` attribute and the 3 basic display methods should still be used for individual element styling, but not for overall website layout design.To implement the Flexbox method, simply set the `display` attribute to a value of `flex`.The `display` can also be set to `inline-flex` which will make the flexbox container span only as much as it needs to.There are lots of attributes that are used to manipulate the flexbox, which can be found at length on MDN Web Docs for Flexbox.One example is the `gap` attribute, which determines the amount of space between each element in the flexbox.This can be set to 4 different values: `row` (this is the default value), `row-reverse`, `column`, and `column-reverse`.What these different values do is set the direction of the main axis and the cross axis.The main axis is the axis which delineates where the next element would be placed in the flexbox, and the cross axis is the axis perpendicular to that.For example, in the default situation of `flex-direction: row`, the main axis runs left to right, and the cross axis runs down and up.This is important because it dictates the order in which the next element would be placed in the flexbox.Also, certain flexbox attributes affect the elements differently depending on the direction of the main axis.For example, `flex-basis` is a property that determines the width of each element in the flexbox, when the `flex-direction` is set to `row`.When `flex-direction` is `column`, flex basis then affects the height of the elements.This property is adjusted on each of the elements inside the flexbox container, not the container itself.As I noticed in the exercise for the last lesson, something that can be problematic if not understood is whether a Flexbox attribute gets applied to the items in the Flexbox (called the children or flex items), or the Flexbox container (called the parent or simply, the container).In the example of the last exercise, the `flex-basis` attribute is one that is applied to the children, not the parent.A couple of useful attributes for manipulating the layout of a Flexbox are `order` and `flex-wrap`.When a non-default value is applied to this attribute, the elements appear based on the size of the `order` attribute's value, from smallest to largest.This means that any elements that do not have sufficient space on the page will be pushed off of it and will be rendered unviewable.The alternate is setting this attribute to `wrap` which will expand the block, wrapping the flex items to the new row/column once there is not enough space in the initial block.There is also the value `wrap-reverse` which will reverse the order of item wrapping.Some of its potential values include: `flex-start` which is the default value, and distributes the flex items one after another from the very beginning of the flex container.Lastly, `space-evenly` works the same as `space-around`, but the spaces are adjusted to be equal between each item,
`align-items` is also a parent attribute, and it dictates the distribution of the items across the cross axis.Because the container by default has a height that is more-or-less "inline", meaning that it only takes up the cross-axis space needed for the content.Thus, to use `align-items`, at least when the cross-axis is vertical, the height must be set to something.One unit that can be used is `vh` which stands for "viewport height".This is given as a percentage of the browser window, with 100vh being the complete height of the browser window.Major values for `align-items` are identical to `justify-content`: `flex-start`, `flex-end`, `center`,`space-between`,`space-around`, and `space-evenly`.While `align-items` is a container value, individual flex items can have their alignment manipulated using the child attribute `align-self` which uses the same values as `align-items`.Lastly, `align-content` behaves the same as `justify-content` but only when `flex-wrap` is set to `wrap` or `wrap-reverse`.This will apply layout rules to flex items that are being wrapped, while `justify-content` and `align-items`do not.In determining the size of flex items, there is an "order of operations" of sorts, that is followed.It goes as follows:
`min-width`/`max-width` value
`flex-basis` value
`width` value
Content width
These attributes are all child properties.The default value for `max-width` when looking at the content's width is the length needed to fit all of the content on one line.The default for `min-width` when taking the content width is the width needed for the longest single word, Flexbox doesn't split up words at all.When a `max-width` is set, the flex items will grow or shrink, but only up to that value.So if there is a `flex-basis` property with a higher value, it will be ignored.Likewise, if a `min-width` is set, any values lower than that will be disregarded, and when shrinking flex items they will only be shrunk until they are at the min width.If the screen becomes too small, those items will simply be pushed off the browser window.The properties that allow for growing and shrinking in Flexbox are ones that can actually be toggled in the CSS.The default for Flexbox is that `flex-shrink` is turned on, and `flex-grow` is turned off.If shrink is turned off, then items will simply be pushed off of the window once there is not enough room for their applied size.If grow is turned on, then items will be given width beyond their max-width/the applied size, until the entirety of the window is utilized.When a value greater than 1 is given to flex-grow or flex-shrink, it gives that item a greater ratio of the avaliable space when growing or shrinking.So a flex-item with a flex-grow value of 2 would get double the size as an item with a flex-grow value of 1.The default value of `flex-basis` is `auto` which will assess the amount of content in each flex item, and give a greater flex basis vaule to the items with more content, and less to the ones with less content.Setting `flex-basis` to 0 will disable this, and setting a numerical value as the flex-basis will set that was the basis for all flex-items.The shorthand for setting the `flex-basis`, `flex-grow`, and `flex-shrink` properties is `flex: 1 1 0;` This would set flex-grow to 1, flex-shrink to 1, and flex-basis to 0.So, syntactically, the first number is the flex-grow value, the second the flex-shrink value, and the third the flex-basis value.Even when I did look at the solution, I didn't just copy code over, I simply compared, and see what differences difference values made.The 3 things that I was stuck on were: getting the dynamic behavior to work properly (snapping instead of wrapping), getting the row to be in the center of the screen instead of the top, and getting the bullet points to be centered by the content center, not the start of the `li` element.The first one I solved by simply not setting the `flex-flow` attribute on the flex-container for the larger screen case.Something about setting it explicitly doesn't allow the media query to override it.The second one I solved by being reminded that, to alter the height/vertical alignment of anything in Flexbox, the height attribute must be set on the parent container.So, I set the height of the body to 100vh, and then the `align-items` attribute to `center` on the flex-container, and that did the trick.It is what makes websites able to do things, and it is among the top 3 most popular programming languages according to GitHub and Stack Overflow.JavaScript is run on the 'client-side', meaning the users local computer, as opposed to a language like PHP which is run on the 'server-side', meaning the server that hosts the website.This is why JavaScript is so powerful, because it can interact with the user's computer directly.This takes a string as an argument and outputs that string on a pop-up.Another command is the `prompt()` command, which acts like an `alert`, but can receive input from the user.Remember, when instatiating variable, the `var` keyword is necessary.That was an error that I ran into while first trying to do things in Javascript, one of those new language adjustments.One method of the string data type in Js is `.length`, which returns the length of a string as a number.The `.slice(x,y)` method allows me to access specific portions of a string.Similar to slicing with array indeces, the range passed in is the lower "fence" and the upper fence, which includes the bottom arguement but excludes the upper one.Also just like slicing with array indeces, if I typed `word.slice(4,)` this would select the word starting at the 5th character, up to and including the end.Or, if I typed `word.slice(,4)` that would slice from the beginning, up to, but not including the 5th character.Two other useful methods are `toUpperCase()` and `toLowerCase` which convert the entirety of the strings that they are called on to upper or lower case.Both ++ and -- can be used to increment and decrement, as well as the `+=`, `-=`, `*=`, `/=`, and `%=` assignment operators.Side note: `console.log()` is a method that will log text to the JS console, instead of a pop-up.It can also be used on a single argument to simply round that number down.I do like the way that the instructor described three different types of functions: functions that take no parameters and have no output are like vanilla, functions that take parameters are like chocolate, and functions that take parameters and return values are like strawberry.There isn't a `randint()` method of any kind, so getting a random number between two arbitrary values can be trickier, but not very tough.You simply multiply the random deciaml by the max range value, then round that number down, and that is your random integer.This can be resolved by keeping the same equation and multiplying the random number by the max range value plus 1, then rounding down.Or, if the bottom range is exclusive, you can multiple the random number by the max and then simply add one after rounding down.The syntax is the same as well, with the curly braces denoting the code block that is executed if the condition is met.The `else if` statement is used to check for multiple conditions, and the `else` statement is used to execute code if none of the previous conditions are met.The `else` and `else if` statements are formatted similarly to PHP.The data type, length, or fact that it is an array doesn't need to be specified, like in other languages.You just type the var keyword, and then include the square brackets.Useful array methods include the `.length` attribute (not a method!To add an item onto the array, use the `.push()` method, and to remove the last item, use the `.pop()` method.To remove the first item, use the `.shift()` method, and to add an item to the beginning, use the `.unshift()` method.The `for` loop syntax is the same as non-Python languages, having initialization, then condition check (both of which are followed by semicolons) and then the iteration.While loops are used more when a certain state or circumstance is being focused on, and for loops for when a certain number of iterations is needed.Inline is done by assigning values to certain HTML attributes that can take Javascript code as a value, such as the `onload` attribute of the `body` tag, which exectues its value's code on loading.Internal Javascript is added using the `script` element, where the JS code is written between the tags.The `type` attribute is not necessary to set, as the default value is `text/javascript`.External Javascript is added by using a `script` element as well, but instead the `src` attribute is set to the path of the external JS file.As far as the placement of the CSS and the Js in the HTML document, the CSS should be placed in the `head` element, so that the styling will render first, and the elements won't render without styling, and then have styling applied later.This looks clunky and unprofessional, so always put the link to the CSS file in the `head` element.Javascript is the opposite, in that the link to the Javascript should be placed at the last line before the closing `</body>` tag.This will ensure that the HTML renders before the Js tries to execute.If the Javascript is placed at the top of the document, it will be executed before the HTML is rendered, and thus will be trying to interact with HTML elements that don't exist yet.The root of the tree is the `document` object, and the branches are the elements of the HTML document.The `document` object has many methods and properties that can be used to interact with the HTML document.For example, if a website has an html element with a `head` element and a `body` element, the document object would be the root, and then its only child would be the `html` element, and then the `head` and `body` elements would be children of the `html` element.This underlies the way that elements are interacted with using Js.For example, `document.firstElementChild` would return the first child of the `document` object, which would be the `html` element.Then, `document.firstElementChild.firstElementChild` would return the `head` element, and so on.Remember a good way of describing objects is by thinking of them in terms of nouns, adjectives, and verbs.The objects are the nouns, the properties are the adjectives, and the methods are the verbs.Each html element is an object, and each object has properties and methods that can be used to interact with it.Such as the `innerHTML` property, which can be used to change the inner HTML of an element, or the `style` property, which can be used to change the style of an element.Examples of methods are `click()`, `setAttribute()` and `appendChild()`.I learned that as I was tinkering around in this lesson, so to get a child element, make sure to use `firstChildElement` or `lastChildElement`.Something to keep in mind when manipulating style using Js, is that the CSS attribute names will appear slightly differently.Instead of being all lowercase with no dashes, they will camel cased, with no dashes in the Javascript code.One of the keys to doing this, when the style or look needs to change dynamically, is by manipulating the `classList` attribute that applies to every Js object/HTML element.The `classList` is an attribute whose value is an array of all of the classes that are applied to the element.The `classList` attribute has methods that can be used to manipulate the classes of the element, such as `add()`, `remove()`, and `toggle()`.The `toggle()` method will add the class if it is not present, and remove it if it is present.The `textContent` attribute will return the text content of the element, without any of the HTML tags, and thus be used to manipulate that text.To manipulate these, use the `getAttribute()` method, while passing in the name of the attribute to get the value for, and the `setAttribute()` method, while passing in 2 arguments: the name of the attribute to set, and the value to set it to.Well over 80% of developers use Git as their VCS (version control system), it is the clear choice as far as VCS is concerned.An important point to remember is the difference between Git and Github.Git is a local application that does the actual version control, and works locally on someone's machine.Github is a cloud based service that stores Git repositories and enables collaboration between developers.